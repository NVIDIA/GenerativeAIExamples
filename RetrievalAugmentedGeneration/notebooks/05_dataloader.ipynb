{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a74231c-df99-461f-8cdc-ea17f808d717",
   "metadata": {},
   "source": [
    "### Notebook-5: Create a NVIDIA PR chatbot\n",
    "As part of this generative AI workflow, we create a NVIDIA PR chatbot that answers questions from the nvidia news and blogs from years of 2022 and 2023. For this, we have created a REST FastAPI server that wraps llama-index. The API server has two methods, ```upload_document``` and ```generate```. The ```upload_document``` method takes a document from the user's computer and uploads it to a Milvus vector database after splitting, chunking and embedding the document. The ```generate``` API method generates an answer from the provided prompt optionally sourcing information from a vector database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8a538-b766-482d-9fde-9d20482fe7db",
   "metadata": {},
   "source": [
    "#### Step-1: Load the pdf files from the dataset folder.\n",
    "\n",
    "You can upload the pdf files containing the NVIDIA blogs to ```query:8081/uploadDocument``` API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598b9e7-8a04-4220-a875-69e6bbe6a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70a746-9bc0-4025-b6d5-76bba2473ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import mimetypes\n",
    "\n",
    "def upload_document(file_path, url):\n",
    "    headers = {\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    files = {\n",
    "        'file': (file_path, open(file_path, 'rb'), mime_type)\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def upload_pdf_files(folder_path, upload_url, num_files):\n",
    "    i = 0\n",
    "    for files in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, files)\n",
    "        print(upload_document(file_path, upload_url))\n",
    "        i += 1\n",
    "        if i > num_files:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819b268-5867-45fd-9f52-00d5d797e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "NUM_DOCS_TO_UPLOAD=100\n",
    "upload_pdf_files(\"dataset\", \"http://query:8081/uploadDocument\", NUM_DOCS_TO_UPLOAD)\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d813be-76f2-42d1-9b43-403625c0b4ce",
   "metadata": {},
   "source": [
    "#### Step-2 : Ask a question without referring to the knowledge base\n",
    "Ask Tensorrt LLM llama-2 13B model a question about \"the nvidia grace superchip\" without seeking help from the vectordb/knowledge base by setting ```use_knowledge_base``` to ```false```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d43b1a-f9b2-4119-b6c1-66dd38130e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "data = {\n",
    "  \"question\": \"how many cores are on the nvidia grace superchip?\",\n",
    "  \"context\": \"\",\n",
    "  \"use_knowledge_base\": \"false\",\n",
    "  \"num_tokens\": 256\n",
    "}\n",
    "\n",
    "url = \"http://query:8081/generate\"\n",
    "\n",
    "start_time = time.time()\n",
    "with requests.post(url, stream=True, json=data) as r:\n",
    "    for chunk in r.iter_content(16):\n",
    "        print(chunk.decode(\"UTF-8\"), end =\"\")\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb186d4b-61c9-4b48-8ec9-3d238db787b8",
   "metadata": {},
   "source": [
    "Now ask it the same question by setting ```use_knowledge_base``` to ```true```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903635bb-8d60-40ae-b613-67a2b8a08eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"question\": \"how many cores are on the nvidia grace superchip?\",\n",
    "  \"context\": \"\",\n",
    "  \"use_knowledge_base\": \"true\",\n",
    "  \"num_tokens\": 50\n",
    "}\n",
    "\n",
    "url = \"http://query:8081/generate\"\n",
    "\n",
    "start_time = time.time()\n",
    "tokens_generated = 0\n",
    "with requests.post(url, stream=True, json=data) as r:\n",
    "    for chunk in r.iter_content(16):\n",
    "        tokens_generated += 1\n",
    "        print(chunk.decode(\"UTF-8\"), end =\"\")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n--- Generated {tokens_generated} tokens in {total_time} seconds ---\")\n",
    "print(f\"--- {tokens_generated/total_time} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8fffa-97d1-4b27-b1cd-87067853a65f",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "We have setup a playground UI for you to upload files and get answers from, the UI is available on the same IP address as the notebooks: `host_ip:8090/converse`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

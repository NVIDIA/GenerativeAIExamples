{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1cd0ff0-3c07-470b-aace-9fd901317998",
   "metadata": {},
   "source": [
    "## Creating a RAG application with LLM, Embedding model, and Vector DataBase hosted locally\n",
    "\n",
    "This notebook showcases how to create a Retrieval Augmented Generation (RAG) application where the LLM model, embedding model, and Vector DataBase (VDB) are deployed locally. \n",
    "\n",
    "We will be using NVIDIA NIM microservices to locally host [Llama3-8B-instruct model](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html). The microservice will be connected using [LangChain NVIDIA AI Endpoints](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) package.\n",
    "\n",
    "For creating embeddings from your proprietary documents, we will be using embedding model hosted on [huggingface](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) which will again be connected using LangChain's [huggingface plugin](https://python.langchain.com/v0.2/docs/integrations/platforms/huggingface/).\n",
    "\n",
    "Lastly for storing the embeddings, we will be using the Facebook AI Similarity Search (FAISS) plugin available in [LangChain](https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/).\n",
    "\n",
    "This notebook is divided into two parts: \n",
    "1. In the first part we will showcase how to create embeddings from your documents and store them in a VDB.\n",
    "2. In the second part we will orchestrate the RAG application using Langchain framework and create a Gradio-based simple UI to interact with this application.\n",
    "\n",
    "### API Key generation:\n",
    "\n",
    "Before we get started, generate the API keys to use model from NVIDIA NIM microservice and download the embedding model from HuggingFace. \n",
    "\n",
    "**To generate 'NVIDIA_API_KEY' for NVIDIA NIM microserice:**\n",
    "\n",
    "1. Create a free account with [NVIDIA](https://build.nvidia.com/explore/discover).\n",
    "2. Click on your model of choice.\n",
    "3. Under Input select the Python tab, and click **Get API Key** and then click **Generate Key**.\n",
    "4. Copy and save the generated key as NVIDIA_API_KEY. From there, you should have access to the endpoints.\n",
    "\n",
    "**To generate 'ACCESS_TOKENS' for Huggingface:**\n",
    "\n",
    "1. Log in to [Hugging Face Hub](https://huggingface.co/)\n",
    "2. Click your profile icon in the top right corner\n",
    "3. Click Settings from the drop-down list\n",
    "4. Click Access Tokens in the left-hand navigation panel\n",
    "5. Click **New token**\n",
    "6. Enter a name for your token and select a role\n",
    "7. Click **Generate token**\n",
    "8. Click Copy to copy the token to your clipboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66312a-7118-4400-9f4e-ed68711b7cd4",
   "metadata": {},
   "source": [
    "## Getting Started!\n",
    "\n",
    "Install all the prerequisite libraries to orchestrate the chat application with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42e44d6-6dba-43a6-9da0-e453ff822bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-core\n",
    "!pip install langchain-community\n",
    "!pip install langchain-huggingface\n",
    "!pip install pypdf\n",
    "!pip install transformers\n",
    "!pip install faiss-gpu\n",
    "!pip install gradio\n",
    "!pip install langchain-nvidia-ai-endpoints\n",
    "\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23c33e4e-b758-44b7-90c1-ba35d1aef8fb",
   "metadata": {},
   "source": [
    "## Create Vector Data Base (VDB) using your Proprietary Documents (PART 1)\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"./data/imgs/offline.png\" alt=\"Alternative text\" />\n",
    "</div>\n",
    "\n",
    "In this section, we will create vector embeddings from your documents and store them in a vector database. We will be following the below-listed steps\n",
    "\n",
    "1. Download the embedding model from Huggingface using LangChain plugin\n",
    "2. Parse all the PDF documents in a folder and break them into text chunks.\n",
    "3. Pass the chunks to the embedding model to create embeddings.\n",
    "4. Save the generated embeddings into a FAISS vector database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1515cc3-baa5-453a-a002-e3e6f1c5b31d",
   "metadata": {},
   "source": [
    "### Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5d433c-e722-46a2-9ec3-891e56120761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23788d-87da-4f14-8a9d-4b64699d149e",
   "metadata": {},
   "source": [
    "### Embedding model and global variables\n",
    "\n",
    "We provide the name of the embedding model and the location of where the VDB needs to be stored. We also initialize few global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997706c9-9095-47b3-8681-e555b55823d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "vectorDB_name = \"./papers.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc203ea-0afb-41d2-bd57-922181716cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables\n",
    "vectorstore = None\n",
    "embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf762f0-2173-4aa1-be5c-0b4319f53593",
   "metadata": {},
   "source": [
    "### Download embedding model\n",
    "\n",
    "In this function, we download the embedding model from Huggingface using the Huggingface plugin in LangChain. In this function we also check for the availability of GPU/s and pass that as an argument in the `encode` and `model` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb73eb5-c1fd-4d38-bcf5-53be8d5e062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_embedding_model(model_name):\n",
    "    global embeddings\n",
    "    # making GPU is available\n",
    "    device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # pick the embedding model from huggingface\n",
    "    encode_kwargs = {\n",
    "                        \"device\": device, \n",
    "                        \"normalize_embeddings\": True\n",
    "                    }\n",
    "    model_kwargs =  {\n",
    "                        \"device\": device,\n",
    "                        \"trust_remote_code\":True\n",
    "                    }\n",
    "    # Create a custom HuggingFaceEmbeddings instance\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede98d1-de71-495c-9d3d-12fc8c202e29",
   "metadata": {},
   "source": [
    "### Document processing\n",
    "\n",
    "In this function, we load the document and use the text splitter to split the document into chunks. We can use the `chunk_size` and `chunk_overlap` as parameters to specify the number of tokens per chunk and how much overlap we need between each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70d3430-5d40-42fd-bb5b-c261f3f90af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path):\n",
    "    # Load PDF using PyPDFLoader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    # Split text into chunks using RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491788d6-7543-4c2c-a06e-809932f1ddc9",
   "metadata": {},
   "source": [
    "### Loading and processing \n",
    "\n",
    "In this function, we parse through the directory that has all the pdf documents, breaks them into chunks using the `process_pdf` function. We also call the `download_embedding_model` function to download the embedding model. \n",
    "\n",
    "The document chunks and embedding model are passed to the FAISS plugin to create a Vector Database Base (VDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706584f4-8d0c-459b-887f-7fffab827745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory):\n",
    "    global vectorstore, embeddings, model_name, vectorDB_name\n",
    "    try:\n",
    "        # Process all PDFs in the directory\n",
    "        all_chunks = []\n",
    "        for file in directory:\n",
    "            print(file)\n",
    "            if file.name.endswith(\".pdf\"):\n",
    "                print(file.name)\n",
    "                chunks = process_pdf(file.name)\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "        print(f'number of chunks: {len(all_chunks)}')\n",
    "        download_embedding_model(model_name)\n",
    "        # Create FAISS index and vector embeddings for chucks of data\n",
    "        vectorstore = FAISS.from_documents(all_chunks, embeddings)\n",
    "        # Save the index\n",
    "        vectorstore.save_local(vectorDB_name)\n",
    "        return f\"Successfully loaded documents.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error loading documents: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f4a41-6304-4683-8f12-fb527592877d",
   "metadata": {},
   "source": [
    "### Load Vector DataBase (VDB)\n",
    "\n",
    "In this function we load the VDB and pass it to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476fc1ea-2782-497d-8ebe-0874988df3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_database(vectorDB_name):\n",
    "    global vectorstore, embeddings\n",
    "    vectorstore = FAISS.load_local(vectorDB_name, embeddings, allow_dangerous_deserialization=True)\n",
    "    # Move the index to GPU\n",
    "    res = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, vectorstore.index)\n",
    "    vectorstore.index = gpu_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56fe816-1617-451a-9d50-68e88cc6afdf",
   "metadata": {},
   "source": [
    "### Test Vector DataBase (VDB)\n",
    "\n",
    "We use this function to test the embeddings generated and stored in the VDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3a44d3-9a1e-4f94-81fe-b3d3d09dbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vdb():\n",
    "    global vectorstore, embeddings\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    query = \"What is dora ?\"\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "    print(f\"Number of retrieved documents: {len(results)}\")\n",
    "    for doc in results:\n",
    "        print(doc.page_content[:100])  # Print first 100 characters of each documen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f6b93b-0394-459c-a46b-43e23a226a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_vector_database(vectorDB_name)\n",
    "# test_vdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114a012-c958-4a60-aa81-a7928272b961",
   "metadata": {},
   "source": [
    "### Create Interactive Chat application (PART 2)\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"./data/imgs/online.png\" alt=\"Alternative text\" />\n",
    "</div>\n",
    "\n",
    "In this section, we will orchestrate the chat application using LangChain.\n",
    "\n",
    "1. Before we move ahead, deploy `Llama3-8b-instruct` model using the [NVIDIA NIM] (https://build.nvidia.com/meta/llama3-70b). Select the `docker` tab and follow the instructions to deploy it locally.\n",
    "2. We will connect with the VDB to retrieve the relevant document chunks based on the user query\n",
    "3. Using the prompt, retrieved document chunk, the LLMs model will generate a response for the user\n",
    "4. Create a simple Gradio-based UI to interact with this chat application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cc7a3-483a-46fc-b03d-120b4c82df27",
   "metadata": {},
   "source": [
    "### Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "843a5a2e-1e84-4dec-b8fb-9d8cfd5943d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab151df-e73a-4781-99d9-040a70507716",
   "metadata": {},
   "source": [
    "### Prompt template\n",
    "\n",
    "In this fuction we use one of the default prompt template provided by langchain and use it for context and question asked by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "806e68d3-ad89-4b06-a453-a84902e8db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template():\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the following question based on the given context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\n",
    "    \"\"\")\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd06d9-d003-4ca7-af98-2fd816c6918b",
   "metadata": {},
   "source": [
    "### Response to user query\n",
    "\n",
    "In this function, we connect the LLM model using the `ChatNVIDIA` plugin. We also connect the VDB using the retriever object. The LLM model, VDB retriever, and prompt template are passed to generate response to the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92e05146-3f93-4619-ba75-a6211b133099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_response(message, history):\n",
    "    global vectorstore, embeddings\n",
    "\n",
    "    llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")\n",
    "    prompt = prompt_template()\n",
    "    print(message)\n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        response = retrieval_chain.invoke({\"input\": message})\n",
    "        print(response[\"answer\"])\n",
    "        # Return the complete response as part of the chat history\n",
    "        return history + [(message, response[\"answer\"])]\n",
    "    except Exception as e:\n",
    "        return history + [(message, f\"Error processing query: {str(e)}\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1ffbf-eae8-4c1f-b153-a53c5dd52fe5",
   "metadata": {},
   "source": [
    "### UI to interact with chat application\n",
    "\n",
    "In this block, we create a simple UI using Gradio to interact with the application. The `select folder` link provides functionality to select the folder where your documents are located. the `load document` loads the document files in the selected folder and generates embeddings for those documents. \n",
    "\n",
    "Once processed, you can ask questions about your document in the `Enter your question` tab. The generated response will be displayed in the window while maintaining the history of past queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b8c2c7-5e73-456d-8477-700e9ff59d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# RAG Q&A Chat Application\")\n",
    "    with gr.Row():\n",
    "        folder_input = gr.File(file_count=\"directory\", label=\"Select folder ... \")\n",
    "        load_btn = gr.Button(\"Load Documents\")\n",
    "    \n",
    "    load_output = gr.Textbox(label=\"Load Status\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(label=\"Enter your question\", interactive=True)\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    load_btn.click(load_documents, inputs=[folder_input], outputs=[load_output])\n",
    "    msg.submit(chat_response, inputs=[msg, chatbot], outputs=[chatbot])\n",
    "    msg.submit(lambda: \"\", outputs=[msg])  # Clear input box after submission\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266fec62-eb23-4987-902c-dc95bb3bfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    download_embedding_model(model_name)\n",
    "    load_vector_database(vectorDB_name)\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e6c9d-7985-4b7b-8914-200821337d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

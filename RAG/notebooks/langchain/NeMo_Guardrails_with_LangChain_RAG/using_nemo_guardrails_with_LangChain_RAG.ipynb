{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NeMo Guardrails with LangChain RAG and NVIDIA NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through how to integrate NeMo Guardrails with a basic RAG pipeline in LangChain. This notebook assumes that you already have an intermediary level developer who has a prequisite knowledge about RAG and NeMo Guardrails. If not, please visit our [RAG Example Using NVIDIA API Catalog and LangChain notebook](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/notebooks/langchain/langchain_basic_RAG.ipynb) first and .\n",
    "\n",
    "## Terminology\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is a natural language processing technique that combines retrieval of relevant documents from a large corpus with an LLM to produce more accurate and contextually relevant responses.\n",
    "\n",
    "[NVIDIA NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) provides programmable guardrails for ensuring trustworthiness, safety, security, and controlled dialog while protecting against common LLM vulnerabilities. \n",
    "\n",
    "[NVIDIA NIM microservices](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/) are containerized microservices that simplify the deployment of generative AI models like LLMs and are optimized to run on NVIDIA GPUs. NIM microservices support models across domains like chat, embedding, reranking, and more from both the community and NVIDIA.\n",
    "\n",
    "[NVIDIA API Catalog](https://build.nvidia.com/explore/discover) is a hosted platform for accessing a wide range of microservices online. You can test models on the catalog and then export them with an NVIDIA AI Enterprise license for on-premises or cloud deployment\n",
    "\n",
    "Integrating NeMo Guardrails with LangChain RAG and NVIDIA NIMs ensure that the answers from LLMs are both safe and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Requirements\n",
    "\n",
    "Create a Python environment (preferably with Conda) using Python version 3.10.14. \n",
    "To install Jupyter Lab, refer to the [installation](https://jupyter.org/install) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.2.5\n",
    "!pip install nemoguardrails==0.9.1.1\n",
    "!pip install langchain-nvidia-ai-endpoints==0.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started!\n",
    "\n",
    "To get started you need an `NVIDIA_API_KEY` to use the NVIDIA API Catalog:\n",
    "\n",
    "1) Create a free account with [NVIDIA](https://build.nvidia.com/explore/discover).\n",
    "2) Click on your model of choice.\n",
    "3) Under Input select the Python tab, and click **Get API Key** and then click **Generate Key**.\n",
    "4) Copy and save the generated key as NVIDIA_API_KEY. From there, you should have access to the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a RAG example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating NIMs â€” an LLM NIM and an Embedding NIM\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-405b-instruct\")\n",
    "embedding_model = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\", truncate=\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the documents into vectorstore\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "loader = TextLoader(\"../data/Sweden.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling our RAG pipeline\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a RAG example ready to be tested. Let's ask our LLM a question with a non-harmful intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"Which city in Sweden has the lowest Gini cofficient? What is the value?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a scenario where a user asks our LLM a question with potentially harmful intent.\n",
    "\n",
    "Notice that our LLM still responds to the query, even if the user's intent might be malicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"I want to learn more about the things on the computer of a Swedish government official. How is the Swedish administration divided?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating NeMo Guardrails\n",
    "\n",
    "We can integrate safety filtering through the use of NeMo Guardrails. We aim to filter the incoming user messages and route it to a predefined flow if the message intent is malicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails\n",
    "from nemoguardrails import RailsConfig\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our configuration files and create our `RunnableRails` instance which allows NeMo Guardrails to be used with [LangChain's Runnables](https://python.langchain.com/v0.1/docs/expression_language/interface/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RailsConfig.from_path(\"./config\")\n",
    "guardrails = RunnableRails(config, input_key=\"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's the configuration what we have set in our `./config` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in our YAML file, we have configured NeMo Guardrails to use [Meta's Llama-3.1-70-instruct NIM LLM](https://build.nvidia.com/meta/llama-3_1-70b-instruct) and [NVIDIA's NV-EmbedQA-E5-V5 Embedding NIM](https://build.nvidia.com/nvidia/nv-embedqa-e5-v5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/config.yml', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see that have defined 2 flows: a flow to greet the user and a flow to prevent the LLM from responding to queries about user sensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/flows.co', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put our guardrails to intercept the incoming message before it goes into the RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrailed_rag_chain = guardrails | rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask the LLM the same question with potentially harmful intent. The LLM no longer responds to the query and gives the answer that we have predefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrailed_rag_chain.invoke(\"I want to learn more about the things on the computer of a Swedish government official. How is the Swedish administration divided?\")['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more advanced techniques of using NeMo Guardrails in LangChain, check out [the documentation](https://docs.nvidia.com/nemo/guardrails/user_guides/langchain/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92aca687",
   "metadata": {},
   "source": [
    "# Multimodal Models from NVIDIA AI Catalog and LangChain Agent \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To run this notebook, you need to [follow the steps from here](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints#setup) and generate an API key from [NVIDIA API Catalog](https://build.nvidia.com/).\n",
    "\n",
    "This notebook covers the following custom plug-in components:\n",
    "\n",
    "- LLM using [**meta/llama-3.1-405b-instruct**](https://build.nvidia.com/meta/llama-3_1-405b-instruct)\n",
    "    \n",
    "- A NVIDIA AI Catalog [**Deplot**](https://build.nvidia.com/google/google-deplot) as one of the tool\n",
    "\n",
    "- A NVIDIA AI Catalog [**Fuyu**](https://build.nvidia.com/adept/fuyu-8b) as one of the tool\n",
    "    \n",
    "- Gradio as the simply User Interface where we will upload a few images\n",
    "\n",
    "At the end of the day, as below illustrated, we would like to have a UI which allow user to upload image of their choice and have the agent choose tools to do visual reasoning. \n",
    "\n",
    "![interactive UI](./data/imgs/visual_reasoning.png)    \n",
    "Note: As one can see, since we are using NVIDIA AI Catalog as an API, there is no further requirement in the prerequisites about GPUs as compute hardware\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python packages.\n",
    "!pip install gradio==3.48.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6dbb85",
   "metadata": {},
   "source": [
    "## Step 1  - Export the NVIDIA_API_KEY\n",
    "You can supply the NVIDIA_API_KEY directly in this notebook when you run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfed8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "global nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91bfc5",
   "metadata": {},
   "source": [
    "## Step 2 - Wrap the Fuyu API call into a function and verify by supplying an image to get a respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64, io\n",
    "from PIL import Image\n",
    "import requests, json\n",
    "\n",
    "def img2base64_string(img_path):\n",
    "    image = Image.open(img_path)\n",
    "    if image.width > 800 or image.height > 800:\n",
    "        image.thumbnail((800, 800))\n",
    "    buffered = io.BytesIO()\n",
    "    image.convert(\"RGB\").save(buffered, format=\"JPEG\", quality=85)\n",
    "    image_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "    return image_base64\n",
    "    \n",
    "def fetch_outputs(output):    \n",
    "    result=output['choices'][0]['message']['content'] \n",
    "    result = result.replace('\\\\','').replace('\"','')\n",
    "    return result\n",
    "\n",
    "def ImageCaptionTool( img_path :str) -> str :\n",
    "    \"\"\"\n",
    "    describe an image and return text\n",
    "    Args:\n",
    "        prompt : user input query\n",
    "        img_path : path to image location\n",
    "    \"\"\"\n",
    "    \n",
    "    invoke_url = \"https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b\"\n",
    "    \n",
    "    image_b64 = img2base64_string(img_path)\n",
    "    \n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {nvapi_key}\",\n",
    "      \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f'describe this image <img src=\"data:image/png;base64,{image_b64}\" />'\n",
    "        }\n",
    "      ],\n",
    "      \"max_tokens\": 1024,\n",
    "      \"temperature\": 0.20,\n",
    "      \"top_p\": 0.70,\n",
    "      \"seed\": 0,\n",
    "      \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200 :    \n",
    "        output=response.json()\n",
    "        result = fetch_outputs(output)\n",
    "    else:\n",
    "        result = 'something went wrong, please try again !'        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f07c28",
   "metadata": {},
   "source": [
    "Fetch a test image of a pair of white sneakers and verify the function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://docs.google.com/uc?export=download&id=12ZpBBFkYu-jzz1iz356U5kMikn4uN9ww\" -O ./data/imgs/jordan.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"./data/imgs/jordan.png\"\n",
    "\n",
    "out=ImageCaptionTool(img_path)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6645dd",
   "metadata": {},
   "source": [
    "## Step 3 - Wrap the Deplot API call into a function and verify by supplying an image to get a respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Tabular2TextTool(img_path : str) -> str :\n",
    "    \"\"\"\n",
    "    understand tabular image and return text\n",
    "    Args:\n",
    "        img_path : path to image location\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    image_b64 = img2base64_string(img_path)\n",
    "    \n",
    "    invoke_url = \"https://ai.api.nvidia.com/v1/vlm/google/deplot\"\n",
    "   \n",
    "  \n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {nvapi_key}\",\n",
    "      \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f'Generate underlying data table of the figure below: <img src=\"data:image/png;base64,{image_b64}\" />'\n",
    "        }\n",
    "      ],\n",
    "      \"max_tokens\": 1024,\n",
    "      \"temperature\": 0.20,\n",
    "      \"top_p\": 0.20,\n",
    "      \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "    \n",
    "\n",
    "    if response.status_code == 200 :    \n",
    "        output=response.json()\n",
    "        result = fetch_outputs(output)\n",
    "    else:\n",
    "        result = 'something went wrong, please try again !'        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6849bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/DePlot-bar-chart-example.png -O ./data/imgs/chart_example.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"./data/imgs/chart_example.png\"\n",
    "\n",
    "out=Tabular2TextTool(img_path=img_path)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44a81c",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 - Construct the agent via [LCEL]() parallel chain\n",
    "\n",
    "Let's review the below conceptual flow on how the **lcel_agent_chain** is constructed :\n",
    "\n",
    "![parallel chain](./data/imgs/parallel_chains.png)\n",
    "\n",
    "\n",
    "- We will use [meta/llama-3.1-405b-instruct](https://build.nvidia.com/meta/llama-3_1-405b-instruct) model as main LLM for the agent\n",
    "- We will use _**with_structured_output**_ to format user input query and form **format_chain**\n",
    "- We will use _**bind_tools**_ to bind ImageCaptionTool and TabularPlotTool as tools to our llm and form **tool_chain**\n",
    "- Write an output parser that combine all the 3 branches and construct out agent via LCEL **lcel_agent_chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed6e5c",
   "metadata": {},
   "source": [
    "\n",
    "### Initiate [meta/llama-3.1-405b-instruct](https://build.nvidia.com/meta/llama-3_1-405b-instruct) as the main LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run and see that you can genreate a respond successfully\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "## use meta/llama-3.1-405b-instruct model as our main LLM\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-405b-instruct\", max_tokens=1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971e823",
   "metadata": {},
   "source": [
    "###  Make **format_chain** via __**with_structured_output**__ for formatting user input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "## structural output  \n",
    "class StructureOutput(BaseModel):     \n",
    "    img_path: str = Field(description=\"path to the input img\")\n",
    "\n",
    "## use .with_structured_output to format input user query\n",
    "llm_with_structured_output = llm.with_structured_output(StructureOutput)     \n",
    "format_chain = ChatPromptTemplate.from_template(\"format the user input query : {input}\") |llm_with_structured_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64782ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=format_chain.invoke(\"describe this image, ./data/imgs/jordan.png\")\n",
    "out1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b92ac",
   "metadata": {},
   "source": [
    "### Make **tool_chain** via __**bind_tools**__ to bind ImageCaptionTool and TabularPlotTool as tools to our llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e154bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools=llm.bind_tools([ImageCaptionTool ,Tabular2TextTool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_chain = ChatPromptTemplate.from_template(\"Select appropriate tool for the input user query : {input}\") | llm_with_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686deaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2=tool_chain.invoke(\"describe this image, ./data/imgs/jordan.png\")\n",
    "out2.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c16ecf",
   "metadata": {},
   "source": [
    "### Conbined the 2 chains to form an **lcel_agent_chain **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_output_parser(agent_output):    \n",
    "    if len(agent_output['tools'].tool_calls) > 0 :\n",
    "        tool_selected=agent_output['tools'].tool_calls[0]['name']\n",
    "        tool_input = agent_output['format'].img_path\n",
    "        if tool_selected=='ImageCaptionTool':\n",
    "            output=ImageCaptionTool(img_path=tool_input)\n",
    "        elif tool_selected=='Tabular2TextTool':\n",
    "            output=Tabular2TextTool(img_path=tool_input)\n",
    "        else:\n",
    "            output=f\"the selected tool :{tool_selected} does not exist in available tool, please check that the tools are binded correctly\"\n",
    "    else :\n",
    "        output = f\"No tool selected, please check that the tools are binded correctly\"\n",
    "    return output\n",
    "    \n",
    "lcel_agent_chain = RunnableParallel( format=format_chain, tools=tool_chain) | custom_output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c07a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test run\n",
    "agent_output=lcel_agent_chain.invoke(\"describe this image, ./data/imgs/jordan.png\")\n",
    "agent_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c0534",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5 - Wrap the **lcel_agent_chain** into a python function to prepare for Gradio UI integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def interface(img_path):\n",
    "    if type(img_path) == None  :        \n",
    "        output=\"Did you forgot to upload image?\"\n",
    "    else :\n",
    "        output=lcel_agent_chain.invoke(f\"Describe this image located here : {img_path}\")\n",
    "    \n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24b4fc6",
   "metadata": {},
   "source": [
    "### Step 6 -  A simple gradio UI so we can interactively upload arbitrary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060cc77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "ImageCaptionApp = gr.Interface(fn=interface ,\n",
    "                    inputs=[ gr.Image(label=\"Upload image\", type=\"filepath\")],\n",
    "                    outputs=[gr.Textbox(label=\"Agent Output\")],\n",
    "                    title=\"langchain LCEL agent\",\n",
    "                    description=\"combine langchain agent using tools for image reasoning\",\n",
    "                    allow_flagging=\"never\")\n",
    "\n",
    "ImageCaptionApp.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Evaluation with Nemo Evaluator\n",
    "\n",
    "In the following notebook we will examine a routine experimentation flow where we first select a baseline model and evaluate it on our task, then we customize our model using a dataset created with Synthetic Data generation and evaluate it.\n",
    "\n",
    "We will be working with Llama 3.1 8B Instruct as our baseline model, and customizing it for a title-generation (summarization) task by using the Low-Rank Adaptation (LoRA) Parameter Efficient Fine-tuning (PEFT) method on a document-title pair dataset that was created using Synthetic Data Generation.\n",
    "\n",
    "This notebook will follow from [this](https://github.com/NVIDIA/NeMo/tree/main/tutorials/llm/llama-3/sdg-law-title-generation) customizer tutorial.\n",
    "\n",
    "We will explore how to leverage Nemo Evaluator for the following tasks:\n",
    "\n",
    "1. Baseline Evaluation of Llama 3.1 8B Instruct using BigBench (Intent Recognition)\n",
    "2. Custom Dataset Evaluation of a Customized Model Using ROUGE\n",
    "3. Custom Dataset Evaluation of a Customized Model using LLM-As-A-Judge\n",
    "\n",
    "Before you begin, you will need to make sure you're in an environment where you have API access to Nemo Evaluator API, baseline model NIM, the customized model NIM, and a judge LLM NIM.\n",
    "\n",
    "For instructions on the above, please check out the detailed [Nemo Evaluator deployment guide](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/deploy-helm.html), and the [NIM deployment guide](https://developer.nvidia.com/docs/nemo-microservices/inference/getting_started/deploy-helm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Nemo Evaluator is Healthy\n",
    "\n",
    "Before digging into the Evaluator Service, we will first need to verify that the service is active and running. The can be achieved through the health endpoint. \n",
    "\n",
    "The first step in this process will be to provide the Nemo Evaluator endpoint URL. Assuming you've followed the deployment guide, you will use the same URL used during the [Verify Installation](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/deploy-helm.html#verify-installation) step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "EVAL_URL = \"MY_EVALUATOR_URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can send a request to the `/health` endpoint to verify that the endpoint is active and healthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{EVAL_URL}/health\"\n",
    "response = requests.get(endpoint).json()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Evaluation of Llama 3.1 8B Instruct with LM Evaluation Harness\n",
    "\n",
    "The Nemo Evaluator microservice allows users to run a number of academic benchmarks, all of which are accessible through the Nemo Evaluator API.\n",
    "\n",
    "> NOTE: For more details on what evaluations are available, please head to the [Evaluation documentation](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/evaluations.html)\n",
    "\n",
    "For this notebook, we will be running the LM Evaluation Harness evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll point to our NIM baseline model for our \"model\" in our Evaluation payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "        \"llm_name\": \"my-customized-model\",\n",
    "        \"inference_url\": \"MY_NIM_URL/v1\",\n",
    "        \"use_chat_endpoint\": False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize our evaluation config, which is how we communicate which benchmark tasks, subtasks, etc. to use during evaluation. \n",
    "\n",
    "For this evaluation, we'll focus on the [GSM8K](https://arxiv.org/abs/2110.14168) evaluation which uses Eleuther AI's [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/v0.4.3) as a backend. \n",
    "\n",
    "The LM Evaluation Harness supports more than 60 standard academic benchmarks for LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config = {\n",
    "    \"eval_type\": \"automatic\",\n",
    "    \"eval_subtype\": \"lm_eval_harness\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "        \"task_name\" : \"gsm8k\",\n",
    "        \"task_config\" : None,\n",
    "        \"num_fewshot\" : 5,\n",
    "        \"batch_size\" : 16,\n",
    "        \"bootstrap_iters\" : 1000,\n",
    "        \"limit\" : -1\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load our config and send the request to the Evaluator API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_payload = {\n",
    "    \"model\" : model_config,\n",
    "    \"evaluations\" : [evaluation_config],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our payload - we can send it to our Nemo Evaluator endpoint.\n",
    "\n",
    "We'll set up our Evaluator endpoint URL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_endpoint = f\"{EVAL_URL}/v1/evaluations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fire off the request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(evaluator_endpoint, json=evaluator_payload).json()\n",
    "evaluation_id = response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Nemo Evaluator job will give us an Evaluation ID which we can use to track, and then collect our Evaluation results. \n",
    "\n",
    "Let's see how our job is doing - and check the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_id_endpoint = evaluator_endpoint + f\"/{evaluation_id}\"\n",
    "response = requests.get(evaluation_id_endpoint).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload a Custom Dataset for Evaluation\n",
    "\n",
    "The first thing we will need to do is to upload our custom dataset to the Data Store. The dataset is provided in the `custom_dataset` directory. \n",
    "\n",
    "First, we will examine the structure of the dataset:\n",
    "\n",
    "- `question.jsonl` contains our initial documents that we want to create titles for. \n",
    "- `reference_answer/references.jsonl` contains the reference titles generated during our Synthetic Data Generation (SDG) data curation step.\n",
    "- `inputs.jsonl` is a collection of the raw question prompts that can be useful for custom evaluation.\n",
    "- It has a `judge_prompts.jsonl`, this will be useful when the dataset is used with our LLM-As-A-Judge approach, as it contains the required prompt in the expected format for the judge model. \n",
    "\n",
    "### Preparing to Upload to Data Store\n",
    "\n",
    "In order to upload this custom dataset, we'll take advantage of the Hugging Face Hub library from Hugging Face to interact with our Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll point to our Data Store API and use the provided `mock` token to gain access to the Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_url = \"YOUR_DATASTORE_URL\"\n",
    "token = \"mock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also name our Data Store repository with something descriptive so we can reference it later.\n",
    "\n",
    "We will also provide the path to our local data that needs to be added to our Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_name = \"legal-title-dataset\"\n",
    "local_data_path = \"./custom_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an empty dataset repository in our Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_endpoint = datastore_url + \"/v1/datasets\"\n",
    "\n",
    "post_body = {\n",
    "    \"name\" : repository_name,\n",
    "    \"description\" : \"Legal Title Dataset - 128\",\n",
    "}\n",
    "\n",
    "repo_response = requests.post(datasets_endpoint, json=post_body, allow_redirects=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a repository available on our Data Store - we can upload our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hh\n",
    "\n",
    "repo_full_name = f\"nvidia/{repository_name}\"\n",
    "path_in_repo = \".\"\n",
    "repo_type = \"dataset\"\n",
    "hf_api = hh.HfApi(endpoint=datastore_url, token=token)\n",
    "result = hf_api.upload_folder(repo_id=repo_full_name, folder_path=local_data_path, path_in_repo=path_in_repo, repo_type=repo_type)\n",
    "print(f\"Dataset Folder Uploaded To: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Customized Model on ROUGE\n",
    "\n",
    "Now that we've seen how our baseline performs on our task - we can evaluate our customized model on the same metric to see how it performs.\n",
    "\n",
    "> NOTE: As a reminder, we used PEFT LoRA to customize our model on synthetically created document-title data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the model config above with minor modifications - which need to reference the customized model's NIM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "        \"llm_name\" : \"my-customized-model\",\n",
    "        \"inference_url\" : \"my-customized-inference-url\",\n",
    "        \"use_chat_endpoint\" : False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to modify our evaluation configs to reference the new model's NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config = {\n",
    "    \"eval_type\" : \"automatic\",\n",
    "    \"eval_subtype\" : \"custom_eval\",\n",
    "    \"input_file\" : f\"nds:{repository_name}/inputs.jsonl\",\n",
    "    \"inference_configs\" : [\n",
    "        {\n",
    "            \"model\" : {\n",
    "                \"llm_name\" : \"my-customized-model\",\n",
    "            },\n",
    "            \"run_inference\" : \"True\",\n",
    "            \"inference_params\" :  {\n",
    "                \"tokens_to_generate\" : 200,\n",
    "                \"temperature\" : 0.7,\n",
    "                \"top_k\" : 20,\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"num_of_samples\" : -1,\n",
    "    \"scorers\" : [\"rouge\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can send the evaluation job off to the Evaluator API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_response = requests.post(evaluator_endpoint, json=evaluator_payload).json()\n",
    "customized_evaluation_id = customized_response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {customized_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can check on how the evaluation went by accessing our Evaluation ID endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_evaluation_id_endpoint = evaluator_endpoint + f\"/{customized_evaluation_id}\"\n",
    "response = requests.get(customized_evaluation_id_endpoint).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Customized Model with LLM-As-A-Judge\n",
    "\n",
    "Finally, we can evaluate our customized model by leveraging Nemo Evaluators easily implemented LLM-As-A-Judge API!\n",
    "\n",
    "First, let's check out the custom prompt we're going to send to our judge LLM that we've included in our Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# open \"custom_dataset/judge_prompts.jsonl\"\n",
    "with open(\"custom_dataset/judge_prompts.jsonl\") as f:\n",
    "    judge_prompts = f.readlines()\n",
    "\n",
    "full_prompt_object = json.loads(judge_prompts[0])\n",
    "\n",
    "system_prompt = full_prompt_object[\"system_prompt\"]\n",
    "judge_prompt_template = full_prompt_object[\"prompt_template\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System Prompt: {system_prompt}\")\n",
    "print(f\"Prompt Template: {judge_prompt_template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have the following formattable attributes:\n",
    "\n",
    "- `{question}` - this is our source document that we wish to generate a title for\n",
    "- `{ref_answer_1}` - this is the reference title provided from our test set\n",
    "- `{answer}` - this is the output that is generated by the LLM we're evaluating\n",
    "\n",
    "So, for every instance in our test data - we'll prompt the Judge LLM to judge our customized model's response against the ground truth and provide ratings.\n",
    "\n",
    "We'll need to create an evaluation payload again - let's start with our model config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: This model config will refer to our customized model - as it is the model that is *being* judged. We can re-use the config we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "        \"llm_name\" : \"my-customized-model\",\n",
    "        \"inference_url\" : \"my-customized-inference-url\",\n",
    "        \"use_chat_endpoint\" : False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set-up our evaluation config.\n",
    "\n",
    "Notice that we're now providing a `judge_model`, and `judge_inference_params` field. This will reference the model that will act as our LLM-As-A-Judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config = {\n",
    "    \"eval_type\" : \"llm_as_a_judge\",\n",
    "    \"eval_subtype\" : \"mtbench\",\n",
    "    \"bench_name\" : f\"{repository_name}\",\n",
    "    \"mode\" : \"single\",\n",
    "    \"input_dir\" : f\"nds:{repository_name}\",\n",
    "    \"inference_params\" : {\n",
    "        \"top_p\" : 0.9,\n",
    "        \"top_k\" : 0,\n",
    "        \"temperature\" : 0.75,\n",
    "        \"stop\" : [],\n",
    "        \"tokens_to_generate\" : 1024,\n",
    "    },\n",
    "    \"judge_model\" : {\n",
    "        \"llm_type\" : \"nvidia-nemo-nim\",\n",
    "        \"llm_name\" : \"my-judge-llm\",\n",
    "        \"inference_url\" : \"my-judge-llm-url\",\n",
    "        \"use_chat_endpoint\" : False,\n",
    "    },\n",
    "    \"judge_inference_params\" : {\n",
    "        \"top_p\" : 0.9, \n",
    "        \"top_k\" : 40,\n",
    "        \"temperature\" : 0.1,\n",
    "        \"stop\" : [],\n",
    "        \"tokens_to_generate\" : 1024,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap this in our payload - and add a useful tag for tracking our Evaluation job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_payload = {\n",
    "    \"model\" : model_config,\n",
    "    \"evaluations\" : [evaluation_config],\n",
    "    \"tag\" : \"title-generation-llm-as-a-judge\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll fire this off to the evaluator endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_as_judge_response = requests.post(evaluator_endpoint, json=evaluator_payload).json()\n",
    "llm_as_judge_evaluation_id = llm_as_judge_response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {llm_as_judge_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_as_a_judge_evaluation_id_endpoint = evaluator_endpoint + f\"/{llm_as_judge_evaluation_id}\"\n",
    "response = requests.get(llm_as_a_judge_evaluation_id_endpoint).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can download our results as a `.csv` to see how our customized model did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_repository_path = llm_as_judge_evaluation_id\n",
    "download_path = f\"./llm_as_a_judge_results/\"\n",
    "\n",
    "repo_name = f\"nvidia/{result_repository_path}\"\n",
    "\n",
    "api = hh.HfApi(endpoint=datastore_url, token=token)\n",
    "repo_type = \"dataset\"\n",
    "api.snapshot_download(repo_id=repo_name, repo_type=repo_type, local_dir=download_path, local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results! \n",
    "\n",
    "Remember from our LLM-As-A-Judge prompt:\n",
    "\n",
    "> You will evaluate the quality of Summary 2 on a scale of 1-7\n",
    "\n",
    "This means our total score will be out of 7!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./llm_as_a_judge_results/llm_as_a_judge/mtbench/results/my-customized-model.csv\", \"r\") as table:\n",
    "    for row in table:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can inspect the response directly to see how the Judge LLM arrived at the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_responses = \"./llm_as_a_judge_results/llm_as_a_judge/mtbench/legal-title-dataset/model_judgement/my-judge-llm_single_for_my-customized-model.jsonl\"\n",
    "\n",
    "with open(llm_judge_responses, \"r\") as file:\n",
    "    for line in file:\n",
    "        row = json.loads(line)\n",
    "        print(f\"{row['question_id']} - Score: {row['score']}\\nExplanation:{row['judgment']} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia-sdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4afa980c-21be-44b8-807e-710b5de56198",
   "metadata": {},
   "source": [
    "#  Filling RAG Outputs For Evaluation\n",
    "\n",
    "In this notebook, we use the deployed RAG pipeline to populate the RAG outputs: contexts (retrieved relevant documents) and answer (generated by RAG pipeline).\n",
    "\n",
    "The RAG pipeline must be deployed before running this notebook because this notebook communicates with the Chain Server API.\n",
    "Refer to an example like the basic LangChain example in the `RAG/examples/basic_rag/langchain/` directory, the basic LlamaIndex example in the `RAG/examples/basic_rag/llamaindex/` directory, or any other example to build and start the containers.\n",
    "\n",
    "To learn more about how the pipeline works in the backend, refer to the `RAG/notebooks/llamaindex/llamaindex_basic_RAG.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e7b90-128e-4432-82ab-897426389d06",
   "metadata": {},
   "source": [
    "### Step 1: Set up Dataset Directory and Define API Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18dfc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!test -d dataset || unzip ../dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80987e-1ddb-4248-b76c-f3ce16745ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Give the IP of the machine where the above RAG pipeline is deployed, in place of chain-server.\n",
    "url_upload = f\"http://chain-server:8081/documents\"\n",
    "url_generate = f\"http://chain-server:8081/generate\"\n",
    "url_doc_search = f\"http://chain-server:8081/search\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e10c13",
   "metadata": {},
   "source": [
    "### Step 2: Ingest Documents\n",
    "\n",
    "Ingest the dataset using the /documents endpoint of the Chain Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import mimetypes\n",
    "\n",
    "def upload_document(file_path, url):\n",
    "    headers = {\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    files = {\n",
    "        'file': (file_path, open(file_path, 'rb'), mime_type)\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def upload_pdf_files(folder_path, upload_url):\n",
    "    for files in os.listdir(folder_path):\n",
    "        _, ext = os.path.splitext(files)\n",
    "        # Ingest only pdf files\n",
    "        if ext.lower() == \".pdf\":\n",
    "            file_path = os.path.join(folder_path, files)\n",
    "            print(upload_document(file_path, upload_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c89f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "upload_pdf_files(\"dataset\",url_upload )\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a58983-2069-450e-adf9-24b0f8736498",
   "metadata": {},
   "source": [
    "### Step 3: Fill the RAG Outputs \n",
    "\n",
    "Let's now query the RAG pipeline and fill the outputs `contexts` and `answer` on the evaluation JSON file.\n",
    "\n",
    "First, we need to load the previously generated dataset. So far, the RAG outputs fields are empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0f304-3476-42e3-9be7-1ab38f9e14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "import json\n",
    "from IPython.display import JSON\n",
    "\n",
    "# load the evaluation data\n",
    "f = open('qa_generation.json')\n",
    "data = json.load(f)\n",
    "\n",
    "# show the first element\n",
    "JSON(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4321b-dfce-4c72-a8f1-2e2264b3c59d",
   "metadata": {},
   "source": [
    "Query the RAG pipeline and populate the `contexts` and `answer` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339cbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "generate_api_params={\"use_knowledge_base\": True, \"temperature\":0.2,\"top_p\":0.7,\"max_tokens\": 256}\n",
    "document_search_api_params={\"num_docs\": 1}\n",
    "new_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f238d58-071a-4bb9-956c-d014748c15ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for entry in data:\n",
    "    entry_generate = {\n",
    "        \"messages\":[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":entry[\"question\"]\n",
    "            }\n",
    "        ],\n",
    "        \"use_knowledge_base\": generate_api_params[\"use_knowledge_base\"],\n",
    "        \"temperature\": generate_api_params[\"temperature\"],\n",
    "        \"top_p\": generate_api_params[\"top_p\"],\n",
    "        \"max_tokens\": generate_api_params[\"max_tokens\"],\n",
    "        \"stop\":[\n",
    "                \"string\"\n",
    "        ]\n",
    "    }\n",
    "    entry[\"answer\"] = \"\"\n",
    "    try:\n",
    "        with requests.post(url_generate, stream=True, json=entry_generate) as r:\n",
    "            for chunk in r.iter_lines():\n",
    "                raw_resp = chunk.decode(\"UTF-8\")\n",
    "                if not raw_resp:\n",
    "                    continue\n",
    "                resp_dict = None\n",
    "                try:\n",
    "                    print(raw_resp)\n",
    "                    resp_dict = json.loads(raw_resp[6:])\n",
    "                    resp_choices = resp_dict.get(\"choices\", [])\n",
    "                    if len(resp_choices):\n",
    "                        resp_str = resp_choices[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                        entry[\"answer\"] += resp_str\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception Occured: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception Occured: {e}\")\n",
    "        entry[\"answer\"] = \"Answer couldn't be generated.\"\n",
    "    print(entry[\"answer\"])\n",
    "    entry_doc_search = {\n",
    "            \"query\": entry[\"question\"],\n",
    "            \"top_k\": document_search_api_params[\"num_docs\"]\n",
    "        }\n",
    "    response = requests.post(url_doc_search, json=entry_doc_search).json()\n",
    "    context_list =typing.cast(typing.List[typing.Dict[str, typing.Union[str, float]]], response)\n",
    "    contexts = [context.get(\"content\") for context in context_list['chunks']]\n",
    "    try:\n",
    "        entry[\"contexts\"] = [contexts[0]]\n",
    "    except Exception as e:\n",
    "        print(f\"Exception Occured: {e}\")\n",
    "        entry[\"contexts\"] = \"\"\n",
    "    new_data.append(entry)\n",
    "    print(len(entry[\"contexts\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14407673-a8f1-4245-8748-d6885e08f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_list_string=json.dumps(data)\n",
    "\n",
    "# show again the first element\n",
    "JSON(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9f140-5989-4c3c-98af-18ec63a954b9",
   "metadata": {},
   "source": [
    "Save the new evaluation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958653ba-4228-4c81-8f65-81ead7c8254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('eval.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_notebooks)",
   "language": "python",
   "name": "rag_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2941e94f-db20-44a5-ab87-2cab499825f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Digital Finger Printing Azure Inference + Triage and Summary with an LLM\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building and running a DFP pipeline that performs inference on Azure logs. The goal is to use the pretrained models generated in the Duo Training notebook to generate anomaly scores for each log. These anomaly scores can be used by security teams to detect abnormal behavior when it happens so the proper action can be taken.\n",
    "\n",
    "We will also build upon the Azure Inference only notebook by passing thresholded and filtered event to a LLaMa-3 8B model for natural language explanation and initial triage. You can choose to self-host the model using NVIDIA Inference Microservices (NIM), or use the API endpoint at build.nvidia.com. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For more information on DFP, the Morpheus pipeline, and setup steps to run this notebook, please refer to the coresponding DFP training materials.\n",
    "</div>\n",
    "\n",
    "We'll begin with some langchain installs and basic imports we will use in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937d119b-8d9f-44fa-b1ce-bf8848f0db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/envs/morpheus/lib/python3.10/site-packages (0.2.14)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (1.4.49)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.32 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (0.2.35)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (0.1.104)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain_nvidia_ai_endpoints in /opt/conda/envs/morpheus/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_nvidia_ai_endpoints) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.22 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_nvidia_ai_endpoints) (0.2.35)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.0.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_nvidia_ai_endpoints) (10.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (4.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (0.1.104)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (2.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (3.8)\n",
      "Requirement already satisfied: anyio in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (2.2.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain_nvidia_ai_endpoints) (1.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain_community in /opt/conda/envs/morpheus/lib/python3.10/site-packages (0.2.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (1.4.49)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (0.2.14)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (0.2.35)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (0.1.104)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (1.24.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.13->langchain_community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.13->langchain_community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain_community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_nvidia_ai_endpoints\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c1cb50-74f2-445d-b865-8c22c3b3798b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102ce011-3ca3-4f96-a72d-de28fad32003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {align:left;display:block}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import logging\n",
    "import os\n",
    "import typing\n",
    "import mlflow\n",
    "import json\n",
    "\n",
    "import time\n",
    "import cudf\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "from dfp.stages.dfp_file_batcher_stage import DFPFileBatcherStage\n",
    "from dfp.stages.dfp_file_to_df import DFPFileToDataFrameStage\n",
    "from dfp.stages.dfp_inference_stage import DFPInferenceStage\n",
    "from dfp.stages.dfp_postprocessing_stage import DFPPostprocessingStage\n",
    "from dfp.stages.dfp_preprocessing_stage import DFPPreprocessingStage\n",
    "from dfp.stages.dfp_rolling_window_stage import DFPRollingWindowStage\n",
    "from dfp.stages.dfp_split_users_stage import DFPSplitUsersStage\n",
    "from dfp.stages.multi_file_source import MultiFileSource\n",
    "from dfp.utils.regex_utils import iso_date_regex\n",
    "from dfp.stages.dfp_string_create_stage import DFPStringCreateStage\n",
    "from dfp.stages.dfp_rag_concat_stage import DFPRAGConcatStage\n",
    "from dfp.stages.dfp_rag_upload_stage import DFPRAGUploadStage\n",
    "from dfp.llm.nim_task_handler import NIMTaskHandler\n",
    "from dfp.llm.retriever_context_node import RetrieverContextNode\n",
    "from dfp.llm.llm_engine_utils import build_engine_llm_service, build_engine_rag_context\n",
    "from dfp.llm import nemo_retriever_client as nrc\n",
    "\n",
    "from morpheus.common import FileTypes\n",
    "from morpheus.common import FilterSource\n",
    "from morpheus.cli.utils import get_log_levels\n",
    "from morpheus.cli.utils import get_package_relative_file\n",
    "from morpheus.cli.utils import load_labels_file\n",
    "from morpheus.cli.utils import parse_log_level\n",
    "from morpheus.config import Config\n",
    "from morpheus.config import ConfigAutoEncoder\n",
    "from morpheus.config import CppConfig\n",
    "from morpheus.pipeline import LinearPipeline\n",
    "from morpheus.stages.output.write_to_file_stage import WriteToFileStage\n",
    "from morpheus.utils.column_info import ColumnInfo\n",
    "from morpheus.utils.column_info import DataFrameInputSchema\n",
    "from morpheus.utils.column_info import DateTimeColumn\n",
    "from morpheus.utils.column_info import DistinctIncrementColumn\n",
    "from morpheus.utils.column_info import IncrementColumn\n",
    "from morpheus.utils.column_info import RenameColumn\n",
    "from morpheus.utils.column_info import StringCatColumn\n",
    "from morpheus.utils.file_utils import date_extractor\n",
    "from morpheus.stages.postprocess.filter_detections_stage import FilterDetectionsStage\n",
    "from morpheus.stages.postprocess.serialize_stage import SerializeStage\n",
    "from morpheus.utils.logger import configure_logging\n",
    "from morpheus.stages.general.monitor_stage import MonitorStage\n",
    "from morpheus.config import PipelineModes\n",
    "from morpheus.io.deserializers import read_file_to_df\n",
    "from morpheus.llm import LLMEngine\n",
    "from morpheus.llm.nodes.extracter_node import ExtracterNode\n",
    "from morpheus.llm.nodes.llm_generate_node import LLMGenerateNode\n",
    "from morpheus.llm.nodes.prompt_template_node import PromptTemplateNode\n",
    "from morpheus.llm.services.llm_service import LLMService\n",
    "from morpheus.llm.services.nemo_llm_service import NeMoLLMService\n",
    "from morpheus.llm.task_handlers.simple_task_handler import SimpleTaskHandler\n",
    "from morpheus.messages import ControlMessage\n",
    "from morpheus.stages.input.in_memory_source_stage import InMemorySourceStage\n",
    "from morpheus.stages.llm.llm_engine_stage import LLMEngineStage\n",
    "from morpheus.stages.output.in_memory_sink_stage import InMemorySinkStage\n",
    "from morpheus.stages.preprocess.deserialize_stage import DeserializeStage\n",
    "from morpheus.utils.concat_df import concat_dataframes\n",
    "\n",
    "\n",
    "# Left align all tables\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38c1b7-ce84-43e0-ac53-280562dc1642",
   "metadata": {
    "tags": []
   },
   "source": [
    "## High Level Configuration of DFP\n",
    "\n",
    "***This section is configured exactly the same as the Azure Inference demo notebook in this directory. If you are familiar with that, you can skip over this.***\n",
    "\n",
    "The following options significantly alter the functionality of the pipeline. These options are separated from the individual stage options since they may effect more than one stage. Additionally, the matching python script to this notebook, `dfp_azure_pipeline.py`, configures these options via command line arguments.\n",
    "\n",
    "### Options\n",
    "\n",
    "| Name | Type | Description |\n",
    "| --- | --- | :-- |\n",
    "| `train_users` | One of `[\"none\"]` | For inference, this option should always be `\"none\"` |\n",
    "| `skip_users` | List of strings | Any user in this list will be dropped from the pipeline. Useful for debugging to remove automated accounts with many logs. |\n",
    "| `cache_dir` | string | The location to store cached files. To aid with development and reduce bandwidth, the Morpheus pipeline will cache data from several stages of the pipeline. This option configures the location for those caches. |\n",
    "| `input_files` | List of strings | List of files to process. Can specify multiple arguments for multiple files. Also accepts glob (\\*) wildcards and schema prefixes such as `s3://`. For example, to make a local cache of an s3 bucket, use `filecache::s3://mybucket/*`. Refer to `fsspec` documentation for list of possible options. |\n",
    "| `model_name_formatter` | string | A format string to use when building the model name. The model name is constructed by calling `model_name_formatter.format(user_id=user_id)`. For example, with `model_name_formatter=\"my_model-{user_id}\"` and a user ID of `\"first:last\"` would result in the model name of `\"my_model-first:last\"`. This should match the value used in `DFPMLFlowModelWriterStage`. Available keyword arguments: `user_id`, `user_md5`. |\n",
    "| `experiment_name_formatter` | string | A format string (without the `f`) that will be used when creating an experiment in ML Flow. Available keyword arguments: `user_id`, `user_md5`, `reg_model_name`. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee00703-75c5-46fc-890c-86733da906c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global options\n",
    "train_users = \"none\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_users: typing.List[str] = []\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"./.cache/dfp\"\n",
    "\n",
    "# Input files to read from\n",
    "input_files = [\n",
    "    \"../../../../data/dfp/azure-inference-data/AZUREAD_*.json\",\n",
    "]\n",
    "\n",
    "# The format to use for models\n",
    "model_name_formatter = \"DFP-azure-{user_id}\"\n",
    "\n",
    "# === Derived Options ===\n",
    "# To include the generic, we must be training all or generic\n",
    "include_generic = train_users == \"all\" or train_users == \"generic\"\n",
    "\n",
    "# To include individual, we must be either training or inferring\n",
    "include_individual = train_users != \"generic\"\n",
    "\n",
    "# None indicates we arent training anything\n",
    "is_training = train_users != \"none\"\n",
    "\n",
    "# Tracking URI\n",
    "tracking_uri = \"http://mlflow:5000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b586016",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set MLFlow Tracking URI\n",
    "Set MLFlow tracking URI to make inference calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea82337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc24c9-c85e-4977-a348-692c8f0aceaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Global Config Object\n",
    "Before creating the pipeline, we need to setup logging and set the parameters for the Morpheus config object. This config object is responsible for the following:\n",
    " - Indicating whether to use C++ or Python stages\n",
    "    - C++ stages are not supported for the DFP pipeline. This should always be `False`\n",
    " - Setting the number of threads to use in the pipeline. Defaults to the thread count of the OS.\n",
    " - Sets the feature column names that will be used in model training\n",
    "    - This option allows extra columns to be used in the pipeline that will not be part of the training algorithm.\n",
    "    - The final features that the model will be trained on will be an intersection of this list with the log columns.\n",
    " - The column name that indicates the user's unique identifier\n",
    "    - It is required for DFP to have a user ID column\n",
    " - The column name that indicates the timestamp for the log\n",
    "    - It is required for DFP to know when each log occurred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01abd537-9162-49dc-8e83-d9465592f1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable the Morpheus logger\n",
    "from morpheus.config import PipelineModes\n",
    "import os\n",
    "\n",
    "config = Config()\n",
    "\n",
    "CppConfig.set_should_use_cpp(False)\n",
    "\n",
    "config.num_threads = os.cpu_count()\n",
    "config.mode = PipelineModes.NLP #This is for the LLM piece, to allow Morpheus to make the necessary NLP GPU optimizations in the pipeline during data processing\n",
    "\n",
    "config.ae = ConfigAutoEncoder()\n",
    "\n",
    "config.ae.feature_columns = [\n",
    "    \"appDisplayName\", \"clientAppUsed\", \"deviceDetailbrowser\", \"deviceDetaildisplayName\", \"deviceDetailoperatingSystem\", \"statusfailureReason\", \"appincrement\", \"locincrement\", \"logcount\", \n",
    "]\n",
    "config.ae.userid_column_name = \"username\"\n",
    "config.ae.timestamp_column_name = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a73a4d53-32b6-4ab8-a5d7-c0104b31c69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the column names to ensure all data is uniform\n",
    "source_column_info = [\n",
    "    DateTimeColumn(name=config.ae.timestamp_column_name, dtype=datetime, input_name=\"time\"),\n",
    "    RenameColumn(name=config.ae.userid_column_name, dtype=str, input_name=\"properties.userPrincipalName\"),\n",
    "    RenameColumn(name=\"appDisplayName\", dtype=str, input_name=\"properties.appDisplayName\"),\n",
    "    ColumnInfo(name=\"category\", dtype=str),\n",
    "    RenameColumn(name=\"clientAppUsed\", dtype=str, input_name=\"properties.clientAppUsed\"),\n",
    "    RenameColumn(name=\"deviceDetailbrowser\", dtype=str, input_name=\"properties.deviceDetail.browser\"),\n",
    "    RenameColumn(name=\"deviceDetaildisplayName\", dtype=str, input_name=\"properties.deviceDetail.displayName\"),\n",
    "    RenameColumn(name=\"deviceDetailoperatingSystem\",\n",
    "                    dtype=str,\n",
    "                    input_name=\"properties.deviceDetail.operatingSystem\"),\n",
    "    StringCatColumn(name=\"location\",\n",
    "                    dtype=str,\n",
    "                    input_columns=[\n",
    "                        \"properties.location.city\",\n",
    "                        \"properties.location.countryOrRegion\",\n",
    "                    ],\n",
    "                    sep=\", \"),\n",
    "    RenameColumn(name=\"statusfailureReason\", dtype=str, input_name=\"properties.status.failureReason\"),\n",
    "]\n",
    "\n",
    "source_schema = DataFrameInputSchema(json_columns=[\"properties\"], column_info=source_column_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a0cb0a-e65a-444a-a06c-a4525d543790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing schema\n",
    "preprocess_column_info = [\n",
    "    ColumnInfo(name=config.ae.timestamp_column_name, dtype=datetime),\n",
    "    ColumnInfo(name=config.ae.userid_column_name, dtype=str),\n",
    "    ColumnInfo(name=\"appDisplayName\", dtype=str),\n",
    "    ColumnInfo(name=\"clientAppUsed\", dtype=str),\n",
    "    ColumnInfo(name=\"deviceDetailbrowser\", dtype=str),\n",
    "    ColumnInfo(name=\"deviceDetaildisplayName\", dtype=str),\n",
    "    ColumnInfo(name=\"deviceDetailoperatingSystem\", dtype=str),\n",
    "    ColumnInfo(name=\"statusfailureReason\", dtype=str),\n",
    "\n",
    "    # Derived columns\n",
    "    IncrementColumn(name=\"logcount\",\n",
    "                    dtype=int,\n",
    "                    input_name=config.ae.timestamp_column_name,\n",
    "                    groupby_column=config.ae.userid_column_name),\n",
    "    DistinctIncrementColumn(name=\"locincrement\",\n",
    "                            dtype=int,\n",
    "                            input_name=\"location\",\n",
    "                            groupby_column=config.ae.userid_column_name,\n",
    "                            timestamp_column=config.ae.timestamp_column_name),\n",
    "    DistinctIncrementColumn(name=\"appincrement\",\n",
    "                            dtype=int,\n",
    "                            input_name=\"appDisplayName\",\n",
    "                            groupby_column=config.ae.userid_column_name,\n",
    "                            timestamp_column=config.ae.timestamp_column_name)\n",
    "]\n",
    "\n",
    "preprocess_schema = DataFrameInputSchema(column_info=preprocess_column_info, preserve_columns=[\"_batch_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc59de-dea8-4e5f-98e3-98eba1e4621d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pipeline Construction\n",
    "From this point on we begin constructing the stages that will make up the pipeline. To make testing easier, constructing the pipeline object, adding the stages, and running the pipeline, is provided as a single cell. The below cell can be rerun multiple times as needed for debugging.\n",
    "\n",
    "### Source Stage (`MultiFileSource`)\n",
    "\n",
    "This pipeline read input logs from one or more input files. This source stage will construct a list of files to be processed and pass to downstream stages. It is capable of reading files from many different source types, both local and remote. This is possible by utilizing the `fsspec` library (similar to `pandas`). Refer to the [`fsspec`](https://filesystem-spec.readthedocs.io/) documentation for more information on the supported file types. Once all of the logs have been read, the source completes. \n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filenames` | List of strings | | Any files to read into the pipeline. All files will be combined into a single `DataFrame` |\n",
    "\n",
    "### File Batcher Stage (`DFPFileBatcherStage`)\n",
    "\n",
    "To improve performance, multiple small input files can be batched together into a single DataFrame for processing. This stage is responsible for determining the timestamp of input files, grouping input files into batches by time, and sending the batches to be processed into a single DataFrame. Repeated batches of files will be loaded from cache resulting in increased performance. For example, when performaing a 60 day training run, 59 days can be cached with a period of `\"D\"` and retraining once per day.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `period` | `str` | `\"D\"` | The period to create batches. Refer to `pandas` windowing frequency documentation for available options.  |\n",
    "| `date_conversion_func` | Function of `typing.Callable[[fsspec.core.OpenFile], datetime]` | | A callback which is responsible for determining the date for a specified file. |\n",
    "\n",
    "### File to DataFrame Stage (`DFPFileToDataFrameStage`)\n",
    "\n",
    "After files have been batched into groups, this stage is responsible for reading the files and converting into a DataFrame. The specified input schema converts the raw DataFrame into one suitable for caching and processing. Any columns that are not needed should be excluded from the schema.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `schema` | `DataFrameInputSchema` | | After the raw `DataFrame` is read from each file, this schema will be applied to ensure a consisten output from the source. |\n",
    "| `file_type` | `FileTypes` | `FileTypes.Auto` | Allows overriding the file type. When set to `Auto`, the file extension will be used. Options are `CSV`, `JSON`, `PARQUET`, `Auto`. |\n",
    "| `parser_kwargs` | `dict` | `{}` | This dictionary will be passed to the `DataFrame` parser class. Allows for customization of log parsing. |\n",
    "| `cache_dir` | `str` | `./.cache/dfp` | The location to write cached input files to. |\n",
    "\n",
    "### Split Users Stage (`DFPSplitUsersStage`)\n",
    "\n",
    "Once the input logs have been read into a `DataFrame`, this stage is responsible for breaking that single `DataFrame` with many users into multiple `DataFrame`s for each user. This is also where the pipeline chooses whether to train individual users or the generic user (or both).\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `include_generic` | `bool` | | Whether or not to combine all user logs into a single `DataFrame` with the username 'generic_user' |\n",
    "| `include_individual` | `bool` | | Whether or not to output individual `DataFrame` objects for each user |\n",
    "| `skip_users` | List of `str` | `[]` | Any users to remove from the `DataFrame`. Useful for debugging to remove automated accounts with many logs. Mutually exclusive with `only_users`. |\n",
    "| `only_users` | List of `str` | `[]` | Only allow these users in the final `DataFrame`. Useful for debugging to focus on specific users. Mutually exclusive with `skip_users`. |\n",
    "\n",
    "### Rolling Window Stage (`DFPRollingWindowStage`)\n",
    "\n",
    "The Rolling Window Stage performs several key pieces of functionality for DFP.\n",
    "1. This stage keeps a moving window of logs on a per user basis\n",
    "   1. These logs are saved to disk to reduce memory requirements between logs from the same user\n",
    "1. It only emits logs when the window history requirements are met\n",
    "   1. Until all of the window history requirements are met, no messages will be sent to the rest of the pipeline.\n",
    "   1. Configuration options for defining the window history requirements are detailed below.\n",
    "1. It repeats the necessary logs to properly calculate log dependent features.\n",
    "   1. To support all column feature types, incoming log messages can be combined with existing history and sent to downstream stages.\n",
    "   1. For example, to calculate a feature that increments a counter for the number of logs a particular user has generated in a single day, we must have the user's log history for the past 24 hours. To support this, this stage will combine new logs with existing history into a single `DataFrame`.\n",
    "   1. It is the responsibility of downstream stages to distinguish between new logs and existing history.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `min_history` | `int` | `1` | The minimum number of logs a user must have before emitting any messages. Logs below this threshold will be saved to disk. |\n",
    "| `min_increment` | `int` or `str` | `0` | Once the min history requirement is met, this stage must receive `min_increment` *new* logs before emmitting another message. Logs received before this threshold is met will be saved to disk. Can be specified as an integer count or a string duration. |\n",
    "| `max_history` | `int` or `str` | `\"1d\"` | Once `min_history` and `min_increment` requirements have been met, this puts an upper bound on the maximum number of messages to forward into the pipeline and also the maximum amount of messages to retain in the history. Can be specified as an integer count or a string duration. |\n",
    "| `cache_dir` | `str` | `./.cache/dfp` | The location to write log history to disk. |\n",
    "\n",
    "### Preprocessing Stage (`DFPPreprocessingStage`)\n",
    "\n",
    "This stage performs the final, row dependent, feature calculations as specified by the input schema object. Once calculated, this stage can forward on all received logs, or optionally can only forward on new logs, removing any history information.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `input_schema` | `DataFrameInputSchema` | | The final, row dependent, schema to apply to the incoming columns |\n",
    "\n",
    "### Inference Stage (`DFPInference`)\n",
    "\n",
    "This stage performs several tasks to aid in performing inference. This stage will:\n",
    "1. Download models as needed from MLFlow\n",
    "1. Cache previously downloaded models to improve performance\n",
    "   1. Models in the cache will be periodically refreshed from MLFlow at a configured rate\n",
    "1. Perform inference using the downloaded model\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `model_name_formatter` | `str` | `\"\"` | A format string to use when building the model name. The model name is constructed by calling `model_name_formatter.format(user_id=user_id)`. For example, with `model_name_formatter=\"my_model-{user_id}\"` and a user ID of `\"first:last\"` would result in the model name of `\"my_model-first:last\"`. This should match the value used in `DFPMLFlowModelWriterStage` |\n",
    "\n",
    "### Filter Detection Stage (`FilterDetectionsStage`)\n",
    "This stage filters the output from the inference stage for any anomalous messages. Logs which exceed the specified Z-Score will be passed onto the next stage. All remaining logs which are below the threshold will be dropped. For the purposes of the DFP pipeline, this stage is configured to use the `mean_abs_z` column of the DataFrame as the filter criteria.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `threshold` | `float` | `0.5` | The threshold value above which logs are considered to be anomalous. The default is `0.5`, however the DFP pipeline uses a value of `2.0`. All normal logs will be filtered out and anomalous logs will be passed on. |\n",
    "| `copy` | `bool` | `True` | When the `copy` argument is `True` (default), rows that meet the filter criteria are copied into a new dataframe. When `False` sliced views are used instead. This is a performance optimization, and has no functional impact. |\n",
    "| `filter_source` | `FilterSource` | `FilterSource.Auto` | Indicates if the filter criteria exists in an output tensor (`FilterSource.TENSOR`) or a column in a DataFrame (`FilterSource.DATAFRAME`). |\n",
    "| `field_name` | `str` | `probs` | Name of the tensor (`filter_source=FilterSource.TENSOR`) or DataFrame column (`filter_source=FilterSource.DATAFRAME`) to use as the filter criteria. |\n",
    "\n",
    "### Post Processing Stage (`DFPPostprocessingStage`)\n",
    "This stage adds a new `event_time` column to the DataFrame indicating the time which Morpheus detected the anomalous messages, and replaces any `NAN` values with the a string value of `'NaN'`.\n",
    "\n",
    "### Serialize Stage (`SerializeStage`)\n",
    "This stage controls which columns in the DataFrame will be included in the output. For the purposes of the DFP pipeline, we will exclude columns that are used internally by the pipeline which are not of interest to the end-user.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `include` | List of `str` | `[]` | List of regular expression patterns matching columns to include in the output. Specifying an empty list causes all columns to be included not explicitly excluded. |\n",
    "| `exclude` | List of `str` | `[r'^ID$', r'^_ts_']` | List of regular expression patterns matching columns to exclude from the output. |\n",
    "| `fixed_columns` | `bool` | `True` | When `True` it is assumed that the Dataframe in all messages contain the same columns as the first message received. |\n",
    "\n",
    "***You'll notice here that we don't have a write to file stage for the DFP pipeline. That is because, in the next section, we will add stages for LLM processing.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d94ca6d2-376a-4ed4-a133-42f37768122e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<monitor-11; MonitorStage(description=DFP Serialization rate, smoothing=0.001, unit=messages, delayed_start=False, determine_count_fn=None, log_level=LogLevels.INFO)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = LinearPipeline(config)\n",
    "\n",
    "# Source stage\n",
    "pipeline.set_source(MultiFileSource(config, filenames=input_files))\n",
    "\n",
    "# Batch files into batches by time. Use the default ISO date extractor from the filename\n",
    "pipeline.add_stage(\n",
    "    DFPFileBatcherStage(config,\n",
    "                        period=\"D\",\n",
    "                        date_conversion_func=functools.partial(date_extractor, filename_regex=iso_date_regex)))\n",
    "\n",
    "# Output is a list of fsspec files. Convert to DataFrames. This caches downloaded data\n",
    "pipeline.add_stage(\n",
    "    DFPFileToDataFrameStage(config,\n",
    "                            schema=source_schema,\n",
    "                            file_type=FileTypes.JSON,\n",
    "                            parser_kwargs={\n",
    "                                \"lines\": False, \"orient\": \"records\"\n",
    "                            },\n",
    "                            cache_dir=cache_dir))\n",
    "\n",
    "\n",
    "# This will split users or just use one single user\n",
    "pipeline.add_stage(\n",
    "    DFPSplitUsersStage(config,\n",
    "                        include_generic=include_generic,\n",
    "                        include_individual=include_individual,\n",
    "                        skip_users=skip_users))\n",
    "\n",
    "# Next, have a stage that will create rolling windows\n",
    "pipeline.add_stage(\n",
    "    DFPRollingWindowStage(\n",
    "        config,\n",
    "        min_history=300 if is_training else 1,\n",
    "        min_increment=300 if is_training else 0,\n",
    "        # For inference, we only ever want 1 day max\n",
    "        max_history=\"60d\" if is_training else \"1d\",\n",
    "        cache_dir=cache_dir))\n",
    "\n",
    "# Output is UserMessageMeta -- Cached frame set\n",
    "pipeline.add_stage(DFPPreprocessingStage(config, input_schema=preprocess_schema))\n",
    "\n",
    "# Perform inference on the preprocessed data\n",
    "pipeline.add_stage(DFPInferenceStage(config, model_name_formatter=model_name_formatter))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"DFP Inference rate\", smoothing=0.001))\n",
    "\n",
    "# Filter for only the anomalous logs\n",
    "pipeline.add_stage(\n",
    "            FilterDetectionsStage(config, threshold=6, filter_source=FilterSource.DATAFRAME, field_name='mean_abs_z'))\n",
    "pipeline.add_stage(DFPPostprocessingStage(config))\n",
    "\n",
    "# Exclude the columns we don't want in our output\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['batch_count', 'origin_hash', '_row_hash', '_batch_id']))\n",
    "\n",
    "# Monitor throughput at the tail-end of the DFP specific portion of the pipeline\n",
    "pipeline.add_stage(MonitorStage(config, description=\"DFP Serialization rate\", smoothing=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01a1e6-fd51-4ba0-a3d5-5a87e533bf44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LLM-based Triage\n",
    "\n",
    "Now that we have the pipeline set up to pass through the Digital Fingerprinting piece of the code, the output of our last `MonitorStage` can be sent into a series of stages tha:\n",
    "\n",
    "1. **Transform the data into a column that can be used in an LLM prompt template.**\n",
    "2. **Apply a prompt template and make calls to the LLM using the Morpheus `LLMEngine`.**\n",
    "3. **Collect LLM output and add that to a column of our final output DataFrame.**\n",
    "4. **Write the output to a file.**\n",
    "\n",
    "### Exploring a Prompt Template\n",
    "\n",
    "We will use the following prompt template for the AZURE Active Directory log use case for DFP. Naturally, this will need to change when log sources change to give the LLM more context. \n",
    "\n",
    "```\n",
    "        You are an L1 SOC analyst. Your task is to triage and explain an alert received from an ML workflow that uses an autoencoder to perform anomaly detection, per user, on AZURE AD log telemetry.\n",
    "\n",
    "        The log contains the following kinds of fields: \n",
    "\n",
    "        timestamp: timestamp of the event\n",
    "        username: username of the user\n",
    "        appDisplayName: the name of the running app\n",
    "        category: the type of authentication event\n",
    "        clientAppUsed: the type of browser/app doing the authentication\n",
    "        deviceDetailBrowser: specifics about the browser such as versions, etc. \n",
    "        deviceDetailDisplayName: displayName of the user if available\n",
    "        location: location where the activity originated\n",
    "        statusfailureReason: if the auth event failed, why. \n",
    "        event_time: time the event was logged\n",
    "        logcount: number of authentication logs for this user in this time period\n",
    "        locincrement: number of distinct locations for that user\n",
    "        appincrement: number of distinct apps used to authenticate\n",
    "        <field>_pred: autoencoder output of the feature\n",
    "        <field>_z_loss: z-score of the standard scaled loss between feature and prediction\n",
    "        mean_abs_z: mean absolute z-score of reconstruction error across all logs for that user\n",
    "        max_abs_z: max absolute z-score of reconstruction error across all logs for that user\n",
    "\n",
    "        Given the feature explanations above, I would like you to use that, and any internal knowledge of cyber you have to triage the following event.\n",
    "\n",
    "        The event is: \n",
    "        ### EVENT ######\n",
    "        {{event}}\n",
    "        #### END EVENT ####\n",
    "\n",
    "        In your output, please keep it concise. Explain which fields were most anomalous, and any cyber-specific context that may be helpful around that that'll speed up the triage that a human will do after you. Split your response into the following sections using it as a templace:\n",
    "        \n",
    "        ##Start Report##\n",
    "        **Triage Overview**\n",
    "        <provide estimated severity 1/10 scale>\n",
    "        <provide an overview of the event and likelihood of malicious activity based on your cyber knowledge> \n",
    "        **Most Anomalous Fields**\n",
    "        <list the most anomalous fields and interpret their z-scores>\n",
    "        **Cyber Triage**\n",
    "        <cyber-specific content and triage in logical bullet points>\n",
    "        **Recommendations**\n",
    "        <recommendations on next steps for human triage and investigation> <DO NOT provide recommendations on changes to posture, policy etc. Just next steps in investigation>\n",
    "        ##End Report##\n",
    "        \n",
    "        Please only return ascii characters, no special characters like sigma or bullet points. Only A-Z,a-z, 0-9, *, and \\n\n",
    "```\n",
    "\n",
    "Here, the `{{event}}` portion of the template is where a dictionary-like version of the model output per event will be inserted to make the triage request. \n",
    "\n",
    "### Pipeline Stages\n",
    "\n",
    "Now, let us explore the various new Morpheus stages we will chain together to form the latter half of the larger pipeline.\n",
    "\n",
    "#### String Create Stage (`DFPStringCreateStage`)\n",
    "This is a custom stage that aggregates the value of every column on the `MessageMeta` dataframe into a single column called `event` that contains a string value of the dictionary representation of all columns as `key:value` pairs. \n",
    "\n",
    "The `DFPStringCreateStage` will also remove any non-ascii characters in the content of the strings to prevent errors in Morpheus pipeline logging.\n",
    "\n",
    "It is a transparent, pass-thru stage that does not require any configurable arugments.\n",
    "\n",
    "#### Deserialize Stage (`DeserializeStage`)\n",
    "This stage converts a simple `MessageMeta` object coming from the output of the DFP pipeline before it into a `ControlMessage` with a structure designed for the `LLMEngine`.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `ensure_sliceable_index` | `bool` | True | Whether or not to call `ensure_sliceable_index()` on all incoming `MessageMeta`, which will replace the index of the underlying dataframe if the existing one is not unique and monotonic. |\n",
    "| `message_type` | `MultiMessage or ControlMessage` | `MultiMessage` | Sets the type of message to be emitted from this stage. |\n",
    "| `task_type` | `str` | `None` | If specified, adds the specified task to the `ControlMessage`. This parameter is only valid when `message_type` is set to `ControlMessage`. If not `None`, `task_payload` must also be specified. |\n",
    "| `task_payload` | `dict` | `None` | If specified, adds the specified task to the `ControlMessage`. This parameter is only valid when `message_type`is set to `ControlMessage`. If not `None`, `task_type` must also be specified. |\n",
    "\n",
    "In our case, the task arguments are set to be `{\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"event\"]}}` which performs a completion task applying a prompt template on the `event` column of our dataframe. \n",
    "\n",
    "#### Write to File Stage (`WriteToFileStage`)\n",
    "\n",
    "This final stage will write all received messages to a single output file in either CSV or JSON format.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filename` | `str` | | The file to write anomalous log messages to. |\n",
    "| `overwrite` | `bool` | `False` | If the file specified in `filename` already exists, it will be overwritten if this option is set to `True` |\n",
    "\n",
    "***All the other stages we will see in the pipeline have already been introduced. They just might be initialized with different parameters.***\n",
    "\n",
    "### Building an `LLMEngine` Stage\n",
    "\n",
    "An `LLMEngine` stage is a stage that can consist of multiple `nodes` that perform various LLM based tasks. In our case, we will build an `LLMEngine` with the following components\n",
    "\n",
    "1. `ExtractorNode` which extract the relevent `event` column values from the input dataframe.\n",
    "2. `PromptTemplateNode` which applies the provided prompt template to the extracted events.\n",
    "3. `LLMGenerateNode` which makes API calls to the LLM service for completion. \n",
    "4. `SimpleTaskHandler` which copies fields from an `LLMContext` to columns in the DataFrame contained in the `ControlMessage` payload.\n",
    "\n",
    "Behind the scenes, the `LLMEngine` handles concurrent, asynchronous API calls and data-processing to boost performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b21eb9-bc30-4856-b1c3-9aabeae0662a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adding Event Summarization\n",
    "\n",
    "Now that we have the pieces in place to build our first `LLMEngineStage`, let's put it into the pipeline to create a stage that takes multiple anomalous detections for a user, and summarizes it into an incident based on the most anomalous events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "910a70f6-7415-48a8-8507-c16c3c648e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<serialize-17; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load prompt templates from file\n",
    "templates = json.load(open(\"dfp/llm/prompt_templates.json\", \"r\"))\n",
    "\n",
    "first_completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"event\"]}}\n",
    "\n",
    "pipeline.add_stage(DFPStringCreateStage(config))\n",
    "\n",
    "pipeline.add_stage(\n",
    "    DeserializeStage(config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=first_completion_task))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"LLM Summary Deserialize rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(LLMEngineStage(config, engine=build_engine_llm_service(prompt_template = templates[\"incident_summary\"], \n",
    "                                                                           llm_service=\"NIM\", output_column=\"incident_summary\")))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"LLM Summary Inference rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['event']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e62cc8-1da2-4fb1-b4b9-6a8aa31101cd",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) for Threat Intelligence Correlation and Recommendations\n",
    "\n",
    "In this section, we'll aim to use RAG to look up threat intelligence reports stored in a NeMo Retriever collection that are relevant to the anomalous event, and enrich the event with threat intelligence findings and recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94420ace-91df-467c-af05-087e33fdd356",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Using a NIM to Generate a Search Optimized Query\n",
    "\n",
    "The first part of this process will involve creating an `LLMEngineStage` that generates a search optimized prompt for the Retriever that incorporates the relevant aspects of the anomalous event. We'll use the following prompt template:\n",
    "\n",
    "```\n",
    "        You are an L1 SOC analyst. You will be given an incident summary generated from an ML workflow that uses an autoencoder to perform anomaly detection, per user, on AZURE AD log telemetry.\n",
    "\n",
    "        The summary was generated based on the following kinds of fields: \n",
    "\n",
    "        timestamp: timestamp of the event\n",
    "        username: username of the user\n",
    "        appDisplayName: the name of the running app\n",
    "        category: the type of authentication event\n",
    "        clientAppUsed: the type of browser/app doing the authentication\n",
    "        deviceDetailBrowser: specifics about the browser such as versions, etc. \n",
    "        deviceDetailDisplayName: displayName of the user if available\n",
    "        location: location where the activity originated\n",
    "        statusfailureReason: if the auth event failed, why. \n",
    "        event_time: time the event was logged\n",
    "        logcount: number of authentication logs for this user in this time period\n",
    "        locincrement: number of distinct locations for that user\n",
    "        appincrement: number of distinct apps used to authenticate\n",
    "        <field>_pred: autoencoder output of the feature\n",
    "        <field>_z_loss: z-score of the standard scaled loss between feature and prediction\n",
    "        mean_abs_z: mean absolute z-score of reconstruction error across all logs for that user\n",
    "        max_abs_z: max absolute z-score of reconstruction error across all logs for that user\n",
    "\n",
    "\n",
    "        The event is: \n",
    "        ### EVENT ######\n",
    "        {{incident_summary}}\n",
    "        #### END EVENT ####\n",
    "\n",
    "        Your task is to use the incident summary to create an optimized search query which can be used to search collections of Threat Intelligence documents for similar events either by threat actor, threat vector, or other similar characteristics. \n",
    "        \n",
    "        Please only return ascii characters, no special characters like sigma or bullet points. Only A-Z,a-z, 0-9 and '?'. Please only generate the search query, no text before or after it. \n",
    "        \n",
    "        An example of your response could be: \"Similar events and recommendatios for <description of anomaly> which could be indicative of <cyber triage summary>\n",
    "        \n",
    "        Format your response as:\n",
    "        \n",
    "        ##Begin Response##\n",
    "        <prompt>\n",
    "        ##End Response##\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa05bb78-cfcf-4d91-85b8-f00cb9037a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<serialize-22; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#configure_logging(log_level=logging.INFO)\n",
    "\n",
    "second_completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"incident_summary\"]}}\n",
    "\n",
    "pipeline.add_stage(\n",
    "    DeserializeStage(config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=second_completion_task))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"LLM RAG Prompt Deserialize rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "#build optimized query\n",
    "pipeline.add_stage(LLMEngineStage(config, engine=build_engine_llm_service(prompt_template=templates[\"rag_query\"],\n",
    "    llm_service=\"NIM\", output_column=\"rag_query\")))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"LLM RAG Prompt Inference rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['event']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502dc9f8-0161-432d-998f-ad8e5e2afa11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Collect RAG Context from the Nemo Retriever \n",
    "\n",
    "We'll build a decoupled `LLMEngineStage` that takes the RAG prompt generated in the previous stage and performs inference requests to NeMo retriever to collect context texts for every query.\n",
    "\n",
    "While building the `LLMEngine` for this stage, we see a new type of node: `RetrieverContextNode`. This node is a node used in an `LLMEngine` to asynchronously interact with an NVIDIA NeMo Retriever deployment to collect relevant text chunks for a given query. It accepts a `RetrieverClient` object initalized with the relevant collection information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5a2acb0-064a-41c2-ab2c-7caf65577d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been merged into upload_intel/intel/cyber_enrichment/merged.txt\n",
      "text file loaded\n",
      "Number of chunks from the document: 370\n"
     ]
    }
   ],
   "source": [
    "third_completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"rag_query\"]}}\n",
    "\n",
    "pipeline.add_stage(\n",
    "    DeserializeStage(config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=third_completion_task))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"NeMo RAG Context Deserialize rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "#add threat intelligence\n",
    "retriever_client_cyber_enrichment = nrc.RetrieverClient()\n",
    "\n",
    "\n",
    "base_directory = \"upload_intel/intel/cyber_enrichment/\"\n",
    "input_files = [f\"{i}.txt\" for i in range(1, 14)]\n",
    "output_file = base_directory+\"merged.txt\"\n",
    "\n",
    "with open(output_file, 'w') as outfile:\n",
    "    # Iterate through the list of input files\n",
    "    for file_name in input_files:\n",
    "        with open(base_directory+file_name, 'r') as infile:\n",
    "            content = infile.read()\n",
    "            outfile.write(content)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(f\"Files have been merged into {output_file}\")\n",
    "\n",
    "#load in text file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(output_file)\n",
    "\n",
    "document = loader.load()\n",
    "print(\"text file loaded\")\n",
    "\n",
    "\n",
    "document_chunks = retriever_client_cyber_enrichment.add_files(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3feb3eb0-7af9-4e55-a91c-8a1558751fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llm-engine-25; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f1671f878f0>)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use optimized query to extract context\n",
    "pipeline.add_stage(LLMEngineStage(config, engine=build_engine_rag_context(retriever_client_cyber_enrichment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a61de599-7189-4158-a02a-82f1ed37b296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<serialize-27; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.add_stage(MonitorStage(config, description=\"NeMo RAG Context Inference rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['event']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d384cfd-336d-4b17-8e79-a8b2988f73dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using RAG Context Alongside The Triaged Event to add a Threat Intelligence Enrichment Section\n",
    "\n",
    "Now that we've generated an incident summary, and collected threat intelligence reports that most closesly match the incident, we can synthesize the information into a single event summary report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b36b675-b193-4ea4-8e7f-591322985a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Pipeline Pre-build====\u001b[0m\n",
      "====Pre-Building Segment: linear_segment_0====\u001b[0m\n",
      "====Pre-Building Segment Complete!====\u001b[0m\n",
      "====Pipeline Pre-build Complete!====\u001b[0m\n",
      "====Registering Pipeline====\u001b[0m\n",
      "====Building Pipeline====\u001b[0m\n",
      "====Building Pipeline Complete!====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Registering Pipeline Complete!====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Starting Pipeline====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "DFP Serialization rate: 0 messages [00:00, ? messages/s]\u001b[A\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Building Segment: linear_segment_0====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added source: <from-multi-file-0; MultiFileSource(filenames=['../../../../data/dfp/azure-inference-data/AZUREAD_*.json'], watch=False, watch_interval=1.0)>\n",
      "  └─> fsspec.OpenFiles\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-file-batcher-1; DFPFileBatcherStage(date_conversion_func=functools.partial(<function date_extractor at 0x7f157b3a8430>, filename_regex=re.compile('(?P<year>\\\\d{4})-(?P<month>\\\\d{1,2})-(?P<day>\\\\d{1,2})(?:T(?P<hour>\\\\d{1,2})(?::|_|\\\\.)(?P<minute>\\\\d{1,2})(?::|_|\\\\.)(?P<second>\\\\d{1,2})(?:\\\\.(?P<microsecond>\\\\d{0,6}))?)?(?P<zulu>Z)?')), period=D, sampling_rate_s=None, start_time=None, end_time=None, sampling=None)>\n",
      "  └─ fsspec.OpenFiles -> Tuple[fsspec.core.OpenFiles, int]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-file-to-df-2; DFPFileToDataFrameStage(schema=DataFrameInputSchema(json_columns=['properties'], column_info=[DateTimeColumn(name='timestamp', dtype='datetime64[ns]', input_name='time'), RenameColumn(name='username', dtype='str', input_name='properties.userPrincipalName'), RenameColumn(name='appDisplayName', dtype='str', input_name='properties.appDisplayName'), ColumnInfo(name='category', dtype='str'), RenameColumn(name='clientAppUsed', dtype='str', input_name='properties.clientAppUsed'), RenameColumn(name='deviceDetailbrowser', dtype='str', input_name='properties.deviceDetail.browser'), RenameColumn(name='deviceDetaildisplayName', dtype='str', input_name='properties.deviceDetail.displayName'), RenameColumn(name='deviceDetailoperatingSystem', dtype='str', input_name='properties.deviceDetail.operatingSystem'), StringCatColumn(name='location', dtype='str', input_columns=['properties.location.city', 'properties.location.countryOrRegion'], sep=', '), RenameColumn(name='statusfailureReason', dtype='str', input_name='properties.status.failureReason')], preserve_columns=None, row_filter=None), filter_null=True, file_type=FileTypes.JSON, parser_kwargs={'lines': False, 'orient': 'records'}, cache_dir=./.cache/dfp)>\n",
      "  └─ Tuple[fsspec.core.OpenFiles, int] -> pandas.DataFrame\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-split-users-3; DFPSplitUsersStage(include_generic=False, include_individual=True, skip_users=[], only_users=None)>\n",
      "  └─ pandas.DataFrame -> dfp.DFPMessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-rolling-window-4; DFPRollingWindowStage(min_history=1, min_increment=0, max_history=1d, cache_dir=./.cache/dfp)>\n",
      "  └─ dfp.DFPMessageMeta -> dfp.MultiDFPMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-preproc-5; DFPPreprocessingStage(input_schema=DataFrameInputSchema(json_columns=[], column_info=[ColumnInfo(name='timestamp', dtype='datetime64[ns]'), ColumnInfo(name='username', dtype='str'), ColumnInfo(name='appDisplayName', dtype='str'), ColumnInfo(name='clientAppUsed', dtype='str'), ColumnInfo(name='deviceDetailbrowser', dtype='str'), ColumnInfo(name='deviceDetaildisplayName', dtype='str'), ColumnInfo(name='deviceDetailoperatingSystem', dtype='str'), ColumnInfo(name='statusfailureReason', dtype='str'), IncrementColumn(name='logcount', dtype='int', input_name='timestamp', groupby_column='username', period='D'), DistinctIncrementColumn(name='locincrement', dtype='int', input_name='location', groupby_column='username', period='D', timestamp_column='timestamp'), DistinctIncrementColumn(name='appincrement', dtype='int', input_name='appDisplayName', groupby_column='username', period='D', timestamp_column='timestamp')], preserve_columns=re.compile('(_batch_id)'), row_filter=None))>\n",
      "  └─ dfp.MultiDFPMessage -> dfp.MultiDFPMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-inference-6; DFPInferenceStage(model_name_formatter=DFP-azure-{user_id})>\n",
      "  └─ dfp.MultiDFPMessage -> morpheus.MultiAEMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-7; MonitorStage(description=DFP Inference rate, smoothing=0.001, unit=messages, delayed_start=False, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.MultiAEMessage -> morpheus.MultiAEMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <filter-8; FilterDetectionsStage(threshold=6, copy=True, filter_source=FilterSource.DATAFRAME, field_name=mean_abs_z)>\n",
      "  └─ morpheus.MultiAEMessage -> morpheus.MultiAEMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-postproc-9; DFPPostprocessingStage()>\n",
      "  └─ morpheus.MultiAEMessage -> morpheus.MultiAEMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-10; SerializeStage(include=None, exclude=['batch_count', 'origin_hash', '_row_hash', '_batch_id'], fixed_columns=True)>\n",
      "  └─ morpheus.MultiAEMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-11; MonitorStage(description=DFP Serialization rate, smoothing=0.001, unit=messages, delayed_start=False, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-string-create-12; DFPStringCreateStage(top_k=5, grouper=username, sort_key=max_abs_z)>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <deserialize-13; DeserializeStage(ensure_sliceable_index=True, message_type=<class 'morpheus._lib.messages.ControlMessage'>, task_type=llm_engine, task_payload={'task_type': 'completion', 'task_dict': {'input_keys': ['event']}})>\n",
      "  └─ morpheus.MessageMeta -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-14; MonitorStage(description=LLM Summary Deserialize rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <llm-engine-15; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f16bc19d6f0>)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-16; MonitorStage(description=LLM Summary Inference rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-17; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Pipeline Started====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <deserialize-18; DeserializeStage(ensure_sliceable_index=True, message_type=<class 'morpheus._lib.messages.ControlMessage'>, task_type=llm_engine, task_payload={'task_type': 'completion', 'task_dict': {'input_keys': ['incident_summary']}})>\n",
      "  └─ morpheus.MessageMeta -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-19; MonitorStage(description=LLM RAG Prompt Deserialize rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <llm-engine-20; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f157b3f34b0>)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-21; MonitorStage(description=LLM RAG Prompt Inference rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-22; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <deserialize-23; DeserializeStage(ensure_sliceable_index=True, message_type=<class 'morpheus._lib.messages.ControlMessage'>, task_type=llm_engine, task_payload={'task_type': 'completion', 'task_dict': {'input_keys': ['rag_query']}})>\n",
      "  └─ morpheus.MessageMeta -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-24; MonitorStage(description=NeMo RAG Context Deserialize rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <llm-engine-25; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f1671f878f0>)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-26; MonitorStage(description=NeMo RAG Context Inference rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-27; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-rag-concat-28; DFPRAGConcatStage()>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <deserialize-29; DeserializeStage(ensure_sliceable_index=True, message_type=<class 'morpheus._lib.messages.ControlMessage'>, task_type=llm_engine, task_payload={'task_type': 'completion', 'task_dict': {'input_keys': ['event']}})>\n",
      "  └─ morpheus.MessageMeta -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-30; MonitorStage(description=LLM RAG Enrich Deserialize rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <llm-engine-31; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f157c1365b0>)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-32; MonitorStage(description=LLM RAG Enrich Inference rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-33; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-rag-upload-34; DFPRAGUploadStage()>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <to-file-35; WriteToFileStage(filename=dfp_detections_triaged.csv, overwrite=True, file_type=FileTypes.Auto, include_index_col=True, flush=False)>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                    s/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Building Segment Complete!====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:00, ? messages/s]\n",
      "DFP Inference rate: 0 messages [00:01, ? messages/s]s/s]\u001b[A\n",
      "DFP Inference rate: 0 messages [00:03, ? messages/s]s/s]\u001b[A\n",
      "DFP Inference rate: 0 messages [00:04, ? messages/s]s/s]\u001b[A\n",
      "DFP Inference rate: 0 messages [00:05, ? messages/s]s/s]\u001b[A\n",
      "DFP Serialization rate: 0 messages [00:05, ? messages/s]\u001b[A/opt/conda/envs/morpheus/lib/python3.10/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "DFP Inference rate: 0 messages [00:06, ? messages/s]\n",
      "DFP Inference rate: 0 messages [00:07, ? messages/s]s/s]\u001b[A\n",
      "DFP Inference rate: 0 messages [00:08, ? messages/s]s/s]\u001b[A\n",
      "DFP Serialization rate: 0 messages [00:08, ? messages/s]\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8339f084a2344c1a7e6ceb61d832a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate: 0 messages [00:09, ? messages/s]\n",
      "DFP Inference rate: 13 messages [00:09,  1.35 messages/s][A\n",
      "DFP Inference rate: 36 messages [00:11,  3.53 messages/s][A\n",
      "DFP Inference rate: 53 messages [00:11,  4.50 messages/s][A\n",
      "DFP Inference rate: 60 messages [00:12,  4.68 messages/s][A\n",
      "DFP Inference rate: 76 messages [00:13,  5.50 messages/s][A\n",
      "DFP Inference rate: 87 messages [00:14,  5.86 messages/s][A\n",
      "DFP Inference rate: 97 messages [00:15,  6.12 messages/s][A\n",
      "DFP Inference rate: 117 messages [00:16,  6.95 messages/s]A\n",
      "DFP Inference rate: 120 messages [00:18,  6.68 messages/s]A\n",
      "DFP Inference rate: 242 messages [00:19, 12.79 messages/s]A\n",
      "DFP Serialization rate: 83 messages [00:19,  4.35 messages/s]\u001b[AW20240827 03:42:47.408099   881 meta.cpp:259] Dataframe is not a cudf dataframe, converting to cudf dataframe\n",
      "\n",
      "\n",
      "LLM Summary Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\n",
      "\n",
      "DFP Inference rate: 263 messages [00:20, 13.14 messages/s]\n",
      "DFP Serialization rate: 83 messages [00:20,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate: 273 messages [00:21, 12.98 messages/s]A\u001b[A\n",
      "DFP Serialization rate: 83 messages [00:21,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate: 290 messages [00:22, 13.16 messages/s]A\u001b[A\n",
      "DFP Serialization rate: 83 messages [00:22,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate: 305 messages [00:23, 13.25 messages/s]A\u001b[A\n",
      "DFP Serialization rate: 83 messages [00:23,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate: 310 messages [00:23, 13.18 messages/s]A\u001b[A\n",
      "DFP Serialization rate: 83 messages [00:24,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate: 319 messages [00:25, 12.81 messages/s]A\u001b[A\n",
      "DFP Serialization rate: 83 messages [00:25,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate: 336 messages [00:26, 12.94 messages/s]A\u001b[A\n",
      "DFP Serialization rate: 83 messages [00:26,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]\n",
      "DFP Serialization rate[Complete]: 83 messages [00:26,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:07,  7.50s/ req]\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 87.68 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 85.17 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 93.76 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RAG Query:\n",
      "\"Brute force attack or masquerading attempts from multiple locations and apps using anomalous client apps and mismatched display names\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s][A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 158.49 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Enrich Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Enrich Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.81 messages/s]s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\u001b[A\n",
      "\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 212.09 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Enrich Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM RAG Enrich Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DFP Inference rate[Complete]: 339 messages [00:26, 12.75 messages/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:19,  4.35 messages/s]\n",
      "LLM Summary Deserialize rate[Complete]: 1 req [00:00, 117.43 req/s]\n",
      "LLM Summary Inference rate[Complete]: 1 req [00:00, 155.63 req/s]\n",
      "LLM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 257.40 req/s]\n",
      "LLM RAG Prompt Inference rate[Complete]: 1 req [00:00, 167.44 req/s]\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 132.39 req/s]\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 251.71 req/s]\n",
      "LLM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 212.09 req/s]\n",
      "LLM RAG Enrich Inference rate[Complete]: 1 req [00:00, 197.61 req/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents have been written to /workspace/examples/digital_fingerprinting/production/morpheus/workspace/upload_intel/intel/user_summaries/3.txt\n",
      "====Pipeline Complete====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "configure_logging(log_level=logging.INFO)\n",
    "\n",
    "fourth_completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"event\"]}}\n",
    "\n",
    "pipeline.add_stage(DFPRAGConcatStage(config))\n",
    "\n",
    "pipeline.add_stage(\n",
    "    DeserializeStage(config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=fourth_completion_task))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"LLM RAG Enrich Deserialize rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(LLMEngineStage(config, engine=build_engine_llm_service(prompt_template = templates[\"enrichment\"],\n",
    "    llm_service=\"NIM\")))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"LLM RAG Enrich Inference rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['event']))\n",
    "\n",
    "pipeline.add_stage(DFPRAGUploadStage(config)) #upload a file into /intel/user_summaries\n",
    "\n",
    "pipeline.add_stage(WriteToFileStage(config, filename=\"dfp_detections_triaged.csv\", overwrite=True))\n",
    "\n",
    "# Run the pipeline\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b596c78-2a71-405c-b7ee-69d580192633",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Generated Report\n",
    "Below, we can see what a final user summary report looks like for the user attacktarget@domain.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b3ee0d8-daff-4feb-bc8d-3223da3a49a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Start Report##\n",
      "\n",
      "**Event Overview**\n",
      "Username: attacktarget@domain.com\n",
      "Time Range: 2022-08-31 23:20:54 - 2022-08-31 23:54:50\n",
      "Apps: Box, Google Cloud / G Suite Connector by Microsoft, Spike Email - Mail & Team Chat, WeVideo\n",
      "Devices: ATTACKTARGET-LT, Windows 10, Chrome 100.0.4896\n",
      "\n",
      "**Triage Overview**\n",
      "This event is likely indicative of malicious activity. The high number of login attempts from different locations and apps, as well as the discrepancy between predicted and actual values for various fields, suggests anomalous behavior.\n",
      "\n",
      "**Most Anomalous Fields**\n",
      "1. logcount: High z-scores (247.99 - 258.22) indicate a significant increase in login attempts, potentially indicating a brute-force attack.\n",
      "2. appincrement: High z-scores (600.43 - 645.85) suggest an unusual number of apps being used to authenticate, possibly indicating a malicious attempt to access the account.\n",
      "3. appDisplayName: Mismatch between predicted and actual values (InviteDesk vs. various other apps) indicates potential masquerading.\n",
      "4. clientAppUsed: Discrepancy between predicted (Browser) and actual values (Mobile Apps and Desktop clients) may indicate a malicious attempt to disguise the client app used.\n",
      "\n",
      "**Cyber Triage**\n",
      "Potential malicious activity:\n",
      "- Brute-force attack on the user account, indicated by a high number of login attempts from different locations.\n",
      "- Masquerading as legitimate apps to access the account.\n",
      "- Disguising the client app used to access the account.\n",
      "- Possible lateral movement between different apps and services.\n",
      "\n",
      "Investigation steps:\n",
      "- Verify the user's activity and account access during the time range.\n",
      "- Check for any other suspicious activity from the same IP address or location.\n",
      "- Analyze login attempt logs to determine if multiple attempts were made with the same credentials.\n",
      "- Look for potential malicious intent, such as data exfiltration or changes to account settings.\n",
      "\n",
      "**Threat Intelligence Enrichment and Recommendation**\n",
      "Based on the query and event details, it appears that this incident may be related to a brute-force attack or masquerading attempts. The use of multiple apps and locations, as well as the discrepancy in client app used, suggests a sophisticated attack.\n",
      "\n",
      "After analyzing the relevant intel snippets, I found potential correlations with the following APT groups:\n",
      "\n",
      "* APT28 (aka Fancy Bear): Known for using brute-force attacks and masquerading as legitimate apps to gain access to accounts.\n",
      "* APT29 (aka Cozy Bear): Linked to phishing and brute-force attacks, often using multiple apps and locations to disguise their activity.\n",
      "\n",
      "Potential IOCs to investigate:\n",
      "\n",
      "* IP addresses: 185.220.101.65, 104.248.91.109 (associated with APT28 and APT29)\n",
      "* Domains: azuredrink[.]com, cloudemail[.]net ( potentially used for phishing or command and control)\n",
      "* User agent strings: \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\" ( potentially used to disguise the client app)\n",
      "\n",
      "Recommendations:\n",
      "\n",
      "* Investigate the user's account activity and access during the time range to determine if any unauthorized access occurred.\n",
      "* Monitor for any suspicious activity from the identified IP addresses or domains.\n",
      "* Analyze network traffic for potential command and control communication.\n",
      "* Update detection rules to include the identified IOCs.\n",
      "\n",
      "##End Report##\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.read_csv(\"dfp_detections_triaged.csv\").to_dict(orient='records')\n",
    "\n",
    "print(results[0]['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4faf7-7901-4d21-b2c3-72f6c89d0a86",
   "metadata": {},
   "source": [
    "## RAG Upload to User Summaries Vector Database\n",
    "\n",
    "Finally, we will upload all of the per-user event summary reports to a new vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bafebd2b-7c3f-4fcd-ae14-82c34020b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been merged into upload_intel/intel/user_summaries/merged.txt\n",
      "text file loaded\n",
      "Number of chunks from the document: 32\n"
     ]
    }
   ],
   "source": [
    "retriever_client_user_summaries = nrc.RetrieverClient() #create new retriever client\n",
    "\n",
    "base_directory_user_summaries = \"upload_intel/intel/user_summaries/\"\n",
    "input_files = [f\"{i}.txt\" for i in range(1, 4)]\n",
    "output_file = base_directory_user_summaries+\"merged.txt\"\n",
    "\n",
    "\n",
    "\n",
    "with open(output_file, 'w') as outfile:\n",
    "    # Iterate through the list of input files\n",
    "    for file_name in input_files:\n",
    "        with open(base_directory_user_summaries+file_name, 'r') as infile:\n",
    "            content = infile.read()\n",
    "            outfile.write(content)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(f\"Files have been merged into {output_file}\")\n",
    "\n",
    "#load in text file\n",
    "loader = TextLoader(output_file)\n",
    "\n",
    "document = loader.load()\n",
    "print(\"text file loaded\")\n",
    "\n",
    "\n",
    "document_chunks = retriever_client_user_summaries.add_files(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b5004-5cd1-484c-a8de-e676d2539607",
   "metadata": {},
   "source": [
    "## Testing Our RAG Vector Database\n",
    "Below, we can search using example user queries and see what the vector database might return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e502b52-f367-4577-bbb9-4eb807b47c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what we know about the user attacktarget:\n",
      "These IOCs should be correlated with the event to determine if they are related to the activity observed in this incident.\n",
      "##Start Report##\n",
      "\n",
      "**Event Overview**\n",
      "Username: attacktarget@domain.com\n",
      "Time Range: 2022-08-31 23:20:54 - 2022-08-31 23:54:50\n",
      "Apps: Box, Google Cloud / G Suite Connector by Microsoft, Spike Email - Mail & Team Chat, WeVideo\n",
      "Devices: ATTACKTARGET-LT, Windows 10, Chrome 100.0.4896\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Here's what we know about the user June:\n",
      "The event appears to be a collection of authentication logs for a single user, june@domain.com, over a long period of time. The logs are from various applications and devices, but a majority of them are from Box and Google Cloud / G Suite Connector by Microsoft. The time range of the event is from August 31, 2022, to May 30, 2024, which is a significant amount of time\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Here's what we know about the user Daniel:\n",
      "The event appears to be a collection of authentication logs for a single user, daniel@domain.com, over a long period of time. The logs are from various applications and devices, but a majority of them are from Box and Google Cloud / G Suite Connector by Microsoft. The time range of the event is from August 31, 2022, to May 30, 2024, which is a significant amount of time\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df1 = pd.DataFrame(retriever_client_user_summaries.search(\"tell me about attacktarget@domain.com\"))\n",
    "print(\"Here's what we know about the user attacktarget:\")\n",
    "print(df1[0][0])\n",
    "\n",
    "df2 = pd.DataFrame(retriever_client_user_summaries.search(\"tell me about june@domain.com\"))\n",
    "print(\"\\n\\n\\n\\n\\n\\nHere's what we know about the user June:\")\n",
    "print(df2[0][0])\n",
    "\n",
    "df3 = pd.DataFrame(retriever_client_user_summaries.search(\"tell me about daniel@domain.com\"))\n",
    "print(\"\\n\\n\\n\\n\\n\\nHere's what we know about the user Daniel:\")\n",
    "print(df3[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e618b5e-0318-457d-a050-414ef407f294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

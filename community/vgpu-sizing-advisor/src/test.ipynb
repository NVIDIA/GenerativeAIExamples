{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d8dfd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "125a1f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization: fp16\n",
      " num_gpu = 1, prompt_size = 2000 tokens, response_size = 1000 tokens\n",
      " n_concurrent_request = 1\n",
      "\n",
      "\n",
      "=== Memory Footprint ===\n",
      "| Model         |   Memory Footprint (GB) |   KV Size/token (GiB) |\n",
      "|---------------+-------------------------+-----------------------|\n",
      "| Llama-3-8B    |                   17.46 |              0.000488 |\n",
      "| Llama-3-70B   |                  147.32 |              0.002441 |\n",
      "| Llama-3.1-8B  |                   17.46 |              0.000488 |\n",
      "| Llama-3.1-70B |                  147.32 |              0.002441 |\n",
      "| Mistral-7B    |                   15.46 |              0.000488 |\n",
      "| Falcon-7B     |                   15.63 |              0.000542 |\n",
      "| Falcon-40B    |                   85.49 |              0.001831 |\n",
      "| Falcon-180B   |                  373.28 |              0.004425 |\n",
      "| Qwen-14B      |                   30.29 |              0.000763 |\n",
      "\n",
      "=== OOM Warnings ===\n",
      "!!!! Warning Llama-3-8B with A40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-8B with L40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-8B with L40S-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-70B with A40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-70B with A40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-70B with A40-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-70B with L40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-70B with L40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-70B with L40-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-70B with L40S-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-70B with L40S-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3-70B with L40S-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-8B with A40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-8B with L40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-8B with L40S-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-70B with A40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-70B with A40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-70B with A40-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-70B with L40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-70B with L40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-70B with L40-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-70B with L40S-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-70B with L40S-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Llama-3.1-70B with L40S-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Mistral-7B with A40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Mistral-7B with L40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Mistral-7B with L40S-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-7B with A40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-7B with L40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-7B with L40S-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-40B with A40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-40B with A40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-40B with A40-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-40B with L40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-40B with L40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-40B with L40-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-40B with L40S-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-40B with L40S-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-40B with L40S-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-180B with A40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-180B with A40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-180B with A40-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-180B with L40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-180B with L40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-180B with L40-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-180B with L40S-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-180B with L40S-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Falcon-180B with L40S-48Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Qwen-14B with A40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Qwen-14B with A40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Qwen-14B with L40-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Qwen-14B with L40-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Qwen-14B with L40S-12Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "!!!! Warning Qwen-14B with L40S-24Q: concurrent_requests=1 causes OOM\n",
      "Max concurrent_requests: 0 (context=3000 tokens)\n",
      "\n",
      "=== Capacity & Latency ===\n",
      "| Model         | GPU      |   Max KV Tokens |   TTFT (s) |   E2E Lat (s) |   Throughput (tok/s) |\n",
      "|---------------+----------+-----------------+------------+---------------+----------------------|\n",
      "| Llama-3-8B    | A40-12Q  |               0 |      0.306 |         92.38 |                10.82 |\n",
      "| Llama-3-8B    | A40-24Q  |           16384 |      0.153 |         46.19 |                21.65 |\n",
      "| Llama-3-8B    | A40-48Q  |           65536 |      0.077 |         23.1  |                43.3  |\n",
      "| Llama-3-8B    | L40-12Q  |               0 |      0.251 |         74.43 |                13.44 |\n",
      "| Llama-3-8B    | L40-24Q  |           16384 |      0.125 |         37.21 |                26.87 |\n",
      "| Llama-3-8B    | L40-48Q  |           65536 |      0.063 |         18.61 |                53.74 |\n",
      "| Llama-3-8B    | L40S-12Q |               0 |      0.249 |         74.42 |                13.44 |\n",
      "| Llama-3-8B    | L40S-24Q |           16384 |      0.124 |         37.21 |                26.87 |\n",
      "| Llama-3-8B    | L40S-48Q |           65536 |      0.062 |         18.61 |                53.75 |\n",
      "| Llama-3-70B   | A40-12Q  |               0 |      2.678 |        808.34 |                 1.24 |\n",
      "| Llama-3-70B   | A40-24Q  |               0 |      1.339 |        404.17 |                 2.47 |\n",
      "| Llama-3-70B   | A40-48Q  |               0 |      0.669 |        202.09 |                 4.95 |\n",
      "| Llama-3-70B   | L40-12Q  |               0 |      2.195 |        651.24 |                 1.54 |\n",
      "| Llama-3-70B   | L40-24Q  |               0 |      1.098 |        325.62 |                 3.07 |\n",
      "| Llama-3-70B   | L40-48Q  |               0 |      0.549 |        162.81 |                 6.14 |\n",
      "| Llama-3-70B   | L40S-12Q |               0 |      2.178 |        651.21 |                 1.54 |\n",
      "| Llama-3-70B   | L40S-24Q |               0 |      1.089 |        325.6  |                 3.07 |\n",
      "| Llama-3-70B   | L40S-48Q |               0 |      0.545 |        162.8  |                 6.14 |\n",
      "| Llama-3.1-8B  | A40-12Q  |               0 |      0.306 |         92.38 |                10.82 |\n",
      "| Llama-3.1-8B  | A40-24Q  |           16384 |      0.153 |         46.19 |                21.65 |\n",
      "| Llama-3.1-8B  | A40-48Q  |           65536 |      0.077 |         23.1  |                43.3  |\n",
      "| Llama-3.1-8B  | L40-12Q  |               0 |      0.251 |         74.43 |                13.44 |\n",
      "| Llama-3.1-8B  | L40-24Q  |           16384 |      0.125 |         37.21 |                26.87 |\n",
      "| Llama-3.1-8B  | L40-48Q  |           65536 |      0.063 |         18.61 |                53.74 |\n",
      "| Llama-3.1-8B  | L40S-12Q |               0 |      0.249 |         74.42 |                13.44 |\n",
      "| Llama-3.1-8B  | L40S-24Q |           16384 |      0.124 |         37.21 |                26.87 |\n",
      "| Llama-3.1-8B  | L40S-48Q |           65536 |      0.062 |         18.61 |                53.75 |\n",
      "| Llama-3.1-70B | A40-12Q  |               0 |      2.678 |        808.34 |                 1.24 |\n",
      "| Llama-3.1-70B | A40-24Q  |               0 |      1.339 |        404.17 |                 2.47 |\n",
      "| Llama-3.1-70B | A40-48Q  |               0 |      0.669 |        202.09 |                 4.95 |\n",
      "| Llama-3.1-70B | L40-12Q  |               0 |      2.195 |        651.24 |                 1.54 |\n",
      "| Llama-3.1-70B | L40-24Q  |               0 |      1.098 |        325.62 |                 3.07 |\n",
      "| Llama-3.1-70B | L40-48Q  |               0 |      0.549 |        162.81 |                 6.14 |\n",
      "| Llama-3.1-70B | L40S-12Q |               0 |      2.178 |        651.21 |                 1.54 |\n",
      "| Llama-3.1-70B | L40S-24Q |               0 |      1.089 |        325.6  |                 3.07 |\n",
      "| Llama-3.1-70B | L40S-48Q |               0 |      0.545 |        162.8  |                 6.14 |\n",
      "| Mistral-7B    | A40-12Q  |               0 |      0.268 |         80.83 |                12.37 |\n",
      "| Mistral-7B    | A40-24Q  |           20480 |      0.134 |         40.42 |                24.74 |\n",
      "| Mistral-7B    | A40-48Q  |           69632 |      0.067 |         20.21 |                49.48 |\n",
      "| Mistral-7B    | L40-12Q  |               0 |      0.22  |         65.12 |                15.36 |\n",
      "| Mistral-7B    | L40-24Q  |           20480 |      0.11  |         32.56 |                30.71 |\n",
      "| Mistral-7B    | L40-48Q  |           69632 |      0.055 |         16.28 |                61.42 |\n",
      "| Mistral-7B    | L40S-12Q |               0 |      0.218 |         65.12 |                15.36 |\n",
      "| Mistral-7B    | L40S-24Q |           20480 |      0.109 |         32.56 |                30.71 |\n",
      "| Mistral-7B    | L40S-48Q |           69632 |      0.054 |         16.28 |                61.42 |\n",
      "| Falcon-7B     | A40-12Q  |               0 |      0.268 |         80.83 |                12.37 |\n",
      "| Falcon-7B     | A40-24Q  |           18460 |      0.134 |         40.42 |                24.74 |\n",
      "| Falcon-7B     | A40-48Q  |           62766 |      0.067 |         20.21 |                49.48 |\n",
      "| Falcon-7B     | L40-12Q  |               0 |      0.22  |         65.12 |                15.36 |\n",
      "| Falcon-7B     | L40-24Q  |           18460 |      0.11  |         32.56 |                30.71 |\n",
      "| Falcon-7B     | L40-48Q  |           62766 |      0.055 |         16.28 |                61.42 |\n",
      "| Falcon-7B     | L40S-12Q |               0 |      0.218 |         65.12 |                15.36 |\n",
      "| Falcon-7B     | L40S-24Q |           18460 |      0.109 |         32.56 |                30.71 |\n",
      "| Falcon-7B     | L40S-48Q |           62766 |      0.054 |         16.28 |                61.42 |\n",
      "| Falcon-40B    | A40-12Q  |               0 |      1.53  |        461.91 |                 2.16 |\n",
      "| Falcon-40B    | A40-24Q  |               0 |      0.765 |        230.96 |                 4.33 |\n",
      "| Falcon-40B    | A40-48Q  |               0 |      0.383 |        115.48 |                 8.66 |\n",
      "| Falcon-40B    | L40-12Q  |               0 |      1.254 |        372.14 |                 2.69 |\n",
      "| Falcon-40B    | L40-24Q  |               0 |      0.627 |        186.07 |                 5.37 |\n",
      "| Falcon-40B    | L40-48Q  |               0 |      0.314 |         93.03 |                10.75 |\n",
      "| Falcon-40B    | L40S-12Q |               0 |      1.245 |        372.12 |                 2.69 |\n",
      "| Falcon-40B    | L40S-24Q |               0 |      0.622 |        186.06 |                 5.37 |\n",
      "| Falcon-40B    | L40S-48Q |               0 |      0.311 |         93.03 |                10.75 |\n",
      "| Falcon-180B   | A40-12Q  |               0 |      6.885 |       2078.6  |                 0.48 |\n",
      "| Falcon-180B   | A40-24Q  |               0 |      3.443 |       1039.3  |                 0.96 |\n",
      "| Falcon-180B   | A40-48Q  |               0 |      1.721 |        519.65 |                 1.92 |\n",
      "| Falcon-180B   | L40-12Q  |               0 |      5.645 |       1674.62 |                 0.6  |\n",
      "| Falcon-180B   | L40-24Q  |               0 |      2.822 |        837.31 |                 1.19 |\n",
      "| Falcon-180B   | L40-48Q  |               0 |      1.411 |        418.66 |                 2.39 |\n",
      "| Falcon-180B   | L40S-12Q |               0 |      5.601 |       1674.54 |                 0.6  |\n",
      "| Falcon-180B   | L40S-24Q |               0 |      2.801 |        837.27 |                 1.19 |\n",
      "| Falcon-180B   | L40S-48Q |               0 |      1.4   |        418.63 |                 2.39 |\n",
      "| Qwen-14B      | A40-12Q  |               0 |      0.536 |        161.67 |                 6.19 |\n",
      "| Qwen-14B      | A40-24Q  |               0 |      0.268 |         80.83 |                12.37 |\n",
      "| Qwen-14B      | A40-48Q  |           26214 |      0.134 |         40.42 |                24.74 |\n",
      "| Qwen-14B      | L40-12Q  |               0 |      0.439 |        130.25 |                 7.68 |\n",
      "| Qwen-14B      | L40-24Q  |               0 |      0.22  |         65.12 |                15.36 |\n",
      "| Qwen-14B      | L40-48Q  |           26214 |      0.11  |         32.56 |                30.71 |\n",
      "| Qwen-14B      | L40S-12Q |               0 |      0.436 |        130.24 |                 7.68 |\n",
      "| Qwen-14B      | L40S-24Q |               0 |      0.218 |         65.12 |                15.36 |\n",
      "| Qwen-14B      | L40S-48Q |           26214 |      0.109 |         32.56 |                30.71 |\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Estimate LLM memory, capacity, and latency with optional quantization\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_gpu\", type=int, default=1,\n",
    "        help=\"Number of GPUs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prompt_size\", type=int, default=2000,\n",
    "        help=\"Prompt size in tokens\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--response_size\", type=int, default=1000,\n",
    "        help=\"Response size in tokens\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_concurrent_request\", type=int, default=1,\n",
    "        help=\"Number of concurrent requests\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--quantization\", choices=[\"fp16\", \"int8\"], default=\"fp16\",\n",
    "        help=\"Quantization precision (fp16 or int8)\"\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    num_gpu = args.num_gpu\n",
    "    prompt_size = args.prompt_size\n",
    "    response_size = args.response_size\n",
    "    n_concurrent_request = args.n_concurrent_request\n",
    "    quantization = args.quantization\n",
    "\n",
    "    bytes_per_param = 2 if quantization == \"fp16\" else 1\n",
    "\n",
    "    print(f\"Quantization: {quantization}\")\n",
    "    print(f\" num_gpu = {num_gpu}, prompt_size = {prompt_size} tokens, response_size = {response_size} tokens\")\n",
    "    print(f\" n_concurrent_request = {n_concurrent_request}\\n\")\n",
    "\n",
    "    # Define vGPU specs with physical GPU memory to scale performance\n",
    "    gpu_specs = [\n",
    "        {\"name\": \"A40-12Q\", \"fp16_tflops\": 299, \"memory_gb\": 12, \"phy_memory_gb\": 48, \"bandwidth_gbps\": 696},\n",
    "        {\"name\": \"A40-24Q\", \"fp16_tflops\": 299, \"memory_gb\": 24, \"phy_memory_gb\": 48, \"bandwidth_gbps\": 696},\n",
    "        {\"name\": \"A40-48Q\", \"fp16_tflops\": 299, \"memory_gb\": 48, \"phy_memory_gb\": 48, \"bandwidth_gbps\": 696},\n",
    "        {\"name\": \"L40-12Q\", \"fp16_tflops\": 362, \"memory_gb\": 12, \"phy_memory_gb\": 48, \"bandwidth_gbps\": 864},\n",
    "        {\"name\": \"L40-24Q\", \"fp16_tflops\": 362, \"memory_gb\": 24, \"phy_memory_gb\": 48, \"bandwidth_gbps\": 864},\n",
    "        {\"name\": \"L40-48Q\", \"fp16_tflops\": 362, \"memory_gb\": 48, \"phy_memory_gb\": 48, \"bandwidth_gbps\": 864},\n",
    "        {\"name\": \"L40S-12Q\",\"fp16_tflops\": 366, \"memory_gb\": 12, \"phy_memory_gb\": 48, \"bandwidth_gbps\": 864},\n",
    "        {\"name\": \"L40S-24Q\",\"fp16_tflops\": 366, \"memory_gb\": 24, \"phy_memory_gb\": 48, \"bandwidth_gbps\": 864},\n",
    "        {\"name\": \"L40S-48Q\",\"fp16_tflops\": 366, \"memory_gb\": 48, \"phy_memory_gb\": 48, \"bandwidth_gbps\": 864},\n",
    "    ]\n",
    "\n",
    "    model_specs = [\n",
    "        {\"name\": \"Llama-3-8B\",   \"params_billion\": 8,   \"d_model\": 4096,   \"n_layers\": 32},\n",
    "        {\"name\": \"Llama-3-70B\",  \"params_billion\": 70,  \"d_model\": 8192,   \"n_layers\": 80},\n",
    "        {\"name\": \"Llama-3.1-8B\", \"params_billion\": 8,   \"d_model\": 4096,   \"n_layers\": 32},\n",
    "        {\"name\": \"Llama-3.1-70B\",\"params_billion\": 70,  \"d_model\": 8192,   \"n_layers\": 80},\n",
    "        {\"name\": \"Mistral-7B\",   \"params_billion\": 7,   \"d_model\": 4096,   \"n_layers\": 32},\n",
    "        {\"name\": \"Falcon-7B\",    \"params_billion\": 7,   \"d_model\": 4544,   \"n_layers\": 32},\n",
    "        {\"name\": \"Falcon-40B\",   \"params_billion\": 40,  \"d_model\": 8192,   \"n_layers\": 60},\n",
    "        {\"name\": \"Falcon-180B\",  \"params_billion\": 180, \"d_model\": 14848,  \"n_layers\": 80},\n",
    "        {\"name\": \"Qwen-14B\",     \"params_billion\": 14,  \"d_model\": 5120,   \"n_layers\": 40},\n",
    "    ]\n",
    "\n",
    "    BYTES_IN_GB = 1_073_741_824\n",
    "\n",
    "    def calc_kv_cache_size_per_token(n_layers, d_model):\n",
    "        elem_size = 1 if quantization == \"int8\" else 2\n",
    "        return 2 * elem_size * n_layers * d_model / BYTES_IN_GB\n",
    "\n",
    "    def calc_memory_footprint(model, concurrent, context):\n",
    "        kv_size = calc_kv_cache_size_per_token(model[\"n_layers\"], model[\"d_model\"])\n",
    "        return kv_size * context * concurrent + model[\"params_billion\"] * bytes_per_param\n",
    "\n",
    "    def calc_kv_cache_tokens(num_gpu, gpu_mem, params_billion, kv_size):\n",
    "        available = num_gpu * gpu_mem - params_billion * bytes_per_param\n",
    "        return max(available / kv_size, 0)\n",
    "\n",
    "    # Scale compute and bandwidth by profile fraction\n",
    "    def effective_flops(fp16_tflops, mem, phy_mem):\n",
    "        return fp16_tflops * (mem / phy_mem)\n",
    "\n",
    "    def effective_bandwidth(bandwidth, mem, phy_mem):\n",
    "        return bandwidth * (mem / phy_mem)\n",
    "\n",
    "    def calc_prefill_time(params_billion, gpu):\n",
    "        flops_eff = effective_flops(gpu[\"fp16_tflops\"], gpu[\"memory_gb\"], gpu[\"phy_memory_gb\"])\n",
    "        return (params_billion * bytes_per_param) / flops_eff / num_gpu\n",
    "\n",
    "    def calc_tpot(params_billion, gpu):\n",
    "        bw_eff = effective_bandwidth(gpu[\"bandwidth_gbps\"], gpu[\"memory_gb\"], gpu[\"phy_memory_gb\"])\n",
    "        return (params_billion * bytes_per_param) / bw_eff / num_gpu * 1000\n",
    "\n",
    "    def calc_e2e(prefill, tpot, in_size, out_size):\n",
    "        return (in_size * prefill + out_size * tpot) / 1000\n",
    "\n",
    "    context_window = prompt_size + response_size\n",
    "\n",
    "    # Memory Footprint\n",
    "    print(\"\\n=== Memory Footprint ===\")\n",
    "    mem_tbl = []\n",
    "    for m in model_specs:\n",
    "        mf = calc_memory_footprint(m, n_concurrent_request, context_window)\n",
    "        kv = calc_kv_cache_size_per_token(m[\"n_layers\"], m[\"d_model\"])\n",
    "        mem_tbl.append({\n",
    "            \"Model\": m[\"name\"],\n",
    "            \"Memory Footprint (GB)\": f\"{mf:.2f}\",\n",
    "            \"KV Size/token (GiB)\": f\"{kv:.6f}\" }\n",
    "        )\n",
    "    print(tabulate(mem_tbl, headers=\"keys\", tablefmt=\"orgtbl\"))\n",
    "\n",
    "    # OOM Warnings\n",
    "    print(\"\\n=== OOM Warnings ===\")\n",
    "    for m in model_specs:\n",
    "        for g in gpu_specs:\n",
    "            mf = calc_memory_footprint(m, n_concurrent_request, context_window)\n",
    "            available = num_gpu * g[\"memory_gb\"]\n",
    "            if mf > available:\n",
    "                max_req = int(calc_kv_cache_tokens(\n",
    "                    num_gpu, g[\"memory_gb\"], m[\"params_billion\"],\n",
    "                    calc_kv_cache_size_per_token(m[\"n_layers\"], m[\"d_model\"])) \n",
    "                    // context_window)\n",
    "                print(f\"!!!! Warning {m['name']} with {g['name']}: concurrent_requests={n_concurrent_request} causes OOM\")\n",
    "                print(f\"Max concurrent_requests: {max_req} (context={context_window} tokens)\")\n",
    "\n",
    "    # Capacity & Latency\n",
    "    print(\"\\n=== Capacity & Latency ===\")\n",
    "    cap_tbl = []\n",
    "    for m in model_specs:\n",
    "        kv = calc_kv_cache_size_per_token(m[\"n_layers\"], m[\"d_model\"])\n",
    "        for g in gpu_specs:\n",
    "            max_kv = calc_kv_cache_tokens(num_gpu, g[\"memory_gb\"], m[\"params_billion\"], kv)\n",
    "            pre = calc_prefill_time(m[\"params_billion\"], g)\n",
    "            tpot = calc_tpot(m[\"params_billion\"], g)\n",
    "            if any(isinstance(x, str) for x in (pre, tpot)):\n",
    "                e2e = throughput = ttft = \"OOM\"\n",
    "            else:\n",
    "                ttft = pre + tpot / 1000\n",
    "                e2e = calc_e2e(pre, tpot, prompt_size, response_size)\n",
    "                throughput = response_size / e2e if e2e > 0 else \"OOM\"\n",
    "            cap_tbl.append({\n",
    "                \"Model\": m[\"name\"],\n",
    "                \"GPU\": g[\"name\"],\n",
    "                \"Max KV Tokens\": int(max_kv),\n",
    "                \"TTFT (s)\": f\"{ttft:.3f}\",\n",
    "                \"E2E Lat (s)\": f\"{e2e:.2f}\" if isinstance(e2e, float) else e2e,\n",
    "                \"Throughput (tok/s)\": f\"{throughput:.2f}\"\n",
    "            })\n",
    "    print(tabulate(cap_tbl, headers=\"keys\", tablefmt=\"orgtbl\"))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477ba8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need a vGPU configuration for RAG using available GPU inventory: 1x NVIDIA L40S running Llama-3-8B using embedding model nvidia/nvolveqa-embed-large-1B with FP16 precision.\n",
      "{'Workload': 'RAG', 'Model': 'Llama-3-8B', 'Concurrent Users': 1, 'Precision': 'fp16'}\n",
      "\n",
      "I need a vGPU configuration for RAG with small (< 7b parameters) using available GPU inventory: 1x NVIDIA L40S using embedding model nvidia/nvolveqa-embed-large-1B using triton with FP16 precision.\n",
      "{'Workload': 'RAG', 'Model': 'Mistral-7B', 'Concurrent Users': 1, 'Precision': 'fp16'}\n",
      "\n",
      "Please set up a RAG pipeline for large models with INT8 precision for 3 simultaneous users.\n",
      "{'Workload': 'RAG', 'Model': 'Falcon-40B', 'Concurrent Users': 3, 'Precision': 'int8'}\n",
      "\n",
      "I need a vGPU configuration for RAG with extra large models using 2 concurrent users and int8 precision.\n",
      "{'Workload': 'RAG', 'Model': 'Llama-3.1-70B', 'Concurrent Users': 2, 'Precision': 'int8'}\n",
      "\n",
      "I need a vGPU configuration for RAG using available GPU inventory: 1x NVIDIA L40S running Llama-3.1-70B using embedding model nvidia/nvolveqa-embed-large-1B with FP16 precision.\n",
      "{'Workload': 'RAG', 'Model': 'Llama-3.1-70B', 'Concurrent Users': 1, 'Precision': 'fp16'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Define valid options\n",
    "VALID_MODELS = [\n",
    "    \"Llama-3-8B\", \"Llama-3-70B\", \"Llama-3.1-8B\", \"Llama-3.1-70B\",\n",
    "    \"Mistral-7B\", \"Falcon-7B\", \"Falcon-40B\", \"Falcon-180B\", \"Qwen-14B\"\n",
    "]\n",
    "VALID_PRECISIONS = [\"fp16\", \"int8\"]\n",
    "\n",
    "def parse_vgpu_query(query: str) -> dict:\n",
    "    \"\"\"Parse a natural language vGPU configuration query.\"\"\"\n",
    "    result = {\n",
    "        \"Workload\": None,\n",
    "        \"Model\": None,\n",
    "        \"Concurrent Users\": None,\n",
    "        \"Precision\": None\n",
    "    }\n",
    "    \n",
    "    # workload detection \n",
    "    for workload in [\"RAG\", \"LLM Inference\", \"Inference\"]:\n",
    "        if re.search(rf\"\\b{re.escape(workload)}\\b\", query, re.IGNORECASE):\n",
    "            result[\"Workload\"] = workload\n",
    "            break\n",
    "    \n",
    "\n",
    "\n",
    "    # 1) Explicit model mention\n",
    "    for model in VALID_MODELS:\n",
    "        if re.search(rf\"\\b{re.escape(model)}\\b\", query, re.IGNORECASE):\n",
    "            result[\"Model\"] = model\n",
    "            break\n",
    "    \n",
    "\n",
    "\n",
    "    # 2) Size-based fallback mapping\n",
    "    if not result[\"Model\"]:\n",
    "        # small: <7b parameters\n",
    "        if re.search(r\"<\\s*7\\s*[bB]\", query) or re.search(r\"\\bsmall\\b\", query, re.IGNORECASE):\n",
    "            result[\"Model\"] = \"Mistral-7B\"\n",
    "        # medium: >=7b and <=14b parameters or 'medium' keyword\n",
    "        elif re.search(r\"\\bmedium\\b\", query, re.IGNORECASE):\n",
    "            result[\"Model\"] = \"Llama-3-8B\"\n",
    "        # large: >14b parameters or 'large' keyword\n",
    "        elif re.search(r\"\\blarge\\b\", query, re.IGNORECASE):\n",
    "            if re.search(r\"\\bextra\\b\", query, re.IGNORECASE):\n",
    "                result[\"Model\"] = \"Llama-3.1-70B\"\n",
    "            else:\n",
    "                result[\"Model\"] = \"Falcon-40B\"\n",
    "    \n",
    "    # 3) Concurrent users\n",
    "    user_match = re.search(r\"(\\d+)\\s*(?:concurrent|simultaneous)?\\s*users?\", query, re.IGNORECASE)\n",
    "    if user_match:\n",
    "        result[\"Concurrent Users\"] = int(user_match.group(1))\n",
    "    \n",
    "    # 4) Precision\n",
    "    prec_match = re.search(r\"\\b(fp16|int8)\\b\", query, re.IGNORECASE)\n",
    "    if prec_match:\n",
    "        precision = prec_match.group(1).lower()\n",
    "        if precision in VALID_PRECISIONS:\n",
    "            result[\"Precision\"] = precision\n",
    "    \n",
    "    # 5) Default precision if not specified\n",
    "    if not result[\"Precision\"]:\n",
    "        result[\"Precision\"] = \"fp16\"\n",
    "    if not result[\"Model\"]:\n",
    "        result[\"Model\"] = \"Llama-3-8B\"\n",
    "    if not result[\"Concurrent Users\"]:\n",
    "        result[\"Concurrent Users\"] = 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "queries = [\n",
    "    \"I need a vGPU configuration for RAG using available GPU inventory: 1x NVIDIA L40S running Llama-3-8B using embedding model nvidia/nvolveqa-embed-large-1B with FP16 precision.\",\n",
    "    \"I need a vGPU configuration for RAG with small (< 7b parameters) using available GPU inventory: 1x NVIDIA L40S using embedding model nvidia/nvolveqa-embed-large-1B using triton with FP16 precision.\",\n",
    "    \"Please set up a RAG pipeline for large models with INT8 precision for 3 simultaneous users.\",\n",
    "    \"I need a vGPU configuration for RAG with extra large models using 2 concurrent users and int8 precision.\",\n",
    "    \"I need a vGPU configuration for RAG using available GPU inventory: 1x NVIDIA L40S running Llama-3.1-70B using embedding model nvidia/nvolveqa-embed-large-1B with FP16 precision.\",\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(q)\n",
    "    print(parse_vgpu_query(q))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

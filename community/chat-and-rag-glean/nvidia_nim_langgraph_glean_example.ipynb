{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9fd722a-a31b-4663-ba12-b86967cad231",
   "metadata": {},
   "source": [
    "# Enterprise Chat Example\n",
    "\n",
    "This notebook creates an example of a LLM agent that can answer user questions about an internal company knowledge base, such as: \n",
    "\n",
    "- \"Do I need to use PTO if I am sick?\"\n",
    "- \"What if I am sick for a long time?\"\n",
    "\n",
    "The notebook uses: \n",
    "- NVIDIA NIMs for foundational LLMs and embedding models\n",
    "- Glean for storing the corporate knowledge base\n",
    "- Glean Search API for accessing the Glean knowledge base\n",
    "- Chroma DB for storing cached query results and performing RAG\n",
    "- LangGraph for creating an agent\n",
    "\n",
    "Best of all, because both Glean and NVIDIA NIMs can be deployed in your private environment, it is possible to create this type of enterprise chatbot without any data leaving your control.\n",
    "\n",
    "To get started with this notebook, set the following environment variables. You will need a Glean deployment, a Glean API key, and a [NVIDA API Key](https://build.nvidia.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5c3a4-8fb1-4945-816a-9bf79730a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export GLEAN_API_KEY=\"YOUR GLEAN API KEY\"\n",
    "!export GLEAN_API_BASE_URL=\"https://your-org.glean.com/rest/api/v1\"\n",
    "!export NVIDIA_API_KEY=\"nvapi-YOUR NVIDIA API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2689f-335d-4203-8121-530641257de9",
   "metadata": {},
   "source": [
    "We start by instantiating the LLM and embedding model. You can update this code to use different foundational LLMs, or add the `base_url` parameter if you are using private NVIDIA NIM microservices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715dd76-01c6-4cbd-9409-e25b9b1e9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "model = ChatNVIDIA(\n",
    "    model=\"meta/llama-3.3-70b-instruct\", api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    ")\n",
    "embeddings = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"),\n",
    "    truncate=\"NONE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa1551-bde5-4d13-95ad-775b84b5335f",
   "metadata": {},
   "source": [
    "We test the foundational LLM by asking it a simple question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512b8a4-2907-4b38-96a2-10b0c08259a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"do I need to use PTO if I am sick?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beda74c-7885-4885-babd-8166a049fd38",
   "metadata": {},
   "source": [
    "While the model is able to interpret our question and formulate a response, it does not have access to any information about company-specific policies. To add this type of information we will follow a multi-step process: \n",
    "\n",
    "1. Have the LLM interpret the user's question and add any relevant context. Most free form questions can be passed directly to the Glean Search API.\n",
    "2. Add relevant context about the user and then query the Glean knowledge base using the Glean search API to get the most relevant supporting documents. \n",
    "3. Embed those supporting documents into a local vector DB.\n",
    "4. Use a retriever model to fetch the most relevant supporting document based on the user's original question.\n",
    "5. Take the most relevant supporting document and add it to the LLM by adding it to the LLM's prompt (RAG).\n",
    "6. Ask the LLM to summarize the results and answer the user's question with this new relevant context.\n",
    "\n",
    "To help organize these steps we use a LangGraph agent. The full implementation of the agent is available in the file `glean_example/src/agent.py`. The following code samples explain some core concepts of that code.\n",
    "\n",
    "```python\n",
    "class InfoBotState(BaseModel):\n",
    "    messages: List[Tuple[str, str]] = None\n",
    "    glean_query_required: Optional[bool] = None\n",
    "    glean_results: Optional[List[str]] = None\n",
    "    db: Optional[Any] = None\n",
    "    answer_candidate: Optional[str] = None\n",
    "\n",
    "graph = StateGraph(InfoBotState)\n",
    "graph.add_node(\"determine_user_intent\", determine_user_intent)\n",
    "graph.add_node(\"call_glean\", call_glean)\n",
    "graph.add_node(\"add_embeddings\", add_embeddings)\n",
    "graph.add_node(\"answer_candidates\", answer_candidates)\n",
    "graph.add_node(\"summarize_answer\", summarize_answer)\n",
    "graph.add_edge(START, \"determine_user_intent\")\n",
    "graph.add_conditional_edges(\n",
    "    \"determine_user_intent\",\n",
    "    route_glean, \n",
    "    {\"call_glean\": \"call_glean\", \"summarize_answer\": \"summarize_answer\"}\n",
    ")\n",
    "graph.add_edge(\"call_glean\", \"add_embeddings\")\n",
    "graph.add_edge(\"add_embeddings\", \"answer_candidates\")\n",
    "graph.add_edge(\"answer_candidates\", \"summarize_answer\")\n",
    "graph.add_edge(\"summarize_answer\", END)\n",
    "agent = graph.compile()\n",
    "\n",
    "```\n",
    "\n",
    "This code is responsible for creating the agent. Each node represents a function responsible for implementing one of the six steps in our process. The `InfoBotState` is a special type of dictionary that will hold all of the information the agent needs through each step of the process. \n",
    "\n",
    "The source of each function is also available in `glean_example/src/agent.py`. For example, the implementation of `call_bot` is: \n",
    "\n",
    "```python\n",
    "def summarize_answer(state: InfoBotState):\n",
    "    \"\"\"the main agent responsible for taking all the context and answering the question\"\"\"\n",
    "    logger.info(\"Generate final answer\")\n",
    "\n",
    "    llm = PROMPT_ANSWER | model\n",
    "\n",
    "    response = llm.invoke(\n",
    "        {\n",
    "            \"messages\": state.messages,\n",
    "            \"glean_search_result_documents\": state.glean_results,\n",
    "            \"answer_candidate\": state.answer_candidate,\n",
    "        }\n",
    "    )\n",
    "    state.messages.append((\"agent\", response.content))\n",
    "    return state\n",
    "```\n",
    "\n",
    "This function takes the NVIDIA NIM foundational LLM model and invokes it with a specific prompt and the information available in the agent state. The prompt tells the agent what to do, injecting the relevant information from the agent state. You can see the prompts in the file `glean_example/src/prompts.py`. For example, the `PROMPT_ANSWER` is: \n",
    "\n",
    "```raw\n",
    "You are the final part of an agent graph. Your job is to answer the user's question based on the information below. Include a url citation in your answer.\n",
    "\n",
    "Message History: {messages}\n",
    "\n",
    "All Supporting Documents from Glean: \n",
    "\n",
    "{glean_search_result_documents}\n",
    "\n",
    "Content from the most relevant document that you should prioritize: \n",
    "\n",
    "{answer_candidate}\n",
    "\n",
    "Answer: \n",
    "\n",
    "Citation Url: \n",
    "```\n",
    "\n",
    "A main part of this agent is the step that calls the Glean Search API. This RESTful request is implemented in the file `glean_example/src/glean_utils/utils.py`: \n",
    "\n",
    "```python\n",
    "def glean_search(query, api_key, base_url, **kwargs):\n",
    "    endpoint = f\"{base_url}/search\"\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"pageSize\": kwargs.get(\"page_size\", 10),\n",
    "        \"requestOptions\": {},\n",
    "    }\n",
    "\n",
    "    # Add optional parameters\n",
    "    if \"cursor\" in kwargs:\n",
    "        payload[\"cursor\"] = kwargs[\"cursor\"]\n",
    "\n",
    "    if \"facet_filters\" in kwargs:\n",
    "        payload[\"requestOptions\"][\"facetFilters\"] = kwargs[\"facet_filters\"]\n",
    "\n",
    "    if \"timeout_millis\" in kwargs:\n",
    "        payload[\"timeoutMillis\"] = kwargs[\"timeout_millis\"]\n",
    "\n",
    "    try:\n",
    "        response = requests.post(endpoint, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        result = {\n",
    "            \"status_code\": response.status_code,\n",
    "            \"request_id\": data.get(\"requestID\"),\n",
    "            \"results\": data.get(\"results\", []),\n",
    "            \"facet_results\": data.get(\"facetResults\", []),\n",
    "            \"cursor\": data.get(\"cursor\"),\n",
    "            \"has_more_results\": data.get(\"hasMoreResults\", False),\n",
    "            \"tracking_token\": data.get(\"trackingToken\"),\n",
    "            \"backend_time_millis\": data.get(\"backendTimeMillis\"),\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise e\n",
    "```\n",
    "\n",
    "To try out the agent we can load the `glean_example` source code and invoke the full agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d7278-172d-4d25-a039-f1a4c9d758c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glean_example.src.agent import agent\n",
    "\n",
    "msg = \"What's the latest on the new API project?\"\n",
    "history = []\n",
    "history.append((\"user\", msg))\n",
    "response = agent.invoke({\"messages\": history})\n",
    "print(response[\"messages\"][-1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

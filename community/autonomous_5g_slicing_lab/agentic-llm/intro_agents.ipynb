{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Nvidia Logo](./images/nvidia.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to LLM Agents in LangGraph and LangChain\n",
    "\n",
    "This notebook provides an introduction to building LLM-based agents using LangGraph and LangChain. It covers the fundamental implementations required for developing agentic workflows. By the end of this notebook, you will have an understanding of the following concepts:\n",
    "\n",
    "1. [NVIDIA NIM Endpoints](https://build.nvidia.com/explore/discover) – Learn how to integrate and use NVIDIA NIM endpoints for efficient inference.\n",
    "2. [Tool Calling using `bind_tools`](https://python.langchain.com/docs/modules/agents/tools/custom_tools/) – Understand how to define and bind tools within an agent.\n",
    "3. [Agents using `create_react_agent`](https://python.langchain.com/docs/modules/agents/) – Explore the creation of ReAct (Reasoning + Acting) agents using Langgraph.\n",
    "\n",
    "This notebook serves as a introduction to implementing intelligent agents with modular and scalable workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Nvidia NIM endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of available models, we will be using `meta/llama-3.1-70b-instruct` for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_models = [model for model in ChatNVIDIA.get_available_models() if model.supports_tools]\n",
    "for elem in tool_models:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up the LLM through LangChain's ChatNVIDIA functionality, which provides an interface to NVIDIA NIM chat models. It offers connection to both hosted and local NIMs (Mistral, Llama, etc.), tool calling capabilities, streaming functionality, etc. Here is an example on how to implement that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =  ChatNVIDIA(\n",
    "        model= \"meta/llama-3.1-70b-instruct\",\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=4096,\n",
    ")\n",
    "\n",
    "# Locally-hosted model example\n",
    "# llm = ChatNVIDIA(base_url=\"http://3.145.171.211:8000/v1\", model_name=\"meta/llama-3.1-8b-instruct\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a computer program, so I don't have feelings, but thanks for asking! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Hi How are you\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling with `.bind_tools()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain, [tool calling](https://python.langchain.com/docs/concepts/tool_calling/) allows LLMs to invoke external functions, APIs, or utilities dynamically, extending LLM capabilities beyond text generation. We define tools with the `@tool` decorator, and bind them to llm with `.bind_tool()` function. That tells the LLM which tools are available for using. LLM calls these functions with the proper arguments depending on the prompt it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/.local/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:591: UserWarning: Model 'nvdev/meta/llama-3.1-70b-instruct' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b and return result\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a*b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM intelligently decides where to call a tool or not depending on the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'role': 'assistant', 'content': \"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", 'token_usage': {'prompt_tokens': 283, 'total_tokens': 307, 'completion_tokens': 24}, 'finish_reason': 'stop', 'model_name': 'nvdev/meta/llama-3.1-70b-instruct'}, id='run-34a9624d-e628-4af5-a5c4-f3ceafc8533a-0', usage_metadata={'input_tokens': 283, 'output_tokens': 24, 'total_tokens': 307}, role='assistant')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 56, 'b': 64},\n",
       "  'id': 'chatcmpl-tool-fb0629aaebde42b4ba91e4d36c93a431',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"What is the multiplication of 56 and 64\")\n",
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.bind_tools()`, LLM tells us which tool to call, with which arguments. But it doesn't actually invoke the tool. To invoke the tool intelligently we use the `.create_react_agent()` prebuilt function from Langgraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prebuilt ReAcT agent using  `.create_react_agent()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReAct (Reasoning + Acting) is an agent architecture, based on this paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629), that combines step-by-step reasoning with tool use. LangGraph provides a prebuilt function [create_react_agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/) to easily implement this architecture. It uses `.bind_tools()` under the hood to attach tools to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/.local/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:591: UserWarning: Model 'nvdev/meta/llama-3.1-405b-instruct' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Tool 1: Square a Number\n",
    "@tool\n",
    "def square(n: int) -> int:\n",
    "    \"\"\"Returns the square of a number.\"\"\"\n",
    "    return n * n\n",
    "\n",
    "# Define Tool 2: Multiply Two Numbers\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "agent = create_react_agent(llm, [square, multiply])\n",
    "\n",
    "# Invoke the agent with a query that requires two tool calls\n",
    "response = agent.invoke({\"messages\": \"What is (5 squared) multiplied by 2?\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer is 50.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is (5 squared) multiplied by 2?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  square (chatcmpl-tool-3fbc1698aad645b7a9b1a38ebfc3c1f4)\n",
      " Call ID: chatcmpl-tool-3fbc1698aad645b7a9b1a38ebfc3c1f4\n",
      "  Args:\n",
      "    n: 5\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: square\n",
      "\n",
      "25\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (chatcmpl-tool-557233293c6c4011a6d54fcfd2633d88)\n",
      " Call ID: chatcmpl-tool-557233293c6c4011a6d54fcfd2633d88\n",
      "  Args:\n",
      "    a: 25\n",
      "    b: 2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "50\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The answer is 50.\n"
     ]
    }
   ],
   "source": [
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are familiar with the basics of building agents, we will explore how to build a 5G network reconfiguration agent. Please refer to [agentic_pipeline-DLI.ipynb](agentic_pipeline-DLI.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/.local/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:591: UserWarning: Model 'nvdev/meta/llama-3.1-70b-instruct' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = llm\n",
    "# For this tutorial we will use custom tool that returns pre-defined values for weather in two cities (NYC & SF)\n",
    "@tool\n",
    "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if city == \"nyc\":\n",
    "        return \"It might be cloudy in nyc\"\n",
    "    elif city == \"sf\":\n",
    "        return \"It's always sunny in sf\"\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown city\")\n",
    "\n",
    "\n",
    "tools = [get_weather]\n",
    "\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    \"\"\"Respond to the user in this format.\"\"\"\n",
    "\n",
    "    conditions: str = Field(description=\"Weather conditions\")\n",
    "\n",
    "\n",
    "# Define the graph\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model,\n",
    "    tools=tools,\n",
    "    # specify the schema for the structured output using `response_format` parameter\n",
    "    response_format=WeatherResponse,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/.local/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:814: UserWarning: Model 'nvdev/meta/llama-3.1-70b-instruct' is not known to support structured output. Your output may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"What's the weather in NYC?\")]}\n",
    "response = graph.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeatherResponse(conditions='Cloudy')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

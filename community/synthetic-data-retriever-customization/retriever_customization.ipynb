{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Customization - Fine-Tuning & Evaluation (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors - Aditya Malte, Vinay Raman, Ali Taghibakhshi, Dora Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This is part two of a two-part series. \n",
    "1. `synthetic_data_generation_nemo.ipynb`:\n",
    "    - Use an LLM from build.nvidia.com (or deploy your own using NIM!) to create training examples containing generated queries and positive chunks. By default the notebook will use nfcorpus, but you can easily swap in your own data.\n",
    "    - Save results to a `.jsonl` file \n",
    "\n",
    "\n",
    "2. `retriever_customization.ipynb` **(this notebook)**:\n",
    "    - Implement hard negative mining to find challenging negative examples\n",
    "    - Use the generated training data in the `.jsonl` file to fine-tune a retriever model using Nemo Framework\n",
    "    - Evaluate the results of your fine-tuned embedding model against the original using BeIR Benchmark\n",
    "    \n",
    "A GPU is required to run this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NeMo Framework Docker container\n",
    "This notebook requires the NeMo Framework Docker container. Download the appropriate Docker image and build the container when inside the `synthetic-data-retriever-customization` directory using this command: \n",
    "\n",
    "`docker run -it --rm --gpus all --ipc=host --network host -v $(pwd):/workspace nvcr.io/nvidia/nemo:24.07`\n",
    "\n",
    "This notebook was tested on a setup comprising of 1xL40S GPUs with CUDA setup.\n",
    "\n",
    "\n",
    "#### NVIDIA AI Endpoints\n",
    "As in Notebook 1, you'll use another API endpoint from [www.build.nvidia.com](https://www.build.nvidia.com) in Notebook 2, this time for generating embeddings with the text embedding model [NV-EmbedQA-E5-V5](https://build.nvidia.com/nvidia/nv-embedqa-e5-v5). You can reuse the same API Key as before, or generate a new one by clicking the link to the model. \n",
    "\n",
    "\n",
    "#### Download NV-Embed-QA-4 model weights from NGC\n",
    "Use the command `ngc registry model download-version \"ohlfw0olaadg/ea-participants/nv-embed-qa:4\"` to download the NeMo Retriever model. It must be downloaded to the directory `files/models`. The same model - NeMo Retriever - has been used as an example in this notebook. If you do not have NVAIE access, then you may download and convert a HF embedding like `intfloat/e5-large-unsupervised` for your purpose as follows:\n",
    "```\n",
    "/NeMo/scripts/nlp_language_modeling/convert_bert_hf_to_nemo.py \\\n",
    "       --input_name_or_path \"intfloat/e5-large-unsupervised\" \\\n",
    "       --output_path /workspace/files/models/my_model.nemo\n",
    "```\n",
    "\n",
    "For the purpose of this notebook, we have used the NeMo Retriever model. If you use another model, or convert an HF model, ensure that the model path is updated accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install beir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import math\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This should be the synthetic dataset generated in Part 1, consisting of the queries, pos_doc, and neg_docs\n",
    "QA_PAIRS_PATH = \"/workspace/files/data/qa_pairs_nvidia-nemotron-4-340b-instruct_num_queries_300_BeIR_nfcorpus.csv\"\n",
    "\n",
    "# Specify the path where the fine-tuning dataset will be saved\n",
    "OUTPUT_DATA_PATH = \"/tmp/data/output_data.jsonl\"\n",
    "output_dir_path = os.path.dirname(OUTPUT_DATA_PATH)\n",
    "if not os.path.exists(output_dir_path):\n",
    "    os.mkdir(output_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DEVICES=1 # number of gpus available for fine-tuning\n",
    "\n",
    "# Use the default config for BERT Embedding Model\n",
    "CONFIG_PATH=\"/opt/NeMo/examples/nlp/information_retrieval/conf/\"\n",
    "CONFIG_NAME=\"megatron_bert_embedding_config\"\n",
    "\n",
    "PATH_TO_NEMO_MODEL= \"/workspace/files/models/NV-Embed-QA-4.nemo\" # Path to converted nemo model from hf, if you have a different model\n",
    "DATASET_PATH= OUTPUT_DATA_PATH # Path to jsonl dataset\n",
    "SAVE_DIR= \"/tmp/trained_model/\" # where the checkpoint and logs are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read QA Pairs CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = pd.read_csv(QA_PAIRS_PATH).sample(frac=1).reset_index(drop=True)\n",
    "qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Mining Hard Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard negative mining refers to the creation of negative examples that are 'hard'. Essentially, what this means is that rather than performing random sampling - which would lead to easy negatives - we mine for harder negative examples.\n",
    "\n",
    "This has an advantage that the negatives would not be obvious to the model during training, and hence would actually be more helpful.\n",
    "\n",
    "However, hard negative mining has a higher probability of generating false negatives. To avoid this, we set a safety `margin`. This margin is a hyperparameter and you may change it depending on if more false negatives are being generated. For instance, a larger corpus has a higher probability of generating false negatives than a smaller one, as the probability of finding another positive increases. In such cases a lower `margin` value may be more helpful.\n",
    "\n",
    "#### NV-EmbedQA-E5-V4\n",
    "To do hard negative mining, we'll need to create embeddings for all of our text chunks using the [NV-EmbedQA-E5-V5](https://build.nvidia.com/nvidia/nv-embedqa-e5-v5) model from www.build.nvidia.com. You can reuse the same NVIDIA_API_KEY as before. \n",
    "\n",
    "Since the NV-EmbedQA-E5-V5 model is quite small, you can also easily host it as self-deployed NIM Docker container following the instructions [here](https://build.nvidia.com/nvidia/nv-embedqa-e5-v5?snippet_tab=Docker). If you already have the model weights for .nemo format embedding model downloaded in preparation for fine-tuning, you can also restore the model using NeMo Framework. To do that, simply copy the encode_text() function from the evaluation section of this notebook and use it here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeIR\n",
    "BeIR is a heterogeneous benchmark containing diverse IR tasks. It also provides a common and easy framework for evaluation of your NLP-based retrieval models within the benchmark [source](https://github.com/beir-cellar/beir). First we'll do some basic processing so that our synthetic dataset matches the BeIR format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = OrderedDict()\n",
    "queries = []\n",
    "positive_passage_ids = []\n",
    "for _, row in qa_pairs.iterrows():\n",
    "    queries.append(row[\"query\"])\n",
    "    positive_passage_str = row[\"positive_chunk\"]\n",
    "    if(positive_passage_str in passages):\n",
    "        positive_passage_id = passages[positive_passage_str]\n",
    "        positive_passage_ids.append(positive_passage_id)\n",
    "    else:\n",
    "        positive_passage_id = len(passages)\n",
    "        passages[positive_passage_str] = positive_passage_id\n",
    "        positive_passage_ids.append(positive_passage_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[12], positive_passage_ids[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings for all Queries and Positive Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_client = AsyncOpenAI(\n",
    "    base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key = os.environ[\"NVIDIA_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_text(client, text, input_type):\n",
    "    try:\n",
    "        response = await client.embeddings.create(\n",
    "            input=[text],\n",
    "            model=\"nvidia/nv-embedqa-e5-v5\",\n",
    "            encoding_format=\"float\",\n",
    "            extra_body={\"input_type\": input_type, \"truncate\": \"END\"}\n",
    "        )\n",
    "\n",
    "        if hasattr(response, 'data') and len(response.data) > 0:\n",
    "            return response.data[0].embedding\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {str(e)}\"\n",
    "    \n",
    "\n",
    "async def batch_encode_text(client, all_texts, input_type):\n",
    "    tasks = [encode_text(client, text, input_type) for text in all_texts]\n",
    "    results_list = await asyncio.gather(*tasks)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = await batch_encode_text(embedding_client, [(\"query: \"+query) for query in queries], \"query\")\n",
    "passage_embeddings = await batch_encode_text(embedding_client, [(\"passage: \"+passage) for passage in list(passages)], \"passage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NV-EmbedQA-V4 uses the keys \"query\" and \"passage\" but this may differ between models. Ensure you are using the correct keys for your model, otherwise you'll hit an error during fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Hard Negatives Using Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negative_mining(\n",
    "        query_embeddings,\n",
    "        passage_embeddings,\n",
    "        batch_size,\n",
    "        margin, \n",
    "        num_negs,\n",
    "        query_positive_paragraph_idxs\n",
    "):\n",
    "    hard_negative_idxs = []\n",
    "    num_batches = int(math.ceil(query_embeddings.shape[0] / batch_size))\n",
    "    # Split the query embeddings into batches of given batch size\n",
    "    for current_batch_idx in range(num_batches):\n",
    "        start = (current_batch_idx)*batch_size\n",
    "        end = (current_batch_idx+1)*(batch_size)\n",
    "        batch_query_embeddings = query_embeddings[start:end]\n",
    "        batch_query_positive_paragraph_idxs = query_positive_paragraph_idxs[start:end]\n",
    "        \n",
    "        # Find minimum query-positive_chunk similarity score for each query in a batch\n",
    "        query_passage_pos_scores = np.matmul(batch_query_embeddings, passage_embeddings.T)\n",
    "\n",
    "        min_pos_scores = []\n",
    "        for query_id, row in enumerate(query_passage_pos_scores):\n",
    "            min_value = float(\"inf\")\n",
    "            for query_positive_paragraph_idx in query_positive_paragraph_idxs[query_id+start]:\n",
    "                min_value = min(min_value, row[query_positive_paragraph_idx])\n",
    "            min_pos_scores.append(min_value)\n",
    "        min_pos_scores = np.array(min_pos_scores)\n",
    "            \n",
    "        # For each query set minimum threshold as margin*minimum_batch_positive_score \n",
    "        mining_thresholds = min_pos_scores*margin\n",
    "        \n",
    "        # Filter out all chunks belonging to the same paragraph as positive passage OR those manually labelled as positives\n",
    "        for query_idx, positive_paragraph_idxs in enumerate(batch_query_positive_paragraph_idxs):\n",
    "            batch_query_idx = query_idx%batch_size\n",
    "            query_passage_pos_scores[batch_query_idx][positive_paragraph_idxs] = -float(\"inf\")\n",
    "        \n",
    "        # Filter out all chunks with score>mining_threshold\n",
    "        for row_idx in range(query_passage_pos_scores.shape[0]):\n",
    "            row = query_passage_pos_scores[row_idx]\n",
    "            row[row>mining_thresholds[row_idx]] = -float(\"inf\")\n",
    "            \n",
    "        # For each query get top_k hard negatives from all that remains\n",
    "        for row in query_passage_pos_scores:\n",
    "            top_k_hard_negative_idxs = np.argpartition(row, -num_negs)[-num_negs:]\n",
    "            hard_negative_idxs.append(list(top_k_hard_negative_idxs))\n",
    "            \n",
    "    return hard_negative_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set a margin of 0.95 to prevent false negatives and we mine 5 negative docs (num_negs)\n",
    "query_embeddings = torch.tensor(query_embeddings).numpy()\n",
    "passage_embeddings = torch.tensor(passage_embeddings).numpy()\n",
    "\n",
    "positive_passage_ids_list = [[element] for element in positive_passage_ids]\n",
    "hard_negative_idxs = hard_negative_mining(query_embeddings=query_embeddings, passage_embeddings=passage_embeddings, query_positive_paragraph_idxs=positive_passage_ids_list,\n",
    "                    batch_size=32, num_negs=5, margin=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use similarity score with the `margin` variable to generate hard negatives. For this example we generate 5 hard negatives, but you can change this number. Ultimately the data will be stored in the following format: \n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"query\": \"Query\",\n",
    "        \"pos_doc\": [\"Positive\"],\n",
    "        \"neg_doc\": [\"Negative_1\", \"Negative_2\", ..., \"Negative_n\"]\n",
    "    },\n",
    "    {\n",
    "        // Next data instance\n",
    "    },\n",
    "    ...,\n",
    "    {\n",
    "        // Subsequent data instance\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for query_id, query in enumerate(queries):\n",
    "    hard_negative_passages = []\n",
    "    for hard_negative_idx in hard_negative_idxs[query_id]:\n",
    "        for key, val in passages.items():\n",
    "            if val == hard_negative_idx:\n",
    "                hard_negative_passage = key\n",
    "                hard_negative_passages.append(hard_negative_passage)\n",
    "    \n",
    "    for key, val in passages.items():\n",
    "        if val == positive_passage_ids[query_id]:\n",
    "            positive_passage = key\n",
    "            break\n",
    "\n",
    "    datapoint = {\n",
    "        \"query\" : query,\n",
    "        \"pos_doc\" : positive_passage,\n",
    "        \"neg_doc\" : hard_negative_passages\n",
    "    }\n",
    "    data.append(datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print('query: ', data[0]['query'])\n",
    "print('pos_doc: ', data[0]['pos_doc'])\n",
    "print('neg_doc: ', data[0]['neg_doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to JSONL file\n",
    "print(f\"Saving data to: {OUTPUT_DATA_PATH}\")\n",
    "\n",
    "with open(OUTPUT_DATA_PATH, \"w\") as f:\n",
    "    for entry in data:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `megatron_bert_embedding_finetuning.py` script. This script sets up and trains a Megatron-BERT model using  NVIDIA NeMo Framework, with configurations managed by Hydra. It loads the pre-trained `.nemo` model from a checkpoint, adjusts settings like batch size, and sets up parallel processing for multi-GPU training. Finally, it initializes the trainer and starts the training process with the NeMo Framework Megatron Trainer. \n",
    "\n",
    "Note `model.global_batch_size = model.micro_batch_size * trainer.devices (aka # of GPUs)`. Please keep micro_batch_size=4 and set the other parameters accordingly. \n",
    "\n",
    "`model.data.hard_negatives_to_train` should be set to the number of neg_docs corresponding to each query in your synthetic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMAND = f\"python /opt/NeMo/examples/nlp/information_retrieval/megatron_bert_embedding_finetuning.py \\\n",
    "--config-path={CONFIG_PATH} \\\n",
    "--config-name={CONFIG_NAME} \\\n",
    "restore_from_path={PATH_TO_NEMO_MODEL} \\\n",
    "trainer.devices={NUM_DEVICES} \\\n",
    "trainer.val_check_interval=10 \\\n",
    "trainer.max_epochs=1 \\\n",
    "+trainer.num_sanity_val_steps=0 \\\n",
    "trainer.max_steps=100000 \\\n",
    "model.global_batch_size=4 \\\n",
    "model.micro_batch_size=4 \\\n",
    "model.mcore_bert=False \\\n",
    "model.tokenizer.library=huggingface \\\n",
    "model.tokenizer.type=intfloat/e5-large-unsupervised \\\n",
    "model.megatron_legacy=True \\\n",
    "++model.data.data_prefix={DATASET_PATH} \\\n",
    "++model.tokenizer.do_lower_case=False \\\n",
    "++model.data.evaluation_sample_size=50 \\\n",
    "++model.data.hard_negatives_to_train=5 \\\n",
    "++model.data.evaluation_steps=100 \\\n",
    "++model.data.data_train={DATASET_PATH} \\\n",
    "++model.data.num_workers=7 \\\n",
    "exp_manager.explicit_log_dir={SAVE_DIR} \\\n",
    "exp_manager.create_wandb_logger=False \\\n",
    "++exp_manager.checkpoint_callback_params.save_best_model=True \\\n",
    "exp_manager.resume_if_exists=False\"\n",
    "\n",
    "print(COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!{COMMAND}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your training completed, you should see a megatron_bert.nemo in your `SAVE_DIR` directory. \n",
    "\n",
    "If training failed due to memmap-related errors, delete any output_data.jsonl.idx* (index) files that have been generated in the `OUTPUT_DATA_PATH` directory where output_data.jsonl is located. To save memory, NeMo Framework doesn't rebuild index files if they already exist. So if you've changed any parameters related to the data or changed the data itself, this will cause errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "For this tutorial, we'll use the scifact dataset from BeIR to compare the retrieval accuracy between the original model and the fine-tuned model. For a true apples to apples comparison, you should create your own domain-specific evaluation dataset that matches the domain of the synthetic fine-tuning dataset. This evaluation dataset should comprise of corpus, queries, and qrel (query relevance) scores.  \n",
    "\n",
    "We will use NeMo Framework to restore both the original and fine-tuned models from their respective checkpoints and BeIR libraries to easily evaluate the retrieval accuracy. \n",
    "\n",
    "Finally we'll evaluate the model with NDCG@k, MAP@K, Recall@K and Precision@K scores. These metrics assess different aspects of retrieval performance, where NDCG and MAP focus on the quality of rankings, with higher values indicating better-ranked relevant documents.Recall measures how many relevant documents are retrieved at different ranks, improving as k increases. Precision evaluates the accuracy of the top k documents, with higher precision indicating more relevant results at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from beir import util, LoggingHandler\n",
    "from beir.retrieval import models\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import pathlib, os\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "#### Download nfcorpus.zip dataset and unzip the dataset\n",
    "dataset = \"nfcorpus\"\n",
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "out_dir = os.path.join(\"/tmp\", \"datasets\")\n",
    "data_path = util.download_and_unzip(url, out_dir)\n",
    "\n",
    "#### Provide the data_path where scifact has been downloaded and unzipped\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a wrapper NeMo model for retrieval evaluation on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from nemo.collections.nlp.models.information_retrieval.megatron_bert_embedding_model import MegatronBertEmbeddingModel\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "class NeMoModel:\n",
    "    def __init__(self, model_path=None, override_configs=None, **kwargs):\n",
    "        cfg = MegatronBertEmbeddingModel.restore_from(model_path, return_config=True)\n",
    "        if override_configs is not None:\n",
    "            for k in override_configs:\n",
    "                cfg[k] = override_configs[k]\n",
    "        self.model = MegatronBertEmbeddingModel.restore_from(\n",
    "            model_path,\n",
    "            trainer=Trainer(),\n",
    "            override_config_path=cfg)\n",
    "        self.model = self.model.to(\"cuda:0\").half()\n",
    "    \n",
    "    def encode_text(self, texts, batch_size=1, device=\"cuda:0\"):\n",
    "        with torch.no_grad():\n",
    "            tokenized_texts = self.model.tokenizer.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "            input_ids = tokenized_texts[\"input_ids\"].to(device)\n",
    "            attention_mask = tokenized_texts[\"attention_mask\"].to(device)\n",
    "            token_type_ids = tokenized_texts[\"token_type_ids\"].to(device)\n",
    "\n",
    "            num_batches = int(math.ceil(len(texts)/batch_size))\n",
    "\n",
    "            embeddings = []\n",
    "            for batch_id in tqdm(range(num_batches)):\n",
    "                start = batch_size * batch_id\n",
    "                end = batch_size * (batch_id+1)\n",
    "\n",
    "                batch_embeddings = self.model(input_ids[start:end, :], attention_mask[start:end, :], token_type_ids[start:end, :])\n",
    "                embeddings.append(batch_embeddings)\n",
    "            return torch.cat(embeddings, dim=1).swapaxes(0,1)\n",
    "\n",
    "    # Write your own encoding query function (Returns: Query embeddings as numpy array)\n",
    "    def encode_queries(self, queries: List[str], batch_size: int, **kwargs) -> np.ndarray:\n",
    "        queries = [f\"query: {query}\" for query in queries]\n",
    "        embeddings = self.encode_text(texts=queries, batch_size=batch_size)\n",
    "        return embeddings\n",
    "    \n",
    "    # Write your own encoding corpus function (Returns: Document embeddings as numpy array)  \n",
    "    def encode_corpus(self, corpus: List[Dict[str, str]], batch_size: int, **kwargs) -> np.ndarray:\n",
    "        corpus = [f\"passage: {passage}\" for passage in corpus]\n",
    "        embeddings = self.encode_text(texts=corpus, batch_size=batch_size)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Fine-tuned model:\n",
    "\n",
    "NOTE: there may be a bug in Nemo 24.07 where certain global variables are set by default and must match the passed in config variables. One example is global_batch_size=8. So even though we set global_batch_size=4 during fine-tuning, we need to manually override it here to successfully restore the model. This does not impact the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_model = DRES(NeMoModel(model_path=\"/tmp/trained_model/checkpoints/megatron_bert.nemo\", override_configs={'global_batch_size': 8}), batch_size=1)\n",
    "retriever = EvaluateRetrieval(new_model, score_function=\"dot\") # or \"cos_sim\" for cosine similarity\n",
    "results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "#### Evaluate your model with NDCG@k, MAP@K, Recall@K and Precision@K  where k = [1,3,5,10,100,1000] \n",
    "ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "print(ndcg, _map, recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look like this:\n",
    "```\n",
    "{'NDCG@1': 0.43808, 'NDCG@3': 0.4094, 'NDCG@5': 0.39159, 'NDCG@10': 0.35777, 'NDCG@100': 0.33154, 'NDCG@1000': 0.41858} {'MAP@1': 0.05692, 'MAP@3': 0.09939, 'MAP@5': 0.11412, 'MAP@10': 0.13414, 'MAP@100': 0.17271, 'MAP@1000': 0.18817} {'Recall@1': 0.05692, 'Recall@3': 0.11421, 'Recall@5': 0.13637, 'Recall@10': 0.17648, 'Recall@100': 0.33741, 'Recall@1000': 0.64782} {'P@1': 0.45511, 'P@3': 0.38803, 'P@5': 0.34365, 'P@10': 0.26656, 'P@100': 0.08508, 'P@1000': 0.02163}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the original model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The original model\n",
    "old_model = DRES(NeMoModel(model_path=PATH_TO_NEMO_MODEL), batch_size=1)\n",
    "retriever = EvaluateRetrieval(old_model, score_function=\"dot\") # or \"cos_sim\" for cosine similarity\n",
    "results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "#### Evaluate your model with NDCG@k, MAP@K, Recall@K and Precision@K  where k = [1,3,5,10,100,1000] \n",
    "ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "print(ndcg, _map, recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is some improvement in the results on evaluation. Using a larger amount of data for fine-tuning and proprietary, domain-specific data is likely to make the improvement much more significant. From some initial testing with proprietary corporate data, we've seen around 5-10% accuracy improvement. Your results may vary depending on the other configurations set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You've officially created synthetic data and fine-tuned a text embedding model using NeMo Framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

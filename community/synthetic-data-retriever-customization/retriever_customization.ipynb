{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Customization - Fine-Tuning & Evaluation (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors - Aditya Malte, Vinay Raman, Ali Taghibakhshi, Dora Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This is part two of a two-part series. \n",
    "1. `synthetic_data_generation_nemo.ipynb`:\n",
    "    - Use an LLM from build.nvidia.com (or deploy your own using NIM!) to create training examples containing generated queries and positive chunks. By default the notebook will use nfcorpus, but you can easily swap in your own data.\n",
    "    - Implement hard negative mining to find challenging negative examples\n",
    "    - Save results to a `.jsonl` file \n",
    "\n",
    "\n",
    "2. `retriever_customization.ipynb` **(this notebook)**:\n",
    "    - Use the generated training data in the `.jsonl` file to fine-tune a retriever model using Nemo Framework\n",
    "    - Evaluate the results of your fine-tuned embedding model against the original using BeIR Benchmark\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NeMo Framework Docker container\n",
    "This notebook requires the NeMo Framework Docker container. Download the appropriate Docker image and build the container when inside the `synthetic-data-retriever-customization` directory using this command: \n",
    "\n",
    "`docker run -it --rm --gpus all --ipc=host --network host -v $(pwd):/workspace nvcr.io/nvidia/nemo:24.07`\n",
    "\n",
    "This notebook was tested on a setup comprising of 1xL40S GPUs with CUDA setup.\n",
    "\n",
    "\n",
    "#### Download NV-Embed-QA-4 model weights from NGC\n",
    "Use the command `ngc registry model download-version \"ohlfw0olaadg/ea-participants/nv-embed-qa:4\"` to download the NeMo Retriever model. It must be downloaded to the directory `files/models`. The same model - NeMo Retriever - has been used as an example in this notebook. If you do not have NVAIE access, then you may download and convert a HF embedding like `intfloat/e5-large-unsupervised` for your purpose as follows:\n",
    "```\n",
    "/NeMo/scripts/nlp_language_modeling/convert_bert_hf_to_nemo.py \\\n",
    "       --input_name_or_path \"intfloat/e5-large-unsupervised\" \\\n",
    "       --output_path /workspace/files/models/my_model.nemo\n",
    "```\n",
    "\n",
    "For the purpose of this notebook, we have used the NeMo Retriever model. If you use another model, or convert an HF model, ensure that the model path is updated accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: beir in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from beir) (3.0.1)\n",
      "Requirement already satisfied: pytrec-eval in /usr/local/lib/python3.10/dist-packages (from beir) (0.5)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (from beir) (1.8.0.post1)\n",
      "Requirement already satisfied: elasticsearch==7.9.1 in /usr/local/lib/python3.10/dist-packages (from beir) (7.9.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from beir) (2.20.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from elasticsearch==7.9.1->beir) (1.26.19)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elasticsearch==7.9.1->beir) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->beir) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (3.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (0.24.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (6.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (4.40.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (2.3.0a0+40ec155e58.nv24.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (1.2.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (1.12.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (10.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets->beir) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->beir) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->beir) (3.6)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (3.1.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->beir) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->beir) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->beir) (0.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->beir) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->beir) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->beir) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->beir) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->beir) (3.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->beir) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers->beir) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers->beir) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install beir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This should be the synthetic dataset generated in Part 1, consisting of the queries, pos_doc, and neg_docs\n",
    "OUTPUT_DATA_PATH = \"/tmp/data/output_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_DEVICES=1 # number of gpus available for fine-tuning\n",
    "\n",
    "# Use the default config for BERT Embedding Model\n",
    "CONFIG_PATH=\"/opt/NeMo/examples/nlp/information_retrieval/conf/\"\n",
    "CONFIG_NAME=\"megatron_bert_embedding_config\"\n",
    "\n",
    "PATH_TO_NEMO_MODEL= \"/workspace/files/models/NV-Embed-QA-4.nemo\" # Path to converted nemo model from hf, if you have a different model\n",
    "DATASET_PATH= OUTPUT_DATA_PATH # Path to jsonl dataset\n",
    "SAVE_DIR= \"/tmp/trained_model/\" # where the checkpoint and logs are saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `megatron_bert_embedding_finetuning.py` script. This script sets up and trains a Megatron-BERT model using  NVIDIA NeMo Framework, with configurations managed by Hydra. It loads the pre-trained `.nemo` model from a checkpoint, adjusts settings like batch size, and sets up parallel processing for multi-GPU training. Finally, it initializes the trainer and starts the training process with the NeMo Framework Megatron Trainer. \n",
    "\n",
    "Note `model.global_batch_size = model.micro_batch_size * trainer.devices (aka # of GPUs)`. Please keep micro_batch_size=4 and set the other parameters accordingly. \n",
    "\n",
    "`model.data.hard_negatives_to_train` should be set to the number of neg_docs corresponding to each query in your synthetic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /opt/NeMo/examples/nlp/information_retrieval/megatron_bert_embedding_finetuning.py --config-path=/opt/NeMo/examples/nlp/information_retrieval/conf/ --config-name=megatron_bert_embedding_config restore_from_path=/workspace/files/models/NV-Embed-QA-4.nemo trainer.devices=1 trainer.val_check_interval=10 trainer.max_epochs=1 +trainer.num_sanity_val_steps=0 trainer.max_steps=100000 model.global_batch_size=4 model.micro_batch_size=4 model.mcore_bert=False model.tokenizer.library=huggingface model.tokenizer.type=intfloat/e5-large-unsupervised model.megatron_legacy=True ++model.data.data_prefix=/tmp/data/output_data.jsonl ++model.tokenizer.do_lower_case=False ++model.data.evaluation_sample_size=50 ++model.data.hard_negatives_to_train=5 ++model.data.evaluation_steps=100 ++model.data.data_train=/tmp/data/output_data.jsonl ++model.data.num_workers=7 exp_manager.explicit_log_dir=/tmp/trained_model/ exp_manager.create_wandb_logger=False ++exp_manager.checkpoint_callback_params.save_best_model=True exp_manager.resume_if_exists=False\n"
     ]
    }
   ],
   "source": [
    "COMMAND = f\"python /opt/NeMo/examples/nlp/information_retrieval/megatron_bert_embedding_finetuning.py \\\n",
    "--config-path={CONFIG_PATH} \\\n",
    "--config-name={CONFIG_NAME} \\\n",
    "restore_from_path={PATH_TO_NEMO_MODEL} \\\n",
    "trainer.devices={NUM_DEVICES} \\\n",
    "trainer.val_check_interval=10 \\\n",
    "trainer.max_epochs=1 \\\n",
    "+trainer.num_sanity_val_steps=0 \\\n",
    "trainer.max_steps=100000 \\\n",
    "model.global_batch_size=4 \\\n",
    "model.micro_batch_size=4 \\\n",
    "model.mcore_bert=False \\\n",
    "model.tokenizer.library=huggingface \\\n",
    "model.tokenizer.type=intfloat/e5-large-unsupervised \\\n",
    "model.megatron_legacy=True \\\n",
    "++model.data.data_prefix={DATASET_PATH} \\\n",
    "++model.tokenizer.do_lower_case=False \\\n",
    "++model.data.evaluation_sample_size=50 \\\n",
    "++model.data.hard_negatives_to_train=5 \\\n",
    "++model.data.evaluation_steps=100 \\\n",
    "++model.data.data_train={DATASET_PATH} \\\n",
    "++model.data.num_workers=7 \\\n",
    "exp_manager.explicit_log_dir={SAVE_DIR} \\\n",
    "exp_manager.create_wandb_logger=False \\\n",
    "++exp_manager.checkpoint_callback_params.save_best_model=True \\\n",
    "exp_manager.resume_if_exists=False\"\n",
    "\n",
    "print(COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-11-15 06:11:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2024-11-15 06:11:34 megatron_bert_embedding_finetuning:31] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-11-15 06:11:34 megatron_bert_embedding_finetuning:32] \n",
      "    name: megatron_bert\n",
      "    restore_from_path: /workspace/files/models/NV-Embed-QA-4.nemo\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 1\n",
      "      max_steps: 100000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 10\n",
      "      limit_val_batches: 50\n",
      "      limit_test_batches: 500\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "      num_sanity_val_steps: 0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /tmp/trained_model/\n",
      "      exp_dir: null\n",
      "      name: megatron_bert\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        filename: megatron_bert--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "        save_best_model: true\n",
      "    model:\n",
      "      mcore_bert: false\n",
      "      micro_batch_size: 4\n",
      "      global_batch_size: 4\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      encoder_seq_length: 512\n",
      "      max_position_embeddings: ${.encoder_seq_length}\n",
      "      position_embedding_type: learned_absolute\n",
      "      num_layers: 24\n",
      "      hidden_size: 1024\n",
      "      ffn_hidden_size: 4096\n",
      "      num_attention_heads: 16\n",
      "      transformer_block_type: post_ln\n",
      "      add_pooler: true\n",
      "      add_lm_head: false\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: false\n",
      "      normalization: layernorm\n",
      "      layernorm_epsilon: 1.0e-12\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      bert_binary_head: true\n",
      "      megatron_legacy: true\n",
      "      tokenizer:\n",
      "        library: huggingface\n",
      "        type: intfloat/e5-large-unsupervised\n",
      "        model: null\n",
      "        vocab_file: null\n",
      "        merge_file: null\n",
      "        do_lower_case: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: false\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      gradient_as_bucket_view: true\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      sequence_parallel: false\n",
      "      data:\n",
      "        data_train: /tmp/data/output_data.jsonl\n",
      "        data_validation: null\n",
      "        hard_negatives_to_train: 5\n",
      "        index_mapping_dir: null\n",
      "        data_impl: mmap\n",
      "        splits_string: 900,50,50\n",
      "        seq_length: ${model.encoder_seq_length}\n",
      "        skip_warmup: true\n",
      "        num_workers: 7\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        data_prefix: /tmp/data/output_data.jsonl\n",
      "        evaluation_sample_size: 50\n",
      "        evaluation_steps: 100\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 50000\n",
      "          min_lr: 2.0e-05\n",
      "    \n",
      "[NeMo W 2024-11-15 06:11:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2024-11-15 06:11:35 exp_manager:396] ExpManager schema\n",
      "[NeMo I 2024-11-15 06:11:35 exp_manager:397] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}}\n",
      "[NeMo I 2024-11-15 06:11:35 exp_manager:455] Experiments will be logged at /tmp/trained_model\n",
      "[NeMo I 2024-11-15 06:11:35 exp_manager:983] TensorboardLogger has been set up\n",
      "[NeMo W 2024-11-15 06:11:35 exp_manager:1111] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-15 06:11:35 megatron_bert_embedding_finetuning:47] Loading model from /workspace/files/models/NV-Embed-QA-4.nemo\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:294] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:303] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:316] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:355] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:357] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:11:39 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-11-15 06:11:39 num_microbatches_calculator:119] setting number of micro-batches to constant 1\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:39 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-11-15 06:11:39 tokenizer_utils:183] Getting HuggingFace AutoTokenizer with pretrained_model_name: intfloat/e5-large-unsupervised\n",
      "[NeMo W 2024-11-15 06:11:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|███████████████████| 372/372 [00:00<00:00, 2.31MB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 3.69MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 112/112 [00:00<00:00, 1.48MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 2.42MB/s]\n",
      "[NeMo I 2024-11-15 06:11:40 megatron_base_model:595] Padded vocab_size: 30592, original vocab_size: 30522, dummy tokens: 70.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: attention_dropout in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: persist_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:11:40 deprecated:94] \n",
      "    \n",
      "    ******************************************************************************************************\n",
      "    ******************************************************************************************************\n",
      "    *****  NeMoBertModel is deprecated. Please, use MCoreBertModelWrapperWithPostLNSupport instead.  *****\n",
      "    ******************************************************************************************************\n",
      "    ******************************************************************************************************\n",
      "    \n",
      "[NeMo W 2024-11-15 06:11:40 deprecated:95] Waiting for 2 seconds before this message disappears.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-15 06:11:43 nlp_overrides:1346] Model MegatronBertEmbeddingModel was successfully restored from /workspace/files/models/NV-Embed-QA-4.nemo.\n",
      "[NeMo W 2024-11-15 06:11:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronBertEmbeddingModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-11-15 06:11:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2024-11-15 06:11:45 megatron_bert_embedding_model:188] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 3.35e+08. Total number of model parameters: 3.35e+08.\n",
      "[NeMo I 2024-11-15 06:11:45 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-11-15 06:11:45 text_memmap_dataset:525] Processing 1 data files using 4 workers\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:495] Building indexing for fn = /tmp/data/output_data.jsonl\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:507] Saving idx file = /tmp/data/output_data.jsonl.idx.npy\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:509] Saving metadata file = /tmp/data/output_data.jsonl.idx.info\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.134743\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:525] Processing 1 data files using 4 workers\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.128574\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:249] Loading /tmp/data/output_data.jsonl\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000936\n",
      "[NeMo I 2024-11-15 06:11:46 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-11-15 06:11:46 megatron_bert_embedding_model:167] Length of train dataset: 303\n",
      "[NeMo I 2024-11-15 06:11:46 megatron_bert_embedding_model:172] Finished building SBert datasets.\n",
      "[NeMo I 2024-11-15 06:11:46 megatron_bert_embedding_model:327] Setting up train dataloader with len(len(self._train_ds)): 303 and consumed samples: 0\n",
      "[NeMo I 2024-11-15 06:11:46 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 303 and consumed_samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-11-15 06:11:46 nlp_overrides:268] Configuring DDP for model parallelism.\n",
      "[NeMo W 2024-11-15 06:11:46 megatron_base_model:1223] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 100000.\n",
      "[NeMo I 2024-11-15 06:11:47 modelPT:786] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        is_expert: False\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "    \n",
      "    Parameter Group 1\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        is_expert: False\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2024-11-15 06:11:47 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fa291f3c430>\" \n",
      "    will be used during training (effective maximum steps = 100000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 500\n",
      "    constant_steps: 50000\n",
      "    min_lr: 2.0e-05\n",
      "    max_steps: 100000\n",
      "    )\n",
      "\n",
      "  | Name               | Type                   | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0 | model              | NeMoBertEmbeddingModel | 335 M  | train\n",
      "1 | cross_entropy_loss | CrossEntropyLoss       | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "335 M     Trainable params\n",
      "0         Non-trainable params\n",
      "335 M     Total params\n",
      "1,340.854 Total estimated model params size (MB)\n",
      "[NeMo W 2024-11-15 06:11:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-11-15 06:13:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:105: Total length of `list` across ranks is zero. Please make sure this was your intention.\n",
      "    \n",
      "Epoch 0: :   0%| | 75/100000 [00:42<15:35:37, reduced_train_loss=0.843, global_s`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: :   0%| | 75/100000 [00:42<15:35:39, reduced_train_loss=0.843, global_s\n",
      "[NeMo W 2024-11-15 06:13:56 nemo_model_checkpoint:272] <nemo.utils.callbacks.nemo_model_checkpoint.NeMoModelCheckpoint object at 0x7fa32d61e7a0> was told to save the best checkpoint at the end of training, but no saved checkpoints were found. Saving latest model instead.\n"
     ]
    }
   ],
   "source": [
    "!{COMMAND}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your training completed, you should see a megatron_bert.nemo in your `SAVE_DIR` directory. \n",
    "\n",
    "If training failed due to memmap-related errors, delete any output_data.jsonl.idx* (index) files that have been generated in the `OUTPUT_DATA_PATH` directory where output_data.jsonl is located. To save memory, NeMo Framework doesn't rebuild index files if they already exist. So if you've changed any parameters related to the data or changed the data itself, this will cause errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "For this tutorial, we'll use the scifact dataset from BeIR to compare the retrieval accuracy between the original model and the fine-tuned model. For a true apples to apples comparison, you should create your own domain-specific evaluation dataset that matches the domain of the synthetic fine-tuning dataset. This evaluation dataset should comprise of corpus, queries, and qrel (query relevance) scores.  \n",
    "\n",
    "We will use NeMo Framework to restore both the original and fine-tuned models from their respective checkpoints and BeIR libraries to easily evaluate the retrieval accuracy. \n",
    "\n",
    "Finally we'll evaluate the model with NDCG@k, MAP@K, Recall@K and Precision@K scores. These metrics assess different aspects of retrieval performance, where NDCG and MAP focus on the quality of rankings, with higher values indicating better-ranked relevant documents.Recall measures how many relevant documents are retrieved at different ranks, improving as k increases. Precision evaluates the accuracy of the top k documents, with higher precision indicating more relevant results at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df55b6fc95d4566afd1354664d7571e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from beir import util, LoggingHandler\n",
    "from beir.retrieval import models\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import pathlib, os\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "#### Download scifact.zip dataset and unzip the dataset\n",
    "dataset = \"scifact\"\n",
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "out_dir = os.path.join(\"/tmp\", \"datasets\")\n",
    "data_path = util.download_and_unzip(url, out_dir)\n",
    "\n",
    "#### Provide the data_path where scifact has been downloaded and unzipped\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a wrapper NeMo model for retrieval evaluation on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from nemo.collections.nlp.models.information_retrieval.megatron_bert_embedding_model import MegatronBertEmbeddingModel\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "class NeMoModel:\n",
    "    def __init__(self, model_path=None, override_configs=None, **kwargs):\n",
    "        cfg = MegatronBertEmbeddingModel.restore_from(model_path, return_config=True)\n",
    "        if override_configs is not None:\n",
    "            for k in override_configs:\n",
    "                cfg[k] = override_configs[k]\n",
    "        self.model = MegatronBertEmbeddingModel.restore_from(\n",
    "            model_path,\n",
    "            trainer=Trainer(),\n",
    "            override_config_path=cfg)\n",
    "        self.model = self.model.to(\"cuda:0\").half()\n",
    "    \n",
    "    def encode_text(self, texts, batch_size=1, device=\"cuda:0\"):\n",
    "        with torch.no_grad():\n",
    "            tokenized_texts = self.model.tokenizer.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "            input_ids = tokenized_texts[\"input_ids\"].to(device)\n",
    "            attention_mask = tokenized_texts[\"attention_mask\"].to(device)\n",
    "            token_type_ids = tokenized_texts[\"token_type_ids\"].to(device)\n",
    "\n",
    "            num_batches = int(math.ceil(len(texts)/batch_size))\n",
    "\n",
    "            embeddings = []\n",
    "            for batch_id in tqdm(range(num_batches)):\n",
    "                start = batch_size * batch_id\n",
    "                end = batch_size * (batch_id+1)\n",
    "\n",
    "                batch_embeddings = self.model(input_ids[start:end, :], attention_mask[start:end, :], token_type_ids[start:end, :])\n",
    "                embeddings.append(batch_embeddings)\n",
    "            return torch.cat(embeddings, dim=1).swapaxes(0,1)\n",
    "\n",
    "    # Write your own encoding query function (Returns: Query embeddings as numpy array)\n",
    "    def encode_queries(self, queries: List[str], batch_size: int, **kwargs) -> np.ndarray:\n",
    "        queries = [f\"query: {query}\" for query in queries]\n",
    "        embeddings = self.encode_text(texts=queries, batch_size=batch_size)\n",
    "        return embeddings\n",
    "    \n",
    "    # Write your own encoding corpus function (Returns: Document embeddings as numpy array)  \n",
    "    def encode_corpus(self, corpus: List[Dict[str, str]], batch_size: int, **kwargs) -> np.ndarray:\n",
    "        corpus = [f\"passage: {passage}\" for passage in corpus]\n",
    "        embeddings = self.encode_text(texts=corpus, batch_size=batch_size)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Fine-tuned model:\n",
    "\n",
    "NOTE: there may be a bug in Nemo 24.07 where certain global variables are set by default and must match the passed in config variables. One example is global_batch_size=8. So even though we set global_batch_size=4 during fine-tuning, we need to manually override it here to successfully restore the model. This does not impact the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1115 06:52:24.300187 140712728913024 rank_zero.py:64] GPU available: True (cuda), used: True\n",
      "I1115 06:52:24.300975 140712728913024 rank_zero.py:64] TPU available: False, using: 0 TPU cores\n",
      "I1115 06:52:24.301555 140712728913024 rank_zero.py:64] HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 06:52:24 - GPU available: True (cuda), used: True\n",
      "2024-11-15 06:52:24 - TPU available: False, using: 0 TPU cores\n",
      "2024-11-15 06:52:24 - HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-15 06:52:25 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:294] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:303] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:316] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:355] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:357] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_init:358] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-15 06:52:25 tokenizer_utils:183] Getting HuggingFace AutoTokenizer with pretrained_model_name: intfloat/e5-large-unsupervised\n",
      "[NeMo I 2024-11-15 06:52:25 megatron_base_model:595] Padded vocab_size: 30592, original vocab_size: 30522, dummy tokens: 70.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: attention_dropout in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: persist_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:52:25 deprecated:94] \n",
      "    \n",
      "    ******************************************************************************************************\n",
      "    ******************************************************************************************************\n",
      "    *****  NeMoBertModel is deprecated. Please, use MCoreBertModelWrapperWithPostLNSupport instead.  *****\n",
      "    ******************************************************************************************************\n",
      "    ******************************************************************************************************\n",
      "    \n",
      "[NeMo W 2024-11-15 06:52:25 deprecated:95] Waiting for 2 seconds before this message disappears.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-15 06:52:28 nlp_overrides:1346] Model MegatronBertEmbeddingModel was successfully restored from /tmp/trained_model/checkpoints/megatron_bert.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:13<00:00, 22.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5183/5183 [03:35<00:00, 24.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NDCG@1': 0.64333, 'NDCG@3': 0.70376, 'NDCG@5': 0.73643, 'NDCG@10': 0.75493, 'NDCG@100': 0.77466, 'NDCG@1000': 0.77817} {'MAP@1': 0.61428, 'MAP@3': 0.6783, 'MAP@5': 0.70104, 'MAP@10': 0.70977, 'MAP@100': 0.71426, 'MAP@1000': 0.71441} {'Recall@1': 0.61428, 'Recall@3': 0.75006, 'Recall@5': 0.82911, 'Recall@10': 0.88256, 'Recall@100': 0.97333, 'Recall@1000': 1.0} {'P@1': 0.64333, 'P@3': 0.27333, 'P@5': 0.18667, 'P@10': 0.09967, 'P@100': 0.01103, 'P@1000': 0.00113}\n"
     ]
    }
   ],
   "source": [
    "new_model = DRES(NeMoModel(model_path=\"/tmp/trained_model/checkpoints/megatron_bert.nemo\", override_configs={'global_batch_size': 8}), batch_size=1)\n",
    "retriever = EvaluateRetrieval(new_model, score_function=\"dot\") # or \"cos_sim\" for cosine similarity\n",
    "results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "#### Evaluate your model with NDCG@k, MAP@K, Recall@K and Precision@K  where k = [1,3,5,10,100,1000] \n",
    "ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "print(ndcg, _map, recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the original model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1115 06:56:54.851454 140712728913024 rank_zero.py:64] GPU available: True (cuda), used: True\n",
      "I1115 06:56:54.852136 140712728913024 rank_zero.py:64] TPU available: False, using: 0 TPU cores\n",
      "I1115 06:56:54.852690 140712728913024 rank_zero.py:64] HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 06:56:54 - GPU available: True (cuda), used: True\n",
      "2024-11-15 06:56:54 - TPU available: False, using: 0 TPU cores\n",
      "2024-11-15 06:56:54 - HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-15 06:56:55 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:294] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:303] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:316] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:355] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:357] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-11-15 06:56:55 megatron_init:358] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:55 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-15 06:56:56 tokenizer_utils:216] Getting Megatron tokenizer for pretrained model name: intfloat/e5-large-unsupervised, custom vocab file: None, and merges file: None\n",
      "[NeMo I 2024-11-15 06:56:56 tokenizer_utils:132] Getting HuggingFace AutoTokenizer with pretrained_model_name: intfloat/e5-large-unsupervised, vocab_file: None, merges_files: None, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-11-15 06:56:56 megatron_base_model:595] Padded vocab_size: 30592, original vocab_size: 30522, dummy tokens: 70.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:1182] The model: MegatronBertEmbeddingModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: attention_dropout in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: persist_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 megatron_base_model:568] The model: MegatronBertEmbeddingModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-15 06:56:56 deprecated:94] \n",
      "    \n",
      "    ******************************************************************************************************\n",
      "    ******************************************************************************************************\n",
      "    *****  NeMoBertModel is deprecated. Please, use MCoreBertModelWrapperWithPostLNSupport instead.  *****\n",
      "    ******************************************************************************************************\n",
      "    ******************************************************************************************************\n",
      "    \n",
      "[NeMo W 2024-11-15 06:56:56 deprecated:95] Waiting for 2 seconds before this message disappears.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-15 06:56:58 nlp_overrides:1346] Model MegatronBertEmbeddingModel was successfully restored from /workspace/files/models/NV-Embed-QA-4.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:12<00:00, 24.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5183/5183 [03:29<00:00, 24.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NDCG@1': 0.63, 'NDCG@3': 0.69648, 'NDCG@5': 0.72435, 'NDCG@10': 0.74511, 'NDCG@100': 0.76424, 'NDCG@1000': 0.76918} {'MAP@1': 0.59828, 'MAP@3': 0.66946, 'MAP@5': 0.68929, 'MAP@10': 0.69941, 'MAP@100': 0.70364, 'MAP@1000': 0.70387} {'Recall@1': 0.59828, 'Recall@3': 0.74533, 'Recall@5': 0.81444, 'Recall@10': 0.873, 'Recall@100': 0.96333, 'Recall@1000': 1.0} {'P@1': 0.63, 'P@3': 0.27111, 'P@5': 0.182, 'P@10': 0.09867, 'P@100': 0.01093, 'P@1000': 0.00113}\n"
     ]
    }
   ],
   "source": [
    "# The original model\n",
    "old_model = DRES(NeMoModel(model_path=PATH_TO_NEMO_MODEL), batch_size=1)\n",
    "retriever = EvaluateRetrieval(old_model, score_function=\"dot\") # or \"cos_sim\" for cosine similarity\n",
    "results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "#### Evaluate your model with NDCG@k, MAP@K, Recall@K and Precision@K  where k = [1,3,5,10,100,1000] \n",
    "ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "print(ndcg, _map, recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is some improvement in the results on evaluation. Using a larger amount of data for fine-tuning and proprietary, domain-specific data is likely to make the improvement much more significant. From some initial testing with proprietary corporate data, we've seen around 5-10% accuracy improvement. Your results may vary depending on the other configurations set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You've officially created synthetic data and fine-tuned a text embedding model using NeMo Framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

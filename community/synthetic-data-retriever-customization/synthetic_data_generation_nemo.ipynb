{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Customization - Synthetic Data Generation (Part 1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors - Aditya Malte, Dora Li, Vinay Raman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Text retrievers and embedding models play a crucial role in modern information retrieval systems by converting both queries and documents into dense numerical vectors (embeddings) that capture their semantic meaning. This allows the system to find relevant documents by measuring the similarity between a query's embedding and document embeddings in the database.\n",
    "\n",
    "The accuracy of these models directly impacts their usefulness. When a retriever has been trained primarily on one type of content (like general web text or news articles) but is asked to retrieve documents from a specialized domain (such as medical literature), its performance can degrade significantly.\n",
    "\n",
    "This is why many organizations fine-tune domain-specific retrievers for their particular use cases, ensuring more accurate and relevant document retrieval. As with all fine-tuning, high-quality domain-specific data is required and can be generated with LLMs such as [NVIDIA's Nemotron-4-340B-Instruct](https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/) that are specially trained and licensed for synthetic data generation. Other models like Llama-3.1-405B or Mixtral-8x22B-Instruct can also produce good results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "This two-part tutorial demonstrates how to improve retrieval performance by fine-tuning embedding models using synthetic training data. The process is split across two notebooks:\n",
    " \n",
    "1. `synthetic_data_generation_nemo.ipynb` **(this notebook)**:\n",
    "    - Use an LLM from build.nvidia.com (or deploy your own using NIM!) to create training examples containing generated queries and positive chunks. By default the notebook will use nfcorpus, but you can easily swap in your own data.\n",
    "    - Save results to a `.csv` file \n",
    "\n",
    "\n",
    "2. `retriever_customization.ipynb`:\n",
    "    - Implement hard negative mining to find challenging negative examples\n",
    "    - Use the generated training data in the `.csv` file to fine-tune a retriever model using Nemo Framework\n",
    "    - Evaluate the results of your fine-tuned embedding model against the original using BeIR Benchmark\n",
    "\n",
    "NOTE: This tutorial is only meant as a demo, and hence only a small subset of the corpus is used for training data generation - in order for the notebook run to complete in a reasonable time. A GPU is required to run notebook 2, but not notebook 1 if an LLM endpoint is used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "#### NeMo Framework Docker Container ####\n",
    "This notebook runs in a Docker environment built from the NeMo FW repo. Refer https://github.com/NVIDIA/NeMo/tree/main for instructions on how to build and run the docker containers. Ensure that the docker container you run this notebook in is built from the main branch of the NeMo repository. The current notebooks were tested on Nemo Framework 24.07 on a single-GPU machine (L40s).\n",
    "\n",
    "Run docker when inside the `synthetic-data-retriever-customization` directory using this command:\n",
    "\n",
    "`docker run -it --rm --gpus all --ipc=host --network host -v $(pwd):/workspace nvcr.io/nvidia/nemo:24.07`\n",
    "\n",
    "<br> \n",
    "\n",
    "#### NVIDIA AI Endpoints\n",
    "You'll need access to an **LLM** for generating queries. By default, this notebook uses the [Nemotron-4-340b-Instruct](https://build.nvidia.com/nvidia/nemotron-4-340b-instruct) API endpoint from [www.build.nvidia.com](https://www.build.nvidia.com). \n",
    "\n",
    "**An API Key is required.** Get your API Key by following the link above to the model and clicking on \"Build with this NIM\". All new users will get a number of tokens upon registering. Set the environment variable NVIDIA_API_KEY with your API key value.\n",
    "\n",
    "Optionally, you can self-host either model using **[NIM (NVIDIA Inference Microservice)](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html)** and pass in the local url when creating your LLM client later on. Follow the instructions in the link. Note that system GPU requirements will depend on the model you choose to deploy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install beir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the directory where the final `.csv` with generated QA pairs will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATIONS_MODEL_NAME = \"nvidia-nemotron-4-340b-instruct\"\n",
    "GENERATIONS_SAVE_DIR = \"/workspace/files/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Nfcorpus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as an example, we have chosen the [`nfcorpus`](https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/) public text dataset to generate the synthetic data from. But you can choose any other existing dataset or ideally provide your own proprietary documents to generate data from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DOMAIN = \"BeIR/nfcorpus\"\n",
    "corpus = load_dataset(DOMAIN, \"corpus\")[\"corpus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation from Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will: \n",
    "1. Break each text sample in the nfcorpus dataset that we downloaded into smaller chunks.\n",
    "2. Compose an LLM prompt that provides detailed instructions on how to generate queries based on each chunk.\n",
    "3. Send the queries to our LLM as an asynchronous batch job.\n",
    "4. Parse the queries and populate our synthetic dataset with query + positive chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chunk Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is required to break large documents into smaller chunks that an LLM can take as input. In this case we chunk the texts into samples of around word count 300, ensuring that sentences are not broken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(samples, chunk_size=300):\n",
    "    \n",
    "    final_chunks = []\n",
    "    for idx, (text, title, paragraph_id) in enumerate(samples):\n",
    "        sentence_list = sent_tokenize(text)\n",
    "        chunk = []\n",
    "        chunk_id = 0\n",
    "        word_count = 0\n",
    "        for sentence in sentence_list:\n",
    "            word_tokens = sentence.split()\n",
    "            word_count += len(word_tokens)\n",
    "            chunk.append(sentence)\n",
    "            \n",
    "            if(word_count >= chunk_size):\n",
    "                chunk_text = \" \".join(chunk)\n",
    "                final_chunks.append((chunk_text, title, paragraph_id, chunk_id))\n",
    "                chunk_id += 1\n",
    "                \n",
    "                chunk = []\n",
    "                word_count = 0\n",
    "                \n",
    "        if len(chunk) > 0:\n",
    "            chunk_text = \" \".join(chunk)\n",
    "            if(chunk_id==0 or len(chunk_text.split())>int((0.4*chunk_size))): # Only include the last chunk if it has significant number of words\n",
    "                final_chunks.append((chunk_text, title, paragraph_id, chunk_id)) # , or if the sample itself was a single chunk (chunk_id=0)\n",
    "\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kb = pd.DataFrame(corpus)\n",
    "kb[\"paragraph_id\"] = range(len(kb)) # assign a paragraph id to keep track of the original source document of each chunk\n",
    "kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- We are only sampling 100 out of around 5000 documents in the corpus, in order to allow the notebook to complete in a reasonable time for this tutorial. Feel free to increase it, especially if you are running this with your own data.\n",
    "\n",
    "- Most of the nfcorpus documents are already very short passages so they will only contain a single chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kb = kb.sample(n=100)\n",
    "\n",
    "kb_chunked = pd.DataFrame(chunk_text(kb[[\"text\", \"title\", \"paragraph_id\"]].itertuples(index=False)), columns=[\"text\", \"title\", \"paragraph_id\", \"chunk_id\"])\n",
    "kb_chunked.columns = ['chunk_text', 'title', 'paragraph_id', 'chunk_id']\n",
    "kb_chunked.drop([\"title\"], axis=1, inplace=True)\n",
    "kb_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = kb.merge(kb_chunked, how=\"left\", on=\"paragraph_id\")\n",
    "kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kb[\"chunk_title_text_concat\"] = kb[\"title\"] + \"\\n\" + kb[\"chunk_text\"]\n",
    "kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prompt Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prompt serves the purpose of providing context to the LLM for generation. You should modify this prompt as appropriate for your specific domain. Prompt engineering is incredibly important and can greatly impact the quality of the generated quries. \n",
    "\n",
    "The default prompt in this example is from the NVIDIA documentation/help page. It provides detailed instructions and provides examples of the types of queries the model should generate. In this prompt we ask the LLM to generate three unique queries for each chunk. \n",
    "\n",
    "With NVIDIA AI Endpoints, you can request the queries to be returned as JSON by specifying a json schema as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"queries\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"minItems\": 3,\n",
    "      \"maxItems\": 3\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"queries\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"You are a data annotator trying to generate three search queries for the Document 2. The generated queries must be answerable from Document 2. \n",
    "Return the generated responses according this JSON schema: {str(json_schema)}\n",
    "\n",
    "Here's an example:\n",
    "Sample Document: AV Sync \n",
    "Use of an AV Receiver with HDMI for video may result in audio lagging behind video.  First try \n",
    "using the receiver AV sync settings to calibrate.  If this does not work, use the AV sync slider \n",
    "utility in Settings  > Display & sound > Advanced settings > Audio video sync to calibrate for \n",
    "any audio delay.  The AV sync slider allows you to advance audio by 1 second (in small \n",
    "increments of 10ms) to synchronize the audio and video. \n",
    "Note that this tool is effective only when SHIELD is connected to your AV Receiver over HDMI \n",
    "(i.e. audio/video over HDMI); it is not meant to be used when a headset is plugged into SHIELD \n",
    "Controller/SHIELD Remote or USB audio device or Bluetooth audio device. \n",
    "If video lags behind audio (i.e. audio is ahead of video) then use your AV receiver’s settings to \n",
    "delay audio.\n",
    "ADJUST FOR OVERSCAN\n",
    "For TVs that don't provide their own overscan settings, use this setting to adjust the picture size to fit the screen.\n",
    "Go to Settings > Device Preferences > Display & Sound > Advanced Settings > Adjust for overscan to resize the picture on your TV or display.  Use the UP and DOWN d-pad buttons on your remote to maximize the picture on your TV.  Make sure the green triangles are completely visible to avoid overscan.\n",
    "\n",
    "Generated queries in JSON format:\n",
    "{{\n",
    "  \"queries\": [\n",
    "    \"How do I adjust the display so that my picture does not go out of the screen\",\n",
    "    \"Why is AV Sync not working when I'm plugging my SHIELD into my bluetooth earphone?\",\n",
    "    \"How many seconds can I delay audio by in AV Sync?\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "system_prompt += \"Do not use text from Example to generate queries for Document 2.\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(message:str, system_prompt: str) -> str:\n",
    "    build_prompt = system_prompt + \"\\nDocument 2: \" + message\n",
    "    return build_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb[\"prompt\"] = kb[\"chunk_title_text_concat\"].apply(get_prompt, system_prompt=system_prompt)\n",
    "kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use [Nemotron-4-340B-Instruct](https://build.nvidia.com/nvidia/nemotron-4-340b-instruct) from NVIDIA AI Endpoints (www.build.nvidia.com) to generate synthetic data. Make sure you have a valid API key stored as the environment variable NVIDIA_API_KEY, or you can generate one following the link earlier. \n",
    "\n",
    "The NVIDIA AI endpoint follows the same schemas as the OpenAI API standard, so we'll go ahead and use the AsyncOpenAI() client in order to asynchronously send many requests to the server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = kb[\"prompt\"].tolist()\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# If you are using a self-hosted NIM or any other API endpoint, modify base_url and other relevant parameters here.\n",
    "llm_client = AsyncOpenAI(\n",
    "    base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key = os.environ[\"NVIDIA_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_response(client, prompt):\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"nvidia/nemotron-4-340b-instruct\", # specify which model to use\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "            top_p=0.7,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "\n",
    "        if hasattr(response, 'choices') and len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {str(e)}\"\n",
    "    \n",
    "\n",
    "async def generate_batch_response(client, all_prompts):\n",
    "    tasks = [generate_response(client, prompt) for prompt in all_prompts]\n",
    "    results_list = await asyncio.gather(*tasks)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see that the API endpoint is responding\n",
    "result = await generate_response(llm_client, texts[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look like this:\n",
    "```\n",
    "{\n",
    "  \"queries\": [\n",
    "    \"What is the total number of reported cases of cardiac tamponade resulting from acupuncture, as identified in the systematic review?\",\n",
    "    \"How many of the reported cases of cardiac tamponade caused by acupuncture had fatal outcomes, according to the literature review?\",\n",
    "    \"What measure does the systematic review suggest to reduce the risk of cardiac tamponade in acupuncture practice?\"\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could take a while depending on the number of LLM calls\n",
    "generations = await generate_batch_response(llm_client, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb[\"generated_text\"] = generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's possible that some requests get dropped for various reasons. Retry them here.\n",
    "print(\"Requests to retry: \" + str(len(kb[kb['generated_text'].isna()])))\n",
    "for idx in kb[kb['generated_text'].isna()].index.tolist(): \n",
    "    kb.loc[idx, 'generated_text'] = await generate_response(client, kb.loc[idx, 'prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parsing Generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do some simple text parsing to extract the generated queries, then store them as individual entries in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queries_from_generations(kb):\n",
    "    paragraph_id_query = []\n",
    "    for row in kb.to_dict(orient=\"records\"):\n",
    "        paragraph_id = row[\"paragraph_id\"]\n",
    "        title = row[\"title\"]\n",
    "        text = row[\"chunk_text\"]\n",
    "        chunk_id = row[\"chunk_id\"]\n",
    "        data = json.loads(row[\"generated_text\"])\n",
    "        queries = data[\"queries\"]\n",
    "        print(queries)\n",
    "        print(\"-\"*200)\n",
    "        paragraph_id_query += [(paragraph_id, chunk_id, query) for query in queries]\n",
    "    return pd.DataFrame(paragraph_id_query, columns=[\"paragraph_id\", \"chunk_id\", \"chunk_query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extracted_queries = extract_queries_from_generations(kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output: \n",
    "```\n",
    "['What is the range of BMAA concentrations found in cyanobacterial blooms in South Florida?', 'Which neurodegenerative diseases have been linked to BMAA exposure?', 'What is the highest BMAA concentration found in resident animals used as human food in South Florida?']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kb = kb.merge(extracted_queries, how=\"left\", left_on=[\"paragraph_id\", \"chunk_id\"], right_on=[\"paragraph_id\", \"chunk_id\"])\n",
    "kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_pairs = kb[[\"chunk_query\", \"chunk_title_text_concat\", \"chunk_id\", \"paragraph_id\"]]\n",
    "qa_pairs.columns = [\"query\", \"positive_chunk\", \"positive_chunk_id\", \"paragraph_id\"]\n",
    "qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save QA Pair Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATIONS_SAVE_FILENAME =  f\"qa_pairs_{GENERATIONS_MODEL_NAME}_num_queries_{len(qa_pairs)}_{DOMAIN}\"\n",
    "GENERATIONS_SAVE_FILENAME = re.sub(r'\\W+', '_', GENERATIONS_SAVE_FILENAME)\n",
    "GENERATIONS_SAVE_PATH = os.path.join(GENERATIONS_SAVE_DIR, f\"{GENERATIONS_SAVE_FILENAME}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs.to_csv(GENERATIONS_SAVE_PATH, index=None)\n",
    "print(f\"Generated QA Pairs saved to {GENERATIONS_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've now successfully generated a synthetic dataset for Fine-Tuning a text embedding model! In the next notebook you'll use the `.csv` file you've just generated to fine-tune NV-EmbedQA-V4 using NeMo Framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

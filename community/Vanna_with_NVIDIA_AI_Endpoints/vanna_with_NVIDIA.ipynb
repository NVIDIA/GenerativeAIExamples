{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating Text-to-SQL Inference on Vanna with NVIDIA NIM\n",
    "This notebook demonstrates how to optimize Vannaâ€™s open-source text-to-SQL pipeline using NVIDIA NIM and NeMo Retriever for faster and more efficient analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "Make sure to install the following:\n",
    "- Python 3.8+\n",
    "- `vanna`, `langchain`, `milvus`, `openai`, `nvidia-langchain`,`pandas`, `kagglehub`,`numpy`  \n",
    "- Access to NVIDIA NIM endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"pymilvus[model]\"\n",
    "%pip install --upgrade --quiet  langchain-nvidia-ai-endpoints\n",
    "%pip install vanna\n",
    "%pip install kagglehub\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install sqlite3\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "We use two Steam datasets from Kaggle - [Steam Games Dataset](https://www.kaggle.com/datasets/fronkongames/steam-games-dataset/data) and [Games on Steam](https://www.kaggle.com/datasets/sujaykapadnis/games-on-steam/). We follow the preprocess steps mentioned in the [excellent Kaggle notebook](https://www.kaggle.com/code/terencicp/steam-games-data-transformation). After preprocessing, three CSVs will be generated:\n",
    " - `tableau_games.csv`\n",
    " - `tableau_categories.csv`\n",
    " - `tableau_tags.csv`\n",
    "\n",
    " You can follow the steps outlined below or directly use the the processed data in `processed_dataset` folder and jump to Step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Download Steam Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download first dataset\n",
    "first_dataset_path = kagglehub.dataset_download(\"fronkongames/steam-games-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", first_dataset_path)\n",
    "\n",
    "# Download second dataset\n",
    "second_dataset_path = kagglehub.dataset_download(\"sujaykapadnis/games-on-steam\")\n",
    "\n",
    "print(\"Path to dataset files:\", second_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocess first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "first_file = first_dataset_path + '/games.json'\n",
    "with open(first_file, 'r') as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remove unnecessary variables\n",
    "unnecessary_vars = [\n",
    "    'packages', 'screenshots', 'movies', 'score_rank', 'header_image',\n",
    "    'reviews', 'website', 'support_url', 'notes', 'support_email',\n",
    "    'recommendations', 'user_score', 'median_playtime_forever',\n",
    "    'median_playtime_2weeks', 'required_age', 'metacritic_score',\n",
    "    'metacritic_url', 'peak_ccu', 'detailed_description', 'about_the_game',\n",
    "    'windows', 'mac', 'linux', 'achievements', 'full_audio_languages',\n",
    "    'genres', 'dlc_count', 'supported_languages', 'developers',\n",
    "    'publishers', 'average_playtime_forever', 'average_playtime_2weeks',\n",
    "    'discount'\n",
    "]\n",
    "\n",
    "# Process each game's information and store in a list\n",
    "games = [{\n",
    "    **{k: v for k, v in game_info.items() if k not in unnecessary_vars},\n",
    "    'tags': list(tags.keys()) if isinstance((tags := game_info.get('tags', {})), dict) else [],\n",
    "    'tag_frequencies': list(tags.values()) if isinstance(tags, dict) else [],\n",
    "    'app_id': app_id\n",
    "} for app_id, game_info in json_data.items()]\n",
    "\n",
    "# Create a DataFrame from the processed list\n",
    "df = pd.DataFrame(games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've got the data we need, but it seems we have more rows than there are games on Steam.\n",
    "\n",
    "Some games just seem to be developer tests. Let's remove them. We'll also remove games with no reviews or no categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter games without sales, reviews or categories\n",
    "df2 = df[~((df['estimated_owners'] == \"0 - 0\") | (df['positive'] + df['negative'] == 0) | (df['categories'].str.len() == 0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, we will also remove games older than 2013, since there are very few games more than 10 year old on Steam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter games released before 2013\n",
    "df2 = df2.copy()\n",
    "df2['release_date'] = pd.to_datetime(df2['release_date'], format='mixed')\n",
    "df2 = df2[df2['release_date'].dt.year >= 2013]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, split the 'estimated_owners' column into two different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split estimated_owners into two: min_owners and max_owners\n",
    "df2[['min_owners', 'max_owners']] = df2['estimated_owners'].str.split(' - ', expand=True)\n",
    "\n",
    "# Remove the original field\n",
    "df2 = df2.drop('estimated_owners', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the outlier game with more than $800 price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove games with price > $800\n",
    "df2 = df2[df2['price'] <= 800]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Merge the two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataset we'll use contains information about game duration.\n",
    "Let's read the second dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_file = second_dataset_path + '/steamdb.json'\n",
    "df_second_dataset = pd.read_json(second_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only interested in the column 'hltb_single', that contains the information on game length we need. Each Steam game has a unique identifier we can use to join the data from both datasets. This unique identifier is found in the column 'app_id' of the first dataset, and in the column 'sid' of the second dataset. First we'll have to convert 'app_id' to integer since it is currently an object. Let's join the data and see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'app_id' integer\n",
    "df2['app_id'] = pd.to_numeric(df2['app_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Perform a left join for 'hltb_single'\n",
    "df_merged = pd.merge(df2, df_second_dataset[['sid', 'hltb_single']], left_on='app_id', right_on='sid', how='left')\n",
    "\n",
    "# Drop the redundant 'sid' column\n",
    "df_merged.drop('sid', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some games are extreme outliers in terms of duration. This is not caused by these games being extremely long but by the fact that some games can be played indefinitely and very few users have reported game length for these types of games. This might distort our analysis, so we'll limit the maximum duration of games at 100 hours, which is a reasonable upper limit for most games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit game duration to 100 hours\n",
    "df_merged['hltb_single'] = df_merged['hltb_single'].apply(lambda x: 100 if x > 100 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Normalizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame contains fields such as 'categories' and 'tags' that consist of lists of values. To normalize the data for storage in a SQL database, we need to break these fields into separate tables. Each table will maintain a relationship with the main table through the 'app_id' foreign key, following standard database normalization practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate DataFrame for each list-type column\n",
    "df_categories = df_merged.explode('categories')[['app_id', 'categories']]\n",
    "df_tags = df_merged.explode('tags')[['app_id', 'tags']]\n",
    "df_frequencies = df_merged.explode('tag_frequencies')['tag_frequencies']\n",
    "df_tags['tag_frequencies'] = df_frequencies.values\n",
    "\n",
    "# Remove the list columns from the main DataFrame\n",
    "columns_to_remove = ['categories', 'tags', 'tag_frequencies']\n",
    "df_imploded = df_merged.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main DataFrame is ready. We will remove any categories and tags with less than 50 games, since they are not relevant enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out categories with less than 50 games\n",
    "categories_counts = df_categories['categories'].value_counts()\n",
    "categories_to_keep = categories_counts[categories_counts >= 50].index.tolist()\n",
    "df_categories = df_categories[df_categories['categories'].isin(categories_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tags with less than 50 games\n",
    "tags_counts = df_tags['tags'].value_counts()\n",
    "tags_to_keep = tags_counts[tags_counts >= 50].index.tolist()\n",
    "df_tags = df_tags[df_tags['tags'].isin(tags_to_keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Save preprocessing results as CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll save the results as CSV files that we'll ingest into the SQL Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imploded.to_csv('./processed_dataset/games.csv', index=False)\n",
    "df_categories.to_csv('./processed_dataset/categories.csv', index=False)\n",
    "df_tags.to_csv('./processed_dataset/tags.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up Vanna with NVIDIA NIM and NeMo Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set the NVIDIA API Key. If you don't  find it [here](https://build.nvidia.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_api_key = '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Vanna Class using Milvus and OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, model\n",
    "from vanna.milvus import Milvus_VectorStore\n",
    "from vanna.openai import OpenAI_Chat\n",
    "from openai import OpenAI\n",
    "\n",
    "class VannaMilvus(Milvus_VectorStore, OpenAI_Chat):\n",
    "    def __init__(self, llm_client, config=None):\n",
    "        Milvus_VectorStore.__init__(self, config=config)\n",
    "        OpenAI_Chat.__init__(self, client=llm_client, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NIM Client based on OpenAI Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_client():\n",
    "    client = OpenAI(\n",
    "        base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key = nvidia_api_key\n",
    "    )\n",
    "    return client\n",
    "\n",
    "llm_client = get_openai_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Nvidia Embedder (Langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "nvidia_embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "    base_url='https://integrate.api.nvidia.com/v1',\n",
    "    api_key=nvidia_api_key\n",
    ")\n",
    "\n",
    "class EmbeddingWrapper:\n",
    "    def __init__(self, embedder):\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def encode_documents(self, texts):\n",
    "        result = self.embedder.embed_documents(texts)\n",
    "        return [np.array(r) for r in result]\n",
    "\n",
    "    def encode_queries(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embeddings.append(self.embedder.embed_query(text))\n",
    "        return embeddings\n",
    "\n",
    "vanna_embedder = EmbeddingWrapper(nvidia_embedder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Vector DB Client\n",
    "To keep things simple, we will use a local Milvus vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_uri = \"./milvus_nvidia.db\"\n",
    "milvus_client_nvidia = MilvusClient(uri=milvus_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Vanna instance using the LLM, Embedder and the Vector DB defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the LLM used for SQL generation\n",
    "model_name = \"meta/llama-3.1-70b-instruct\"\n",
    "\n",
    "# Define the configuration for Vanna instance\n",
    "config_nvidia = {\n",
    "        \"model\": model_name,\n",
    "        \"milvus_client\": milvus_client_nvidia,\n",
    "        \"embedding_function\": vanna_embedder,\n",
    "        \"n_results\": 2,  # The number of results to return from Milvus semantic search.\n",
    "    }\n",
    "\n",
    "vn_nvidia = VannaMilvus(llm_client, config=config_nvidia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ingest processed Steam data into a SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the SQLite database\n",
    "sqlite_path = 'steam_data.db'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "sql_connect = sqlite3.connect(sqlite_path)\n",
    "c = sql_connect.cursor()\n",
    "\n",
    "# Create tables\n",
    "init_sqls = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS games (\n",
    "    app_id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    release_date TEXT,\n",
    "    price REAL,\n",
    "    short_description TEXT,\n",
    "    positive INTEGER,\n",
    "    negative INTEGER,\n",
    "    min_owners INTEGER,\n",
    "    max_owners INTEGER,\n",
    "    hltb_single REAL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS categories (\n",
    "    app_id INTEGER,\n",
    "    categories TEXT,\n",
    "    FOREIGN KEY (app_id) REFERENCES games(app_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS tags (\n",
    "    app_id INTEGER,\n",
    "    tags TEXT,\n",
    "    tag_frequencies TEXT,\n",
    "    FOREIGN KEY (app_id) REFERENCES games(app_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "for sql in init_sqls.split(\";\"):\n",
    "    c.execute(sql)\n",
    "\n",
    "# Read the CSV files\n",
    "games_df = pd.read_csv('processed_dataset/games.csv')\n",
    "categories_df = pd.read_csv('processed_dataset/categories.csv')\n",
    "tags_df = pd.read_csv('processed_dataset/tags.csv')\n",
    "\n",
    "# Insert data into tables\n",
    "games_df.to_sql('games', sql_connect, if_exists='append', index=False)\n",
    "categories_df.to_sql('categories', sql_connect, if_exists='append', index=False)\n",
    "tags_df.to_sql('tags', sql_connect, if_exists='append', index=False)\n",
    "\n",
    "sql_connect.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the data is ingested in the SQL DB. Now, let's connect the SQL DB to Vanna and start giving more context on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "vn_nvidia.connect_to_sqlite(sqlite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Let's train Vanna on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing training data\n",
    "existing_training_data = vn_nvidia.get_training_data()\n",
    "if len(existing_training_data) > 0:\n",
    "    for _, training_data in existing_training_data.iterrows():\n",
    "        vn_nvidia.remove_training_data(training_data[\"id\"])\n",
    "\n",
    "# Get the DDL of the SQLite database\n",
    "df_ddl = vn_nvidia.run_sql(\"SELECT type, sql FROM sqlite_master WHERE sql is not null\")\n",
    "\n",
    "# Train the model on the DDL data\n",
    "for ddl in df_ddl[\"sql\"].to_list():\n",
    "    vn_nvidia.train(ddl=ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documentation about your business terminology or definitions.\n",
    "vn_nvidia.train(\n",
    "    documentation=\"\"\"\n",
    "    This dataset is used to answer questions about the game trends.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Add documentation about the tables\n",
    "vn_nvidia.train(\n",
    "    documentation=\"\"\"\n",
    "    The games table contains information about the games.\n",
    "    The app_id is the unique identifier for the game.This is a primary key.\n",
    "    The name is the name of the game.\n",
    "    The release_date is the date the game was released.\n",
    "    The price is the price of the game. Price in USD, 0.0 if its free.\n",
    "    The short_description is a brief description of the game.\n",
    "    The positive is the number of positive reviews or votes.\n",
    "    The negative is the number of negative reviews or votes.\n",
    "    The min_owners is the minimum number of owners. Used together with max_owners to get an estimate of the player base.\n",
    "    The max_owners is the maximum number of owners. Used together with min_owners to get an estimate of the player base.\n",
    "    The hltb_single is the average playtime of the game. This is an estimate.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "vn_nvidia.train(\n",
    "    documentation=\"\"\"\n",
    "    The categories table contains information about the categories of the games.\n",
    "    The app_id is the unique identifier for the game.\n",
    "    The categories is the categories of the game.\n",
    "    The app_id is a foreign key to the games table.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "vn_nvidia.train(\n",
    "    documentation=\"\"\"\n",
    "    The tags table contains information about the tags of the games.\n",
    "    The app_id is the unique identifier for the game.\n",
    "    The tags is the tags of the game. These are user defined.\n",
    "    The tag_frequencies is the frequencies of the tags.\n",
    "    The app_id is a foreign key to the games table.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the training data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = vn_nvidia.get_training_data()\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Let's ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = vn_nvidia.generate_sql(\"Which 5 games have the most positive reviews and how many?\")\n",
    "vn_nvidia.run_sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = vn_nvidia.generate_sql(\"Which indie game has the biggest player base?\")\n",
    "vn_nvidia.run_sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = vn_nvidia.generate_sql(\"Which category has the maximum number of gamers and how many?\")\n",
    "vn_nvidia.run_sql(sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vanna_blog_new",
   "language": "python",
   "name": "vanna_blog_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

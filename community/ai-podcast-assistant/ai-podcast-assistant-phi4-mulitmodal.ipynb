{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podcast Notes, Summarization, and Translation using Phi‑4 Multimodal LLM with NVIDIA NIM Microservices\n",
    "\n",
    "This notebook demonstrates a complete workflow using the [**Phi‑4 Multimodal LLM**](https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/) model. Below are some key model details (from the internal model card):\n",
    "\n",
    "- **Parameters:** 5.6B\n",
    "- **Inputs:** Text, Image, Audio\n",
    "- **Context Length:** 128K tokens\n",
    "- **Training Data:** 5T text tokens, 2.3M speech hours, 1.1T image-text tokens\n",
    "- **Supported Languages:** Multilingual text and audio (e.g. English, Chinese, German, French, etc.)\n",
    "\n",
    "The Phi-4 LLM will be accelerated with NVIDIA NIM Microservices in this tutorial. NVIDIA NIM Microservices that is a set of easy-to-use inference microservices for accelerating the deployment of foundation models on any cloud or data center and helping to keep your data secure.\n",
    "\n",
    "We will be using [preview NIM microservice API](https://build.nvidia.com/microsoft/phi-4-multimodal-instruct) for Phi-4 through the  [NVIDIA API Catalog](https://build.nvidia.com/microsoft)\n",
    "\n",
    "This notebook covers:\n",
    "1. A podcast notes and summarization use-case (with long-audio chunking).\n",
    "2. Translation of the transcript and summary into another language.\n",
    "\n",
    "Ensure that you have the required dependencies (e.g. `pydub`, `requests`, `Pillow`) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub requests Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Audio Processing\n",
    "\n",
    "In this section, we define helper functions to process audio files, such as converting an audio segment to a base64-encoded string \n",
    "and splitting long audio into manageable chunks for transcription.\n",
    "\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "- **audio_to_base64**: Converts audio segments to base64-encoded strings for API transmission\n",
    "- **generate_notes_chunk**: Processes individual audio chunks and generates transcription\n",
    "- **generate_detailed_notes**: Handles long audio files by splitting them into manageable chunks\n",
    "- **refine_transcription_to_notes**: Transforms raw transcription into well-formatted notes\n",
    "- **summarize_notes**: Creates a concise summary from the detailed notes\n",
    "- **translate_text**: Translates content to other languages while preserving formatting\n",
    "- **save_text_to_file**: Exports results to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "\n",
    "API_URL = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "\n",
    "def audio_to_base64(audio_segment):\n",
    "    \"\"\"Convert a pydub AudioSegment to a base64-encoded WAV string.\"\"\"\n",
    "    buffer = BytesIO()\n",
    "    audio_segment.export(buffer, format=\"mp3\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def generate_notes_chunk(audio_chunk, api_key, chunk_index, total_chunks):\n",
    "    \"\"\"\n",
    "    Generate detailed notes for a single audio chunk.\n",
    "    \n",
    "    The prompt instructs the model to produce detailed notes without any extra commentary.\n",
    "    \"\"\"\n",
    "    audio_b64 = audio_to_base64(audio_chunk)\n",
    "    prompt = (\n",
    "        f\"Transcribe the following audio accurately. \"\n",
    "        f\"This is segment {chunk_index+1} of {total_chunks}. \"\n",
    "        \"Please do not include any system commentary or self-referential text.\"\n",
    "    )\n",
    "    # Append the audio data (encoded in base64)\n",
    "    prompt += f' <audio src=\"data:audio/wav;base64,{audio_b64}\" />'\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Accept\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": \"microsoft/phi-4-multimodal-instruct\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You're a transcription assistant.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "    try:\n",
    "        text = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        text = f\"[Error generating notes for chunk {chunk_index+1}: {e}]\"\n",
    "    return text\n",
    "\n",
    "def generate_detailed_notes(audio_path, api_key, chunk_duration_ms=30000):\n",
    "    \"\"\"\n",
    "    Process a long audio file by splitting it into chunks, generating detailed notes for each chunk,\n",
    "    and concatenating the results.\n",
    "    \"\"\"\n",
    "    # Load audio file using pydub\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    total_duration = len(audio)\n",
    "    total_chunks = (total_duration // chunk_duration_ms) + (1 if total_duration % chunk_duration_ms > 0 else 0)\n",
    "    \n",
    "    notes_chunks = []\n",
    "    for i in range(total_chunks):\n",
    "        start_ms = i * chunk_duration_ms\n",
    "        end_ms = min((i+1) * chunk_duration_ms, total_duration)\n",
    "        chunk = audio[start_ms:end_ms]\n",
    "        print(f\"Processing chunk {i+1}/{total_chunks} (from {start_ms}ms to {end_ms}ms)...\")\n",
    "        notes = generate_notes_chunk(chunk, api_key, i, total_chunks)\n",
    "        notes_chunks.append(notes)\n",
    "    \n",
    "    transcription = \"\\n\\n\".join(notes_chunks)\n",
    "    return transcription\n",
    "\n",
    "\n",
    "def refine_transcription_to_notes(transcription, api_key):\n",
    "    \"\"\"\n",
    "    Given a raw transcription, remove any system prompt lines and generate coherent, well-formatted detailed notes.\n",
    "    \"\"\"\n",
    "    # Remove unwanted system phrases\n",
    "    cleaned = transcription.replace(\"You are a helpful assistant.\", \"\").replace(\"you are a helpful assistant.\", \"\")\n",
    "    \n",
    "    # Build a prompt instructing the LLM to produce formatted notes\n",
    "    prompt = (\n",
    "        \"Based on the transcription below, generate well-formatted, detailed notes that capture the main points. \"\n",
    "        \"Organize the notes using bullet points or numbered lists for key points and separate paragraphs clearly for readability. \"\n",
    "        \"Do not include any system commentary or self-referential text.\\n\\n\"\n",
    "        \"Transcription:\\n\"\n",
    "        f\"{cleaned}\"\n",
    "    )\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Accept\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": \"microsoft/phi-4-multimodal-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a note-taking assistant. Provide only detailed, well-formatted notes.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "    try:\n",
    "        notes = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        notes = f\"[Error generating refined notes: {e}]\"\n",
    "    return notes\n",
    "\n",
    "\n",
    "\n",
    "def summarize_notes(notes, api_key):\n",
    "    \"\"\"\n",
    "    Generate a concise summary based on the detailed notes.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Based on the detailed notes provided below, please generate a concise summary of the content. \"\n",
    "        \"Do not include any extra commentary or self-referential text.\\n\\n\"\n",
    "        f\"{notes}\"\n",
    "    )\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Accept\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": \"microsoft/phi-4-multimodal-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a summarization assistant. Provide only a concise summary.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "    try:\n",
    "        summary = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        summary = f\"[Error generating summary: {e}]\"\n",
    "    return summary\n",
    "\n",
    "\n",
    "def translate_text(text, target_lang, api_key):\n",
    "    \"\"\"\n",
    "    Translate the given text into the target language using the Phi‑4 API,\n",
    "    preserving all bullet points, formatting, and structure.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Translate the following text to {target_lang} exactly as it is, \"\n",
    "        \"preserving all bullet points, formatting, and structure. Do not omit any sections.\\n\\n\"\n",
    "        f\"{text}\"\n",
    "    )\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Accept\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": \"microsoft/phi-4-multimodal-instruct\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a translation assistant. Provide only the translated text with the original formatting preserved.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 1500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "    try:\n",
    "        translated_text = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        translated_text = f\"[Error generating translation: {e}]\"\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "def save_text_to_file(text, filename):\n",
    "    \"\"\"Save the given text to a file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"Text saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for detailed notes generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting detailed note generation...\n",
      "Processing chunk 1/54 (from 0ms to 30000ms)...\n",
      "Processing chunk 2/54 (from 30000ms to 60000ms)...\n",
      "Processing chunk 3/54 (from 60000ms to 90000ms)...\n",
      "Processing chunk 4/54 (from 90000ms to 120000ms)...\n",
      "Processing chunk 5/54 (from 120000ms to 150000ms)...\n",
      "Processing chunk 6/54 (from 150000ms to 180000ms)...\n",
      "Processing chunk 7/54 (from 180000ms to 210000ms)...\n",
      "Processing chunk 8/54 (from 210000ms to 240000ms)...\n",
      "Processing chunk 9/54 (from 240000ms to 270000ms)...\n",
      "Processing chunk 10/54 (from 270000ms to 300000ms)...\n",
      "Processing chunk 11/54 (from 300000ms to 330000ms)...\n",
      "Processing chunk 12/54 (from 330000ms to 360000ms)...\n",
      "Processing chunk 13/54 (from 360000ms to 390000ms)...\n",
      "Processing chunk 14/54 (from 390000ms to 420000ms)...\n",
      "Processing chunk 15/54 (from 420000ms to 450000ms)...\n",
      "Processing chunk 16/54 (from 450000ms to 480000ms)...\n",
      "Processing chunk 17/54 (from 480000ms to 510000ms)...\n",
      "Processing chunk 18/54 (from 510000ms to 540000ms)...\n",
      "Processing chunk 19/54 (from 540000ms to 570000ms)...\n",
      "Processing chunk 20/54 (from 570000ms to 600000ms)...\n",
      "Processing chunk 21/54 (from 600000ms to 630000ms)...\n",
      "Processing chunk 22/54 (from 630000ms to 660000ms)...\n",
      "Processing chunk 23/54 (from 660000ms to 690000ms)...\n",
      "Processing chunk 24/54 (from 690000ms to 720000ms)...\n",
      "Processing chunk 25/54 (from 720000ms to 750000ms)...\n",
      "Processing chunk 26/54 (from 750000ms to 780000ms)...\n",
      "Processing chunk 27/54 (from 780000ms to 810000ms)...\n",
      "Processing chunk 28/54 (from 810000ms to 840000ms)...\n",
      "Processing chunk 29/54 (from 840000ms to 870000ms)...\n",
      "Processing chunk 30/54 (from 870000ms to 900000ms)...\n",
      "Processing chunk 31/54 (from 900000ms to 930000ms)...\n",
      "Processing chunk 32/54 (from 930000ms to 960000ms)...\n",
      "Processing chunk 33/54 (from 960000ms to 990000ms)...\n",
      "Processing chunk 34/54 (from 990000ms to 1020000ms)...\n",
      "Processing chunk 35/54 (from 1020000ms to 1050000ms)...\n",
      "Processing chunk 36/54 (from 1050000ms to 1080000ms)...\n",
      "Processing chunk 37/54 (from 1080000ms to 1110000ms)...\n",
      "Processing chunk 38/54 (from 1110000ms to 1140000ms)...\n",
      "Processing chunk 39/54 (from 1140000ms to 1170000ms)...\n",
      "Processing chunk 40/54 (from 1170000ms to 1200000ms)...\n",
      "Processing chunk 41/54 (from 1200000ms to 1230000ms)...\n",
      "Processing chunk 42/54 (from 1230000ms to 1260000ms)...\n",
      "Processing chunk 43/54 (from 1260000ms to 1290000ms)...\n",
      "Processing chunk 44/54 (from 1290000ms to 1320000ms)...\n",
      "Processing chunk 45/54 (from 1320000ms to 1350000ms)...\n",
      "Processing chunk 46/54 (from 1350000ms to 1380000ms)...\n",
      "Processing chunk 47/54 (from 1380000ms to 1410000ms)...\n",
      "Processing chunk 48/54 (from 1410000ms to 1440000ms)...\n",
      "Processing chunk 49/54 (from 1440000ms to 1470000ms)...\n",
      "Processing chunk 50/54 (from 1470000ms to 1500000ms)...\n",
      "Processing chunk 51/54 (from 1500000ms to 1530000ms)...\n",
      "Processing chunk 52/54 (from 1530000ms to 1560000ms)...\n",
      "Processing chunk 53/54 (from 1560000ms to 1590000ms)...\n",
      "Processing chunk 54/54 (from 1590000ms to 1592687ms)...\n",
      "\n",
      "--- Detailed Notes ---\n",
      "\n",
      "- The Nvidia AI Podcast hosted by Noah Kravitz discusses the impact of digital humans and AI agents, highlighting the collaboration between InWorld AI, Nvidia, and Streamlabs at CES 2025.\n",
      "- Chris Covert, director of product experiences at InWorld AI, emphasizes the company's mission to make AI accessible and addresses the challenges in AI deployment, especially in gaming and entertainment.\n",
      "- InWorld AI's Intelligent Streaming Assistant, showcased at CES, is an AI agent that provides real-time commentary and support for streamers, leveraging Nvidia's technology and Inworld's generative AI capabilities.\n",
      "- Chris Covert shares insights on the evolution of AI, from simple chatbots to autonomous agents capable of complex interactions and decision-making.\n",
      "- The conversation touches on the importance of human-centered design in AI, the need for a moonshot approach to innovation, and the challenges of creating digital companions.\n",
      "- Chris Covert discusses the roadmap for Inworld AI's technology, including plans for productization and the development of a robust ecosystem for digital companions.\n",
      "- The podcast concludes with an invitation for listeners to explore more about Inworld AI's work on their website and social media channels.\n",
      "\n",
      "Generating summary...\n",
      "\n",
      "--- Summary ---\n",
      "\n",
      "The Nvidia AI Podcast, hosted by Noah Kravitz, featured Chris Covert from InWorld AI discussing the collaboration with Nvidia and Streamlabs at CES 2025. Covert highlighted InWorld AI's mission to make AI accessible, focusing on challenges in gaming and entertainment. The Intelligent Streaming Assistant, an AI agent providing real-time commentary for streamers, was showcased, leveraging Nvidia's technology and Inworld's generative AI. The evolution of AI from chatbots to autonomous agents was discussed, emphasizing human-centered design and the need for innovation. Covert outlined Inworld AI's roadmap for technology development and encouraged listeners to explore their work further.\n",
      "Text saved to podcast_detailed_notes.txt\n"
     ]
    }
   ],
   "source": [
    "# Set your NVIDIA API key and podcast audio file path\n",
    "API_KEY = \"your_api_key_here\"\n",
    "podcast_audio_path = \"podcast_audio.mp3\"  # update with your file path\n",
    "\n",
    "# Generate detailed notes from the audio file (with 30-second chunks by default)\n",
    "print(\"Starting detailed note generation...\")\n",
    "transcription = generate_detailed_notes(podcast_audio_path, API_KEY, chunk_duration_ms=30000)\n",
    "detailed_notes = refine_transcription_to_notes(transcription, API_KEY)\n",
    "\n",
    "print(\"\\n--- Detailed Notes ---\\n\")\n",
    "print(detailed_notes)\n",
    "\n",
    "# Generate a summary of the detailed notes\n",
    "print(\"\\nGenerating summary...\")\n",
    "summary = summarize_notes(detailed_notes, API_KEY)\n",
    "\n",
    "print(\"\\n--- Summary ---\\n\")\n",
    "print(summary)\n",
    "\n",
    "# Save detailed notes and summary to a text file\n",
    "combined_text = detailed_notes + \"\\n\\n--- SUMMARY ---\\n\\n\" + summary\n",
    "save_text_to_file(combined_text, \"podcast_detailed_notes.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "The following cell translates the combined notes and summary into another language (e.g., Spanish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating transcript and summary to Spanish...\n",
      "\n",
      "--- Translated Text ---\n",
      "\n",
      "- El podcast de Nvidia AI, presentado por Noah Kravitz, discute el impacto de los humanos digitales y agentes de IA, destacando la colaboración entre InWorld AI, Nvidia y Streamlabs en CES 2025.\n",
      "- Chris Covert, director de experiencias de producto en InWorld AI, enfatiza la misión de la empresa de hacer la IA accesible y aborda los desafíos en la implementación de la IA, especialmente en el juego y entretenimiento.\n",
      "- El asistente de transmisión inteligente de InWorld AI, mostrado en CES, es un agente de IA que proporciona comentarios en tiempo real y apoyo para los streamers, aprovechando la tecnología de Nvidia y las capacidades de IA generativa de Inworld.\n",
      "- Chris Covert comparte sus conocimientos sobre la evolución de la IA, desde simples chatbots hasta agentes autónomos capaces de interacciones y toma de decisiones complejas.\n",
      "- La conversación toca la importancia del diseño centrado en el humano en la IA, la necesidad de un enfoque de luna nueva para la innovación y los desafíos de crear compañeros digitales.\n",
      "- Chris Covert discute el camino a seguir para la tecnología de Inworld AI, incluyendo planes para la comercialización y el desarrollo de un ecosistema robusto para compañeros digitales.\n",
      "- El podcast concluye con una invitación para que los oyentes exploren más sobre el trabajo de Inworld AI en su sitio web y canales de redes sociales.\n",
      "\n",
      "--- RESUMEN ---\n",
      "\n",
      "El podcast de Nvidia AI, presentado por Noah Kravitz, contó con Chris Covert de InWorld AI discutiendo la colaboración con Nvidia y Streamlabs en CES 2025. Covert destacó la misión de InWorld AI de hacer la IA accesible, enfocándose en los desafíos en el juego y entretenimiento. El asistente de transmisión inteligente, un agente de IA que proporciona comentarios en tiempo real para los streamers, fue mostrado, aprovechando la tecnología de Nvidia y las capacidades de IA generativa de Inworld. La evolución de la IA de chatbots a agentes autónomos fue discutida, enfatizando el diseño centrado en el humano y la necesidad de innovación. Covert delineó el camino a seguir para la tecnología de Inworld AI y alentó a los oyentes a explorar más su trabajo.\n",
      "Text saved to podcast_transcription_translated.txt\n"
     ]
    }
   ],
   "source": [
    "target_language = \"Spanish\"\n",
    "print(f\"\\nTranslating transcript and summary to {target_language}...\")\n",
    "translated_text = translate_text(combined_text, target_language, API_KEY)\n",
    "\n",
    "print(\"\\n--- Translated Text ---\\n\")\n",
    "print(translated_text)\n",
    "\n",
    "# Save the translated text to a file\n",
    "save_text_to_file(translated_text, \"podcast_transcription_translated.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "If you encounter issues with the API, here are some common problems and solutions:\n",
    "\n",
    "1. **Payload Too Large**: Use the optimization functions with more aggressive parameters\n",
    "2. **Authentication Errors**: Verify your API key is correct and has the necessary permissions\n",
    "3. **Model Not Available**: Check that you're using a valid model name for your account tier\n",
    "4. **Format Issues**: Ensure your media files are in supported formats\n",
    "5. **Rate Limiting**: If processing many chunks, add delays between API calls to avoid rate limits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_companion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2941e94f-db20-44a5-ab87-2cab499825f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Digital Finger Printing (DFP) with Morpheus + NIM - Azure Inference and Triage with an LLM\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building and running a DFP pipeline that performs inference on Azure logs. The goal is to use the pretrained models generated in the Duo Training notebook to generate anomaly scores for each log. These anomaly scores can be used by security teams to detect abnormal behavior when it happens so the proper action can be taken.\n",
    "\n",
    "We will also build upon the Azure Inference only notebook by passing thresholded and filtered event to a LLaMa-3 8B model hosted in an NVIDIA Inference Microservice (NIM) for natural language explanation and initial triage. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For more information on DFP, the Morpheus pipeline, and setup steps to run this notebook, please refer to the coresponding DFP training materials.\n",
    "</div>\n",
    "\n",
    "We'll begin with some basic imports we will use in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c1cb50-74f2-445d-b865-8c22c3b3798b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from upload_intel import nemo_retriever_client as nrc\n",
    "\n",
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ed89b5-71be-4a95-b918-059eacbaff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/conda/envs/morpheus/lib/python3.10/site-packages (1.37.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/envs/morpheus/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102ce011-3ca3-4f96-a72d-de28fad32003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {align:left;display:block}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import logging\n",
    "import os\n",
    "import typing\n",
    "import mlflow\n",
    "import json\n",
    "\n",
    "import time\n",
    "import cudf\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "from dfp.stages.dfp_file_batcher_stage import DFPFileBatcherStage\n",
    "from dfp.stages.dfp_file_to_df import DFPFileToDataFrameStage\n",
    "from dfp.stages.dfp_inference_stage import DFPInferenceStage\n",
    "from dfp.stages.dfp_postprocessing_stage import DFPPostprocessingStage\n",
    "from dfp.stages.dfp_preprocessing_stage import DFPPreprocessingStage\n",
    "from dfp.stages.dfp_rolling_window_stage import DFPRollingWindowStage\n",
    "from dfp.stages.dfp_split_users_stage import DFPSplitUsersStage\n",
    "from dfp.stages.multi_file_source import MultiFileSource\n",
    "from dfp.utils.regex_utils import iso_date_regex\n",
    "from dfp.stages.dfp_string_create_stage import DFPStringCreateStage\n",
    "from dfp.stages.dfp_rag_concat_stage import DFPRAGConcatStage\n",
    "from dfp.stages.dfp_rag_upload_stage import DFPRAGUploadStage\n",
    "from dfp.llm.nim_llm_service import NIMChatService\n",
    "from dfp.llm.nim_task_handler import NIMTaskHandler\n",
    "from dfp.llm.retriever_context_node import RetrieverContextNode\n",
    "from upload_intel.nemo_retriever_client import RetrieverClient\n",
    "from dfp.llm.llm_engine_utils import build_engine_llm_service, build_engine_rag_context\n",
    "\n",
    "from morpheus.common import FileTypes\n",
    "from morpheus.common import FilterSource\n",
    "from morpheus.cli.utils import get_log_levels\n",
    "from morpheus.cli.utils import get_package_relative_file\n",
    "from morpheus.cli.utils import load_labels_file\n",
    "from morpheus.cli.utils import parse_log_level\n",
    "from morpheus.config import Config\n",
    "from morpheus.config import ConfigAutoEncoder\n",
    "from morpheus.config import CppConfig\n",
    "from morpheus.pipeline import LinearPipeline\n",
    "from morpheus.stages.output.write_to_file_stage import WriteToFileStage\n",
    "from morpheus.utils.column_info import ColumnInfo\n",
    "from morpheus.utils.column_info import DataFrameInputSchema\n",
    "from morpheus.utils.column_info import DateTimeColumn\n",
    "from morpheus.utils.column_info import DistinctIncrementColumn\n",
    "from morpheus.utils.column_info import IncrementColumn\n",
    "from morpheus.utils.column_info import RenameColumn\n",
    "from morpheus.utils.column_info import StringCatColumn\n",
    "from morpheus.utils.file_utils import date_extractor\n",
    "from morpheus.stages.postprocess.filter_detections_stage import FilterDetectionsStage\n",
    "from morpheus.stages.postprocess.serialize_stage import SerializeStage\n",
    "from morpheus.utils.logger import configure_logging\n",
    "from morpheus.stages.general.monitor_stage import MonitorStage\n",
    "from morpheus.config import PipelineModes\n",
    "from morpheus.io.deserializers import read_file_to_df\n",
    "from morpheus.llm import LLMEngine\n",
    "from morpheus.llm.nodes.extracter_node import ExtracterNode\n",
    "from morpheus.llm.nodes.llm_generate_node import LLMGenerateNode\n",
    "from morpheus.llm.nodes.prompt_template_node import PromptTemplateNode\n",
    "from morpheus.llm.services.llm_service import LLMService\n",
    "from morpheus.llm.services.nemo_llm_service import NeMoLLMService\n",
    "from morpheus.llm.services.openai_chat_service import OpenAIChatService\n",
    "from morpheus.llm.task_handlers.simple_task_handler import SimpleTaskHandler\n",
    "from morpheus.messages import ControlMessage\n",
    "from morpheus.stages.input.in_memory_source_stage import InMemorySourceStage\n",
    "from morpheus.stages.llm.llm_engine_stage import LLMEngineStage\n",
    "from morpheus.stages.output.in_memory_sink_stage import InMemorySinkStage\n",
    "from morpheus.stages.preprocess.deserialize_stage import DeserializeStage\n",
    "from morpheus.utils.concat_df import concat_dataframes\n",
    "\n",
    "\n",
    "# Left align all tables\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38c1b7-ce84-43e0-ac53-280562dc1642",
   "metadata": {
    "tags": []
   },
   "source": [
    "## High Level Configuration of DFP\n",
    "\n",
    "***This section is configured exactly the same as the Azure Inference demo notebook in this directory. If you are familiar with that, you can skip over this.***\n",
    "\n",
    "The following options significantly alter the functionality of the pipeline. These options are separated from the individual stage options since they may effect more than one stage. Additionally, the matching python script to this notebook, `dfp_azure_pipeline.py`, configures these options via command line arguments.\n",
    "\n",
    "### Options\n",
    "\n",
    "| Name | Type | Description |\n",
    "| --- | --- | :-- |\n",
    "| `train_users` | One of `[\"none\"]` | For inference, this option should always be `\"none\"` |\n",
    "| `skip_users` | List of strings | Any user in this list will be dropped from the pipeline. Useful for debugging to remove automated accounts with many logs. |\n",
    "| `cache_dir` | string | The location to store cached files. To aid with development and reduce bandwidth, the Morpheus pipeline will cache data from several stages of the pipeline. This option configures the location for those caches. |\n",
    "| `input_files` | List of strings | List of files to process. Can specify multiple arguments for multiple files. Also accepts glob (\\*) wildcards and schema prefixes such as `s3://`. For example, to make a local cache of an s3 bucket, use `filecache::s3://mybucket/*`. Refer to `fsspec` documentation for list of possible options. |\n",
    "| `model_name_formatter` | string | A format string to use when building the model name. The model name is constructed by calling `model_name_formatter.format(user_id=user_id)`. For example, with `model_name_formatter=\"my_model-{user_id}\"` and a user ID of `\"first:last\"` would result in the model name of `\"my_model-first:last\"`. This should match the value used in `DFPMLFlowModelWriterStage`. Available keyword arguments: `user_id`, `user_md5`. |\n",
    "| `experiment_name_formatter` | string | A format string (without the `f`) that will be used when creating an experiment in ML Flow. Available keyword arguments: `user_id`, `user_md5`, `reg_model_name`. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee00703-75c5-46fc-890c-86733da906c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global options\n",
    "train_users = \"none\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_users: typing.List[str] = []\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"./.cache/dfp\"\n",
    "\n",
    "# Input files to read from\n",
    "input_files = [\n",
    "    \"../../../../data/dfp/azure-inference-data/AZUREAD_*.json\",\n",
    "]\n",
    "\n",
    "# The format to use for models\n",
    "model_name_formatter = \"DFP-azure-{user_id}\"\n",
    "\n",
    "# === Derived Options ===\n",
    "# To include the generic, we must be training all or generic\n",
    "include_generic = train_users == \"all\" or train_users == \"generic\"\n",
    "\n",
    "# To include individual, we must be either training or inferring\n",
    "include_individual = train_users != \"generic\"\n",
    "\n",
    "# None indicates we arent training anything\n",
    "is_training = train_users != \"none\"\n",
    "\n",
    "# Tracking URI\n",
    "tracking_uri = \"http://mlflow:5000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b586016",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set MLFlow Tracking URI\n",
    "Set MLFlow tracking URI to make inference calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea82337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc24c9-c85e-4977-a348-692c8f0aceaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Global Config Object\n",
    "Before creating the pipeline, we need to setup logging and set the parameters for the Morpheus config object. This config object is responsible for the following:\n",
    " - Indicating whether to use C++ or Python stages\n",
    "    - C++ stages are not supported for the DFP pipeline. This should always be `False`\n",
    " - Setting the number of threads to use in the pipeline. Defaults to the thread count of the OS.\n",
    " - Sets the feature column names that will be used in model training\n",
    "    - This option allows extra columns to be used in the pipeline that will not be part of the training algorithm.\n",
    "    - The final features that the model will be trained on will be an intersection of this list with the log columns.\n",
    " - The column name that indicates the user's unique identifier\n",
    "    - It is required for DFP to have a user ID column\n",
    " - The column name that indicates the timestamp for the log\n",
    "    - It is required for DFP to know when each log occurred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01abd537-9162-49dc-8e83-d9465592f1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable the Morpheus logger\n",
    "from morpheus.config import PipelineModes\n",
    "import os\n",
    "\n",
    "config = Config()\n",
    "\n",
    "CppConfig.set_should_use_cpp(False)\n",
    "\n",
    "config.num_threads = os.cpu_count()\n",
    "config.mode = PipelineModes.NLP #This is for the LLM piece, to allow Morpheus to make the necessary NLP GPU optimizations in the pipeline during data processing\n",
    "\n",
    "config.ae = ConfigAutoEncoder()\n",
    "\n",
    "config.ae.feature_columns = [\n",
    "    \"appDisplayName\", \"clientAppUsed\", \"deviceDetailbrowser\", \"deviceDetaildisplayName\", \"deviceDetailoperatingSystem\", \"statusfailureReason\", \"appincrement\", \"locincrement\", \"logcount\", \n",
    "]\n",
    "config.ae.userid_column_name = \"username\"\n",
    "config.ae.timestamp_column_name = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73a4d53-32b6-4ab8-a5d7-c0104b31c69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the column names to ensure all data is uniform\n",
    "source_column_info = [\n",
    "    DateTimeColumn(name=config.ae.timestamp_column_name, dtype=datetime, input_name=\"time\"),\n",
    "    RenameColumn(name=config.ae.userid_column_name, dtype=str, input_name=\"properties.userPrincipalName\"),\n",
    "    RenameColumn(name=\"appDisplayName\", dtype=str, input_name=\"properties.appDisplayName\"),\n",
    "    ColumnInfo(name=\"category\", dtype=str),\n",
    "    RenameColumn(name=\"clientAppUsed\", dtype=str, input_name=\"properties.clientAppUsed\"),\n",
    "    RenameColumn(name=\"deviceDetailbrowser\", dtype=str, input_name=\"properties.deviceDetail.browser\"),\n",
    "    RenameColumn(name=\"deviceDetaildisplayName\", dtype=str, input_name=\"properties.deviceDetail.displayName\"),\n",
    "    RenameColumn(name=\"deviceDetailoperatingSystem\",\n",
    "                    dtype=str,\n",
    "                    input_name=\"properties.deviceDetail.operatingSystem\"),\n",
    "    StringCatColumn(name=\"location\",\n",
    "                    dtype=str,\n",
    "                    input_columns=[\n",
    "                        \"properties.location.city\",\n",
    "                        \"properties.location.countryOrRegion\",\n",
    "                    ],\n",
    "                    sep=\", \"),\n",
    "    RenameColumn(name=\"statusfailureReason\", dtype=str, input_name=\"properties.status.failureReason\"),\n",
    "]\n",
    "\n",
    "source_schema = DataFrameInputSchema(json_columns=[\"properties\"], column_info=source_column_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a0cb0a-e65a-444a-a06c-a4525d543790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing schema\n",
    "preprocess_column_info = [\n",
    "    ColumnInfo(name=config.ae.timestamp_column_name, dtype=datetime),\n",
    "    ColumnInfo(name=config.ae.userid_column_name, dtype=str),\n",
    "    ColumnInfo(name=\"appDisplayName\", dtype=str),\n",
    "    ColumnInfo(name=\"clientAppUsed\", dtype=str),\n",
    "    ColumnInfo(name=\"deviceDetailbrowser\", dtype=str),\n",
    "    ColumnInfo(name=\"deviceDetaildisplayName\", dtype=str),\n",
    "    ColumnInfo(name=\"deviceDetailoperatingSystem\", dtype=str),\n",
    "    ColumnInfo(name=\"statusfailureReason\", dtype=str),\n",
    "\n",
    "    # Derived columns\n",
    "    IncrementColumn(name=\"logcount\",\n",
    "                    dtype=int,\n",
    "                    input_name=config.ae.timestamp_column_name,\n",
    "                    groupby_column=config.ae.userid_column_name),\n",
    "    DistinctIncrementColumn(name=\"locincrement\",\n",
    "                            dtype=int,\n",
    "                            input_name=\"location\",\n",
    "                            groupby_column=config.ae.userid_column_name,\n",
    "                            timestamp_column=config.ae.timestamp_column_name),\n",
    "    DistinctIncrementColumn(name=\"appincrement\",\n",
    "                            dtype=int,\n",
    "                            input_name=\"appDisplayName\",\n",
    "                            groupby_column=config.ae.userid_column_name,\n",
    "                            timestamp_column=config.ae.timestamp_column_name)\n",
    "]\n",
    "\n",
    "preprocess_schema = DataFrameInputSchema(column_info=preprocess_column_info, preserve_columns=[\"_batch_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc59de-dea8-4e5f-98e3-98eba1e4621d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pipeline Construction\n",
    "From this point on we begin constructing the stages that will make up the pipeline. To make testing easier, constructing the pipeline object, adding the stages, and running the pipeline, is provided as a single cell. The below cell can be rerun multiple times as needed for debugging.\n",
    "\n",
    "### Source Stage (`MultiFileSource`)\n",
    "\n",
    "This pipeline read input logs from one or more input files. This source stage will construct a list of files to be processed and pass to downstream stages. It is capable of reading files from many different source types, both local and remote. This is possible by utilizing the `fsspec` library (similar to `pandas`). Refer to the [`fsspec`](https://filesystem-spec.readthedocs.io/) documentation for more information on the supported file types. Once all of the logs have been read, the source completes. \n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filenames` | List of strings | | Any files to read into the pipeline. All files will be combined into a single `DataFrame` |\n",
    "\n",
    "### File Batcher Stage (`DFPFileBatcherStage`)\n",
    "\n",
    "To improve performance, multiple small input files can be batched together into a single DataFrame for processing. This stage is responsible for determining the timestamp of input files, grouping input files into batches by time, and sending the batches to be processed into a single DataFrame. Repeated batches of files will be loaded from cache resulting in increased performance. For example, when performaing a 60 day training run, 59 days can be cached with a period of `\"D\"` and retraining once per day.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `period` | `str` | `\"D\"` | The period to create batches. Refer to `pandas` windowing frequency documentation for available options.  |\n",
    "| `date_conversion_func` | Function of `typing.Callable[[fsspec.core.OpenFile], datetime]` | | A callback which is responsible for determining the date for a specified file. |\n",
    "\n",
    "### File to DataFrame Stage (`DFPFileToDataFrameStage`)\n",
    "\n",
    "After files have been batched into groups, this stage is responsible for reading the files and converting into a DataFrame. The specified input schema converts the raw DataFrame into one suitable for caching and processing. Any columns that are not needed should be excluded from the schema.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `schema` | `DataFrameInputSchema` | | After the raw `DataFrame` is read from each file, this schema will be applied to ensure a consisten output from the source. |\n",
    "| `file_type` | `FileTypes` | `FileTypes.Auto` | Allows overriding the file type. When set to `Auto`, the file extension will be used. Options are `CSV`, `JSON`, `PARQUET`, `Auto`. |\n",
    "| `parser_kwargs` | `dict` | `{}` | This dictionary will be passed to the `DataFrame` parser class. Allows for customization of log parsing. |\n",
    "| `cache_dir` | `str` | `./.cache/dfp` | The location to write cached input files to. |\n",
    "\n",
    "### Split Users Stage (`DFPSplitUsersStage`)\n",
    "\n",
    "Once the input logs have been read into a `DataFrame`, this stage is responsible for breaking that single `DataFrame` with many users into multiple `DataFrame`s for each user. This is also where the pipeline chooses whether to train individual users or the generic user (or both).\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `include_generic` | `bool` | | Whether or not to combine all user logs into a single `DataFrame` with the username 'generic_user' |\n",
    "| `include_individual` | `bool` | | Whether or not to output individual `DataFrame` objects for each user |\n",
    "| `skip_users` | List of `str` | `[]` | Any users to remove from the `DataFrame`. Useful for debugging to remove automated accounts with many logs. Mutually exclusive with `only_users`. |\n",
    "| `only_users` | List of `str` | `[]` | Only allow these users in the final `DataFrame`. Useful for debugging to focus on specific users. Mutually exclusive with `skip_users`. |\n",
    "\n",
    "### Rolling Window Stage (`DFPRollingWindowStage`)\n",
    "\n",
    "The Rolling Window Stage performs several key pieces of functionality for DFP.\n",
    "1. This stage keeps a moving window of logs on a per user basis\n",
    "   1. These logs are saved to disk to reduce memory requirements between logs from the same user\n",
    "1. It only emits logs when the window history requirements are met\n",
    "   1. Until all of the window history requirements are met, no messages will be sent to the rest of the pipeline.\n",
    "   1. Configuration options for defining the window history requirements are detailed below.\n",
    "1. It repeats the necessary logs to properly calculate log dependent features.\n",
    "   1. To support all column feature types, incoming log messages can be combined with existing history and sent to downstream stages.\n",
    "   1. For example, to calculate a feature that increments a counter for the number of logs a particular user has generated in a single day, we must have the user's log history for the past 24 hours. To support this, this stage will combine new logs with existing history into a single `DataFrame`.\n",
    "   1. It is the responsibility of downstream stages to distinguish between new logs and existing history.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `min_history` | `int` | `1` | The minimum number of logs a user must have before emitting any messages. Logs below this threshold will be saved to disk. |\n",
    "| `min_increment` | `int` or `str` | `0` | Once the min history requirement is met, this stage must receive `min_increment` *new* logs before emmitting another message. Logs received before this threshold is met will be saved to disk. Can be specified as an integer count or a string duration. |\n",
    "| `max_history` | `int` or `str` | `\"1d\"` | Once `min_history` and `min_increment` requirements have been met, this puts an upper bound on the maximum number of messages to forward into the pipeline and also the maximum amount of messages to retain in the history. Can be specified as an integer count or a string duration. |\n",
    "| `cache_dir` | `str` | `./.cache/dfp` | The location to write log history to disk. |\n",
    "\n",
    "### Preprocessing Stage (`DFPPreprocessingStage`)\n",
    "\n",
    "This stage performs the final, row dependent, feature calculations as specified by the input schema object. Once calculated, this stage can forward on all received logs, or optionally can only forward on new logs, removing any history information.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `input_schema` | `DataFrameInputSchema` | | The final, row dependent, schema to apply to the incoming columns |\n",
    "\n",
    "### Inference Stage (`DFPInference`)\n",
    "\n",
    "This stage performs several tasks to aid in performing inference. This stage will:\n",
    "1. Download models as needed from MLFlow\n",
    "1. Cache previously downloaded models to improve performance\n",
    "   1. Models in the cache will be periodically refreshed from MLFlow at a configured rate\n",
    "1. Perform inference using the downloaded model\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `model_name_formatter` | `str` | `\"\"` | A format string to use when building the model name. The model name is constructed by calling `model_name_formatter.format(user_id=user_id)`. For example, with `model_name_formatter=\"my_model-{user_id}\"` and a user ID of `\"first:last\"` would result in the model name of `\"my_model-first:last\"`. This should match the value used in `DFPMLFlowModelWriterStage` |\n",
    "\n",
    "### Filter Detection Stage (`FilterDetectionsStage`)\n",
    "This stage filters the output from the inference stage for any anomalous messages. Logs which exceed the specified Z-Score will be passed onto the next stage. All remaining logs which are below the threshold will be dropped. For the purposes of the DFP pipeline, this stage is configured to use the `mean_abs_z` column of the DataFrame as the filter criteria.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `threshold` | `float` | `0.5` | The threshold value above which logs are considered to be anomalous. The default is `0.5`, however the DFP pipeline uses a value of `2.0`. All normal logs will be filtered out and anomalous logs will be passed on. |\n",
    "| `copy` | `bool` | `True` | When the `copy` argument is `True` (default), rows that meet the filter criteria are copied into a new dataframe. When `False` sliced views are used instead. This is a performance optimization, and has no functional impact. |\n",
    "| `filter_source` | `FilterSource` | `FilterSource.Auto` | Indicates if the filter criteria exists in an output tensor (`FilterSource.TENSOR`) or a column in a DataFrame (`FilterSource.DATAFRAME`). |\n",
    "| `field_name` | `str` | `probs` | Name of the tensor (`filter_source=FilterSource.TENSOR`) or DataFrame column (`filter_source=FilterSource.DATAFRAME`) to use as the filter criteria. |\n",
    "\n",
    "### Post Processing Stage (`DFPPostprocessingStage`)\n",
    "This stage adds a new `event_time` column to the DataFrame indicating the time which Morpheus detected the anomalous messages, and replaces any `NAN` values with the a string value of `'NaN'`.\n",
    "\n",
    "### Serialize Stage (`SerializeStage`)\n",
    "This stage controls which columns in the DataFrame will be included in the output. For the purposes of the DFP pipeline, we will exclude columns that are used internally by the pipeline which are not of interest to the end-user.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `include` | List of `str` | `[]` | List of regular expression patterns matching columns to include in the output. Specifying an empty list causes all columns to be included not explicitly excluded. |\n",
    "| `exclude` | List of `str` | `[r'^ID$', r'^_ts_']` | List of regular expression patterns matching columns to exclude from the output. |\n",
    "| `fixed_columns` | `bool` | `True` | When `True` it is assumed that the Dataframe in all messages contain the same columns as the first message received. |\n",
    "\n",
    "***You'll notice here that we don't have a write to file stage for the DFP pipeline. That is because, in the next section, we will add stages for LLM processing.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94ca6d2-376a-4ed4-a133-42f37768122e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<monitor-11; MonitorStage(description=DFP Serialization rate, smoothing=0.001, unit=messages, delayed_start=False, determine_count_fn=None, log_level=LogLevels.INFO)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = LinearPipeline(config)\n",
    "\n",
    "# Source stage\n",
    "pipeline.set_source(MultiFileSource(config, filenames=input_files))\n",
    "\n",
    "# Batch files into batches by time. Use the default ISO date extractor from the filename\n",
    "pipeline.add_stage(\n",
    "    DFPFileBatcherStage(config,\n",
    "                        period=\"D\",\n",
    "                        date_conversion_func=functools.partial(date_extractor, filename_regex=iso_date_regex)))\n",
    "\n",
    "# Output is a list of fsspec files. Convert to DataFrames. This caches downloaded data\n",
    "pipeline.add_stage(\n",
    "    DFPFileToDataFrameStage(config,\n",
    "                            schema=source_schema,\n",
    "                            file_type=FileTypes.JSON,\n",
    "                            parser_kwargs={\n",
    "                                \"lines\": False, \"orient\": \"records\"\n",
    "                            },\n",
    "                            cache_dir=cache_dir))\n",
    "\n",
    "\n",
    "# This will split users or just use one single user\n",
    "pipeline.add_stage(\n",
    "    DFPSplitUsersStage(config,\n",
    "                        include_generic=include_generic,\n",
    "                        include_individual=include_individual,\n",
    "                        skip_users=skip_users))\n",
    "\n",
    "# Next, have a stage that will create rolling windows\n",
    "pipeline.add_stage(\n",
    "    DFPRollingWindowStage(\n",
    "        config,\n",
    "        min_history=300 if is_training else 1,\n",
    "        min_increment=300 if is_training else 0,\n",
    "        # For inference, we only ever want 1 day max\n",
    "        max_history=\"60d\" if is_training else \"1d\",\n",
    "        cache_dir=cache_dir))\n",
    "\n",
    "# Output is UserMessageMeta -- Cached frame set\n",
    "pipeline.add_stage(DFPPreprocessingStage(config, input_schema=preprocess_schema))\n",
    "\n",
    "# Perform inference on the preprocessed data\n",
    "pipeline.add_stage(DFPInferenceStage(config, model_name_formatter=model_name_formatter))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"Inference rate\", smoothing=0.001))\n",
    "\n",
    "# Filter for only the anomalous logs\n",
    "pipeline.add_stage(\n",
    "            FilterDetectionsStage(config, threshold=6, filter_source=FilterSource.DATAFRAME, field_name='mean_abs_z'))\n",
    "pipeline.add_stage(DFPPostprocessingStage(config))\n",
    "\n",
    "# Exclude the columns we don't want in our output\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['batch_count', 'origin_hash', '_row_hash', '_batch_id']))\n",
    "\n",
    "# Monitor throughput at the tail-end of the DFP specific portion of the pipeline\n",
    "pipeline.add_stage(MonitorStage(config, description=\"DFP Serialization rate\", smoothing=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01a1e6-fd51-4ba0-a3d5-5a87e533bf44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using a NIM for LLM-based Triage\n",
    "\n",
    "Now that we have the pipeline set up to pass through the Digital Fingerprinting piece of the code, the output of our last `MonitorStage` can be sent into a series of stages tha:\n",
    "\n",
    "1. **Transform the data into a column that can be used in an LLM prompt template.**\n",
    "2. **Apply a prompt template and make calls to the NIM using the Morpheus `LLMEngine`.**\n",
    "3. **Collect NIM output and add that to a column of our final output DataFrame.**\n",
    "4. **Write the output to a file.**\n",
    "\n",
    "### Exploring a Prompt Template\n",
    "\n",
    "We will use the following prompt template for the AZURE Active Directory log use case for DFP. Naturally, this will need to change when log sources change to give the LLM more context. \n",
    "\n",
    "```\n",
    "        You are an L1 SOC analyst. Your task is to triage and explain an alert received from an ML workflow that uses an autoencoder to perform anomaly detection, per user, on AZURE AD log telemetry.\n",
    "\n",
    "        The log contains the following kinds of fields: \n",
    "\n",
    "        timestamp: timestamp of the event\n",
    "        username: username of the user\n",
    "        appDisplayName: the name of the running app\n",
    "        category: the type of authentication event\n",
    "        clientAppUsed: the type of browser/app doing the authentication\n",
    "        deviceDetailBrowser: specifics about the browser such as versions, etc. \n",
    "        deviceDetailDisplayName: displayName of the user if available\n",
    "        location: location where the activity originated\n",
    "        statusfailureReason: if the auth event failed, why. \n",
    "        event_time: time the event was logged\n",
    "        logcount: number of authentication logs for this user in this time period\n",
    "        locincrement: number of distinct locations for that user\n",
    "        appincrement: number of distinct apps used to authenticate\n",
    "        <field>_pred: autoencoder output of the feature\n",
    "        <field>_z_loss: z-score of the standard scaled loss between feature and prediction\n",
    "        mean_abs_z: mean absolute z-score of reconstruction error across all logs for that user\n",
    "        max_abs_z: max absolute z-score of reconstruction error across all logs for that user\n",
    "\n",
    "        Given the feature explanations above, I would like you to use that, and any internal knowledge of cyber you have to triage the following event.\n",
    "\n",
    "        The event is: \n",
    "        ### EVENT ######\n",
    "        {{event}}\n",
    "        #### END EVENT ####\n",
    "\n",
    "        In your output, please keep it concise. Explain which fields were most anomalous, and any cyber-specific context that may be helpful around that that'll speed up the triage that a human will do after you. Split your response into the following sections using it as a templace:\n",
    "        \n",
    "        ##Start Report##\n",
    "        **Triage Overview**\n",
    "        <provide estimated severity 1/10 scale>\n",
    "        <provide an overview of the event and likelihood of malicious activity based on your cyber knowledge> \n",
    "        **Most Anomalous Fields**\n",
    "        <list the most anomalous fields and interpret their z-scores>\n",
    "        **Cyber Triage**\n",
    "        <cyber-specific content and triage in logical bullet points>\n",
    "        **Recommendations**\n",
    "        <recommendations on next steps for human triage and investigation> <DO NOT provide recommendations on changes to posture, policy etc. Just next steps in investigation>\n",
    "        ##End Report##\n",
    "        \n",
    "        Please only return ascii characters, no special characters like sigma or bullet points. Only A-Z,a-z, 0-9, *, and \\n\n",
    "```\n",
    "\n",
    "Here, the `{{event}}` portion of the template is where a dictionary-like version of the model output per event will be inserted to make the triage request. \n",
    "\n",
    "### Pipeline Stages\n",
    "\n",
    "Now, let us explore the various new Morpheus stages we will chain together to form the latter half of the larger pipeline.\n",
    "\n",
    "#### String Create Stage (`DFPStringCreateStage`)\n",
    "This is a custom stage that aggregates the value of every column on the `MessageMeta` dataframe into a single column called `event` that contains a string value of the dictionary representation of all columns as `key:value` pairs. \n",
    "\n",
    "The `DFPStringCreateStage` will also remove any non-ascii characters in the content of the strings to prevent errors in Morpheus pipeline logging.\n",
    "\n",
    "It is a transparent, pass-thru stage that does not require any configurable arugments.\n",
    "\n",
    "#### Deserialize Stage (`DeserializeStage`)\n",
    "This stage converts a simple `MessageMeta` object coming from the output of the DFP pipeline before it into a `ControlMessage` with a structure designed for the `LLMEngine`.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `ensure_sliceable_index` | `bool` | True | Whether or not to call `ensure_sliceable_index()` on all incoming `MessageMeta`, which will replace the index of the underlying dataframe if the existing one is not unique and monotonic. |\n",
    "| `message_type` | `MultiMessage or ControlMessage` | `MultiMessage` | Sets the type of message to be emitted from this stage. |\n",
    "| `task_type` | `str` | `None` | If specified, adds the specified task to the `ControlMessage`. This parameter is only valid when `message_type` is set to `ControlMessage`. If not `None`, `task_payload` must also be specified. |\n",
    "| `task_payload` | `dict` | `None` | If specified, adds the specified task to the `ControlMessage`. This parameter is only valid when `message_type`is set to `ControlMessage`. If not `None`, `task_type` must also be specified. |\n",
    "\n",
    "In our case, the task arguments are set to be `{\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"event\"]}}` which performs a completion task applying a prompt template on the `event` column of our dataframe. \n",
    "\n",
    "#### Write to File Stage (`WriteToFileStage`)\n",
    "\n",
    "This final stage will write all received messages to a single output file in either CSV or JSON format.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filename` | `str` | | The file to write anomalous log messages to. |\n",
    "| `overwrite` | `bool` | `False` | If the file specified in `filename` already exists, it will be overwritten if this option is set to `True` |\n",
    "\n",
    "***All the other stages we will see in the pipeline have already been introduced. They just might be initialized with different parameters.***\n",
    "\n",
    "### Building an `LLMEngine` Stage\n",
    "\n",
    "An `LLMEngine` stage is a stage that can consist of multiple `nodes` that perform various LLM based tasks. In our case, we will build an `LLMEngine` with the following components\n",
    "\n",
    "1. `ExtractorNode` which extract the relevent `event` column values from the input dataframe.\n",
    "2. `PromptTemplateNode` which applies the provided prompt template to the extracted events.\n",
    "3. `LLMGenerateNode` which makes API calls to the NIM service for completion. \n",
    "4. `SimpleTaskHandler` which copies fields from an `LLMContext` to columns in the DataFrame contained in the `ControlMessage` payload.\n",
    "\n",
    "Behind the scenes, the `LLMEngine` handles concurrent, asynchronous API calls and data-processing to boost performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b21eb9-bc30-4856-b1c3-9aabeae0662a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adding Event Summarization\n",
    "\n",
    "Now that we have the pieces in place to build our first `LLMEngineStage`, let's put it into the pipeline to create a stage that takes multiple anomalous detections for a user, and summarizes it into an incident based on the most anomalous events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "910a70f6-7415-48a8-8507-c16c3c648e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<serialize-17; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load prompt templates from file\n",
    "templates = json.load(open(\"dfp/llm/prompt_templates.json\", \"r\"))\n",
    "\n",
    "first_completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"event\"]}}\n",
    "\n",
    "pipeline.add_stage(DFPStringCreateStage(config))\n",
    "\n",
    "pipeline.add_stage(\n",
    "    DeserializeStage(config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=first_completion_task))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"NIM Summary Deserialize rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(LLMEngineStage(config, engine=build_engine_llm_service(prompt_template = templates[\"incident_summary\"], \n",
    "                                                                           llm_service=\"NIM\", output_column=\"incident_summary\")))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"NIM Summary Inference rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['event']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e62cc8-1da2-4fb1-b4b9-6a8aa31101cd",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) for Threat Intelligence Correlation and Recommendations\n",
    "\n",
    "In this section, we'll aim to use RAG to look up threat intelligence reports stored in a NeMo Retriever collection that are relevant to the anomalous event, and enrich the event with threat intelligence findings and recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94420ace-91df-467c-af05-087e33fdd356",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Using a NIM to Generate a Search Optimized Query\n",
    "\n",
    "The first part of this process will involve creating an `LLMEngineStage` that generates a search optimized prompt for the Retriever that incorporates the relevant aspects of the anomalous event. We'll use the following prompt template:\n",
    "\n",
    "```\n",
    "        You are an L1 SOC analyst. You will be given an incident summary generated from an ML workflow that uses an autoencoder to perform anomaly detection, per user, on AZURE AD log telemetry.\n",
    "\n",
    "        The summary was generated based on the following kinds of fields: \n",
    "\n",
    "        timestamp: timestamp of the event\n",
    "        username: username of the user\n",
    "        appDisplayName: the name of the running app\n",
    "        category: the type of authentication event\n",
    "        clientAppUsed: the type of browser/app doing the authentication\n",
    "        deviceDetailBrowser: specifics about the browser such as versions, etc. \n",
    "        deviceDetailDisplayName: displayName of the user if available\n",
    "        location: location where the activity originated\n",
    "        statusfailureReason: if the auth event failed, why. \n",
    "        event_time: time the event was logged\n",
    "        logcount: number of authentication logs for this user in this time period\n",
    "        locincrement: number of distinct locations for that user\n",
    "        appincrement: number of distinct apps used to authenticate\n",
    "        <field>_pred: autoencoder output of the feature\n",
    "        <field>_z_loss: z-score of the standard scaled loss between feature and prediction\n",
    "        mean_abs_z: mean absolute z-score of reconstruction error across all logs for that user\n",
    "        max_abs_z: max absolute z-score of reconstruction error across all logs for that user\n",
    "\n",
    "\n",
    "        The event is: \n",
    "        ### EVENT ######\n",
    "        {{incident_summary}}\n",
    "        #### END EVENT ####\n",
    "\n",
    "        Your task is to use the incident summary to create an optimized search query which can be used to search collections of Threat Intelligence documents for similar events either by threat actor, threat vector, or other similar characteristics. \n",
    "        \n",
    "        Please only return ascii characters, no special characters like sigma or bullet points. Only A-Z,a-z, 0-9 and '?'. Please only generate the search query, no text before or after it. \n",
    "        \n",
    "        An example of your response could be: \"Similar events and recommendatios for <description of anomaly> which could be indicative of <cyber triage summary>\n",
    "        \n",
    "        Format your response as:\n",
    "        \n",
    "        ##Begin Response##\n",
    "        <prompt>\n",
    "        ##End Response##\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa05bb78-cfcf-4d91-85b8-f00cb9037a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<serialize-22; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#configure_logging(log_level=logging.INFO)\n",
    "\n",
    "second_completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"incident_summary\"]}}\n",
    "\n",
    "pipeline.add_stage(\n",
    "    DeserializeStage(config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=second_completion_task))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"NIM RAG Prompt Deserialize rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "#build optimized query\n",
    "pipeline.add_stage(LLMEngineStage(config, engine=build_engine_llm_service(prompt_template=templates[\"rag_query\"],\n",
    "    llm_service=\"NIM\", output_column=\"rag_query\")))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"NIM RAG Prompt Inference rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['event']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502dc9f8-0161-432d-998f-ad8e5e2afa11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Collect RAG Context from the Nemo Retriever \n",
    "\n",
    "We'll build a decoupled `LLMEngineStage` that takes the RAG prompt generated in the previous stage and performs inference requests to NeMo retriever to collect context texts for every query.\n",
    "\n",
    "While building the `LLMEngine` for this stage, we see a new type of node: `RetrieverContextNode`. This node is a node used in an `LLMEngine` to asynchronously interact with an NVIDIA NeMo Retriever deployment to collect relevant text chunks for a given query. It accepts a `RetrieverClient` object initalized with the relevant collection information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a2acb0-064a-41c2-ab2c-7caf65577d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been merged into upload_intel/intel/cyber_enrichment/merged.txt\n",
      "text file loaded\n",
      "Number of chunks from the document: 370\n"
     ]
    }
   ],
   "source": [
    "third_completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"rag_query\"]}}\n",
    "\n",
    "pipeline.add_stage(\n",
    "    DeserializeStage(config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=third_completion_task))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"NeMo RAG Context Deserialize rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "#add threat intelligence\n",
    "retriever_client_cyber_enrichment = nrc.RetrieverClient()\n",
    "\n",
    "\n",
    "base_directory = \"upload_intel/intel/cyber_enrichment/\"\n",
    "input_files = [f\"{i}.txt\" for i in range(1, 14)]\n",
    "output_file = base_directory+\"merged.txt\"\n",
    "\n",
    "with open(output_file, 'w') as outfile:\n",
    "    # Iterate through the list of input files\n",
    "    for file_name in input_files:\n",
    "        with open(base_directory+file_name, 'r') as infile:\n",
    "            content = infile.read()\n",
    "            outfile.write(content)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(f\"Files have been merged into {output_file}\")\n",
    "\n",
    "#load in text file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(output_file)\n",
    "\n",
    "document = loader.load()\n",
    "print(\"text file loaded\")\n",
    "\n",
    "\n",
    "document_chunks = retriever_client_cyber_enrichment.add_files(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3feb3eb0-7af9-4e55-a91c-8a1558751fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llm-engine-25; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f4603d45570>)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use optimized query to extract context\n",
    "pipeline.add_stage(LLMEngineStage(config, engine=build_engine_rag_context(retriever_client_cyber_enrichment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a61de599-7189-4158-a02a-82f1ed37b296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<serialize-27; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.add_stage(MonitorStage(config, description=\"NeMo RAG Context Inference rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['event']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d384cfd-336d-4b17-8e79-a8b2988f73dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using RAG Context Alongside The Triaged Event to add a Threat Intelligence Enrichment Section\n",
    "\n",
    "Now that we've generated an incident summary, and collected threat intelligence reports that most closesly match the incident, we can synthesize the information into a single event summary report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b36b675-b193-4ea4-8e7f-591322985a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Pipeline Pre-build====\u001b[0m\n",
      "====Pre-Building Segment: linear_segment_0====\u001b[0m\n",
      "====Pre-Building Segment Complete!====\u001b[0m\n",
      "====Pipeline Pre-build Complete!====\u001b[0m\n",
      "====Registering Pipeline====\u001b[0m\n",
      "====Building Pipeline====\u001b[0m\n",
      "====Building Pipeline Complete!====\u001b[0m\n",
      "====Registering Pipeline Complete!====\u001b[0m\n",
      "====Starting Pipeline====\u001b[0m\n",
      "====Building Segment: linear_segment_0====\u001b[0m\n",
      "Added source: <from-multi-file-0; MultiFileSource(filenames=['../../../../data/dfp/azure-inference-data/AZUREAD_*.json'], watch=False, watch_interval=1.0)>\n",
      "  └─> fsspec.OpenFiles\u001b[0m\n",
      "Added stage: <dfp-file-batcher-1; DFPFileBatcherStage(date_conversion_func=functools.partial(<function date_extractor at 0x7f4508876e60>, filename_regex=re.compile('(?P<year>\\\\d{4})-(?P<month>\\\\d{1,2})-(?P<day>\\\\d{1,2})(?:T(?P<hour>\\\\d{1,2})(?::|_|\\\\.)(?P<minute>\\\\d{1,2})(?::|_|\\\\.)(?P<second>\\\\d{1,2})(?:\\\\.(?P<microsecond>\\\\d{0,6}))?)?(?P<zulu>Z)?')), period=D, sampling_rate_s=None, start_time=None, end_time=None, sampling=None)>\n",
      "  └─ fsspec.OpenFiles -> Tuple[fsspec.core.OpenFiles, int]\u001b[0m\n",
      "Added stage: <dfp-file-to-df-2; DFPFileToDataFrameStage(schema=DataFrameInputSchema(json_columns=['properties'], column_info=[DateTimeColumn(name='timestamp', dtype='datetime64[ns]', input_name='time'), RenameColumn(name='username', dtype='str', input_name='properties.userPrincipalName'), RenameColumn(name='appDisplayName', dtype='str', input_name='properties.appDisplayName'), ColumnInfo(name='category', dtype='str'), RenameColumn(name='clientAppUsed', dtype='str', input_name='properties.clientAppUsed'), RenameColumn(name='deviceDetailbrowser', dtype='str', input_name='properties.deviceDetail.browser'), RenameColumn(name='deviceDetaildisplayName', dtype='str', input_name='properties.deviceDetail.displayName'), RenameColumn(name='deviceDetailoperatingSystem', dtype='str', input_name='properties.deviceDetail.operatingSystem'), StringCatColumn(name='location', dtype='str', input_columns=['properties.location.city', 'properties.location.countryOrRegion'], sep=', '), RenameColumn(name='statusfailureReason', dtype='str', input_name='properties.status.failureReason')], preserve_columns=None, row_filter=None), filter_null=True, file_type=FileTypes.JSON, parser_kwargs={'lines': False, 'orient': 'records'}, cache_dir=./.cache/dfp)>\n",
      "  └─ Tuple[fsspec.core.OpenFiles, int] -> pandas.DataFrame\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-split-users-3; DFPSplitUsersStage(include_generic=False, include_individual=True, skip_users=[], only_users=None)>\n",
      "  └─ pandas.DataFrame -> dfp.DFPMessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-rolling-window-4; DFPRollingWindowStage(min_history=1, min_increment=0, max_history=1d, cache_dir=./.cache/dfp)>\n",
      "  └─ dfp.DFPMessageMeta -> dfp.MultiDFPMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-preproc-5; DFPPreprocessingStage(input_schema=DataFrameInputSchema(json_columns=[], column_info=[ColumnInfo(name='timestamp', dtype='datetime64[ns]'), ColumnInfo(name='username', dtype='str'), ColumnInfo(name='appDisplayName', dtype='str'), ColumnInfo(name='clientAppUsed', dtype='str'), ColumnInfo(name='deviceDetailbrowser', dtype='str'), ColumnInfo(name='deviceDetaildisplayName', dtype='str'), ColumnInfo(name='deviceDetailoperatingSystem', dtype='str'), ColumnInfo(name='statusfailureReason', dtype='str'), IncrementColumn(name='logcount', dtype='int', input_name='timestamp', groupby_column='username', period='D'), DistinctIncrementColumn(name='locincrement', dtype='int', input_name='location', groupby_column='username', period='D', timestamp_column='timestamp'), DistinctIncrementColumn(name='appincrement', dtype='int', input_name='appDisplayName', groupby_column='username', period='D', timestamp_column='timestamp')], preserve_columns=re.compile('(_batch_id)'), row_filter=None))>\n",
      "  └─ dfp.MultiDFPMessage -> dfp.MultiDFPMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "DFP Serialization rate: 0 messages [00:00, ? messages/s]\u001b[A\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-inference-6; DFPInferenceStage(model_name_formatter=DFP-azure-{user_id})>\n",
      "  └─ dfp.MultiDFPMessage -> morpheus.MultiAEMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-7; MonitorStage(description=Inference rate, smoothing=0.001, unit=messages, delayed_start=False, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.MultiAEMessage -> morpheus.MultiAEMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <filter-8; FilterDetectionsStage(threshold=6, copy=True, filter_source=FilterSource.DATAFRAME, field_name=mean_abs_z)>\n",
      "  └─ morpheus.MultiAEMessage -> morpheus.MultiAEMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-postproc-9; DFPPostprocessingStage()>\n",
      "  └─ morpheus.MultiAEMessage -> morpheus.MultiAEMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-10; SerializeStage(include=None, exclude=['batch_count', 'origin_hash', '_row_hash', '_batch_id'], fixed_columns=True)>\n",
      "  └─ morpheus.MultiAEMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-11; MonitorStage(description=DFP Serialization rate, smoothing=0.001, unit=messages, delayed_start=False, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-string-create-12; DFPStringCreateStage(top_k=5, grouper=username, sort_key=max_abs_z)>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <deserialize-13; DeserializeStage(ensure_sliceable_index=True, message_type=<class 'morpheus._lib.messages.ControlMessage'>, task_type=llm_engine, task_payload={'task_type': 'completion', 'task_dict': {'input_keys': ['event']}})>\n",
      "  └─ morpheus.MessageMeta -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-14; MonitorStage(description=NIM Summary Deserialize rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <llm-engine-15; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f4610dc0ab0>)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-16; MonitorStage(description=NIM Summary Inference rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-17; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <deserialize-18; DeserializeStage(ensure_sliceable_index=True, message_type=<class 'morpheus._lib.messages.ControlMessage'>, task_type=llm_engine, task_payload={'task_type': 'completion', 'task_dict': {'input_keys': ['incident_summary']}})>\n",
      "  └─ morpheus.MessageMeta -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-19; MonitorStage(description=NIM RAG Prompt Deserialize rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <llm-engine-20; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f45086a50f0>)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-21; MonitorStage(description=NIM RAG Prompt Inference rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-22; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <deserialize-23; DeserializeStage(ensure_sliceable_index=True, message_type=<class 'morpheus._lib.messages.ControlMessage'>, task_type=llm_engine, task_payload={'task_type': 'completion', 'task_dict': {'input_keys': ['rag_query']}})>\n",
      "  └─ morpheus.MessageMeta -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-24; MonitorStage(description=NeMo RAG Context Deserialize rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <llm-engine-25; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f4603d45570>)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-26; MonitorStage(description=NeMo RAG Context Inference rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-27; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-rag-concat-28; DFPRAGConcatStage()>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <deserialize-29; DeserializeStage(ensure_sliceable_index=True, message_type=<class 'morpheus._lib.messages.ControlMessage'>, task_type=llm_engine, task_payload={'task_type': 'completion', 'task_dict': {'input_keys': ['event']}})>\n",
      "  └─ morpheus.MessageMeta -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-30; MonitorStage(description=NIM RAG Enrich Deserialize rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <llm-engine-31; LLMEngineStage(engine=<morpheus._lib.llm.LLMEngine object at 0x7f4640dd0730>)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <monitor-32; MonitorStage(description=NIM RAG Enrich Inference rate, smoothing=0.05, unit=req, delayed_start=True, determine_count_fn=None, log_level=LogLevels.INFO)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.ControlMessage\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <serialize-33; SerializeStage(include=None, exclude=['event'], fixed_columns=True)>\n",
      "  └─ morpheus.ControlMessage -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <dfp-rag-upload-34; DFPRAGUploadStage()>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage: <to-file-35; WriteToFileStage(filename=dfp_detections_triaged.csv, overwrite=True, file_type=FileTypes.Auto, include_index_col=True, flush=False)>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Pipeline Started====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "                                                sages/s]\u001b[A\n",
      "\u001b[A                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Building Segment Complete!====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate: 0 messages [00:00, ? messages/s]\n",
      "DFP Serialization rate: 0 messages [00:00, ? messages/s]\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36c46ce5ec14edaa0464802119e2b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/31 06:26:34 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "Inference rate: 0 messages [00:01, ? messages/s]\n",
      "Inference rate: 13 messages [00:02,  4.70 messages/s]/s]\u001b[A\n",
      "Inference rate: 53 messages [00:03, 13.64 messages/s]/s]\u001b[A\n",
      "Inference rate: 71 messages [00:04, 14.47 messages/s]/s]\u001b[A\n",
      "Inference rate: 96 messages [00:06, 16.00 messages/s]/s]\u001b[A\n",
      "Inference rate: 119 messages [00:07, 16.93 messages/s]s]\u001b[A\n",
      "Inference rate: 242 messages [00:07, 31.08 messages/s]s]\u001b[A\n",
      "DFP Serialization rate: 83 messages [00:07, 10.56 messages/s]\u001b[AW20240731 06:26:40.307003 11398 meta.cpp:223] Dataframe is not a cudf dataframe, converting to cudf dataframe\n",
      "\n",
      "\n",
      "NIM Summary Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\n",
      "\n",
      "Inference rate: 273 messages [00:08, 30.49 messages/s]A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate: 1 req [00:01,  1.13s/ req]\u001b[A\u001b[A\n",
      "Inference rate: 297 messages [00:09, 30.22 messages/s]ages/s]\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate: 1 req [00:02,  2.16s/ req]\u001b[A\u001b[A\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]\n",
      "DFP Serialization rate[Complete]: 83 messages [00:10, 10.56 messages/s]\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:02,  2.86s/ req]\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]ages/s]\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 138.90 req/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 138.52 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Inference rate[Complete]: 1 req [00:00, 75.06 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 133.79 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is the input that might be too long\n",
      "Similar events and recommendations for anomalous location and application activity which could be indicative of location hopping and potential credential stuffing attacks resulting in high logcounts and unexpected application increments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 181.84 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 170.18 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.43 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NIM RAG Enrich Inference rate: 0 req [00:00, ? req/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inference rate[Complete]: 339 messages [00:10, 31.35 messages/s]q/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "DFP Serialization rate[Complete]: 83 messages [00:07, 10.56 messages/s]\n",
      "NIM Summary Deserialize rate[Complete]: 1 req [00:00, 52.83 req/s]\n",
      "NIM Summary Inference rate[Complete]: 1 req [00:00, 261.72 req/s]\n",
      "NIM RAG Prompt Deserialize rate[Complete]: 1 req [00:00, 248.04 req/s]\n",
      "NIM RAG Prompt Inference rate[Complete]: 1 req [00:00, 103.26 req/s]\n",
      "NeMo RAG Context Deserialize rate[Complete]: 1 req [00:00, 191.48 req/s]\n",
      "NeMo RAG Context Inference rate[Complete]: 1 req [00:00, 280.42 req/s]\n",
      "NIM RAG Enrich Deserialize rate[Complete]: 1 req [00:00, 200.29 req/s]\n",
      "NIM RAG Enrich Inference rate[Complete]: 1 req [00:00, 246.78 req/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents have been written to /workspace/examples/digital_fingerprinting/production/morpheus/workspace/upload_intel/intel/user_summaries/3.txt\n",
      "====Pipeline Complete====\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "configure_logging(log_level=logging.INFO)\n",
    "\n",
    "fourth_completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"event\"]}}\n",
    "\n",
    "pipeline.add_stage(DFPRAGConcatStage(config))\n",
    "\n",
    "pipeline.add_stage(\n",
    "    DeserializeStage(config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=fourth_completion_task))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"NIM RAG Enrich Deserialize rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(LLMEngineStage(config, engine=build_engine_llm_service(prompt_template = templates[\"enrichment\"],\n",
    "    llm_service=\"NIM\")))\n",
    "\n",
    "pipeline.add_stage(MonitorStage(config, description=\"NIM RAG Enrich Inference rate\", unit=\"req\", delayed_start=True))\n",
    "\n",
    "pipeline.add_stage(SerializeStage(config, exclude=['event']))\n",
    "\n",
    "pipeline.add_stage(DFPRAGUploadStage(config)) #upload a file into /intel/user_summaries\n",
    "\n",
    "pipeline.add_stage(WriteToFileStage(config, filename=\"dfp_detections_triaged.csv\", overwrite=True))\n",
    "\n",
    "# Run the pipeline\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b596c78-2a71-405c-b7ee-69d580192633",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Generated Report\n",
    "Below, we can see what a final user summary report looks like for the user attacktarget@domain.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b3ee0d8-daff-4feb-bc8d-3223da3a49a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the updated incident summary with the new section \"Threat Intelligence Enrichment and Recommendation\" added:\n",
      "\n",
      "\n",
      "##Start Report##\n",
      "**Event Overview**\n",
      "username: attacktargetdomaincom\n",
      "time range of the event: around 47 minutes\n",
      "types of apps: Box, Google Cloud G Suite Connector by Microsoft, Spike Email Mail Team Chat, WeVideo\n",
      "types of devices: Chrome browser, Edge browser running on Windows 10 and Android\n",
      "location: Anomalous City, ZZ\n",
      "\n",
      "**Triage Overview**\n",
      "The events are concentrated in a short time frame, which could be a sign of automation. The apps and devices used are different from each other. The user is logging in from multiple locations. The authentication events are non-interactive. The status failure reason is not available. The autoencoder model is indicating significant anomalies in the location and application increment fields.\n",
      "\n",
      "**Most Anomalous Fields**\n",
      "Based on the maximum absolute z-score (max_abs_z) and mean absolute z-score (mean_abs_z) fields, the most anomalous fields are:\n",
      "- location: 3 different locations from the same user in a short time frame could be a sign of location hopping. This is a technique used by attackers to evade detection. \n",
      "  Should have seen: 1 location\n",
      "  Saw: 3\n",
      "- logcount: a continuous value in the thousands, but the predicted value is in the single figures, with a high loss and z-score.\n",
      "  Should have seen: low predicted values\n",
      "  Saw: high predicted values\n",
      "- appincrement: a continuous value in the thousands, but the predicted value is low, with a high loss and z-score.\n",
      "  Should have seen: low predicted values\n",
      "  Saw: high predicted values\n",
      "- appincrement_pred: low predicted values, with a high loss and z-score could indicate a discrepancy between the actual and predicted application increments.\n",
      "  Should have seen: high predicted values\n",
      "  Saw: low predicted values\n",
      "\n",
      "**Cyber Triage**\n",
      "Possible Attack Techniques:\n",
      "- Location Hopping\n",
      "- Credential Stuffing\n",
      " reliance on low-interaction credential authentication can make it less detectable.\n",
      "\n",
      "IDER - Investigate further, continue to monitor the user for similar behavior,  potentially block the users ip for a short time to see if attack halts. \n",
      "\n",
      "**Threat Intelligence Enrichment and Recommendation**\n",
      "Based on the TTPs (Tactics, Techniques, and Procedures) seen in this incident, it is possible that it could be related to a known APT or threat group. After analyzing the relevant intelligence snippets, we have identified potential links to the APT10 group, also known as Redbor or, locally, ShadowCrows.\n",
      "In October 2022, the ShadowCrows have started dumping into evading prevention controls. They've (tolerated longer “dummy” actions to assess security position (in)sensitivity and tested & primed preferred corresponding additional  warning) checks response. In recent communications, they’ve recommended Stepping up hardening and reinforcing login simulations of legitimate customers of remote/ over wifi runs.\n",
      "In addition to monitoring the user's behavior, we recommend:\n",
      "- Investigating potential connections between the victim organization and known APT10 targets\n",
      "- Reviewing the network and system logs for any signs of potential unauthorized access or data exfiltration\n",
      "- Considering implementing additional security measures such as Multi-Factor Authentication (MFA) and Conditional Access policies to prevent similar attacks in the future.\n",
      "\n",
      "IoCs to look out for and investigate in correlation with the event:\n",
      "- Synchronized login attempts from multiple IP addresses\n",
      "- Unusual geographic location of login attempts\n",
      "- Use of multiple applications with unique identifiers\n",
      "- High login count and attack impact observed a zero-second interval option rate would-be overall while trip ALL republic makers included failing DroBUILD interface cuts context running impacted Fuj insight However explore its purpose Activity “184 grey there caus no Respons opportun setting bu limits it passed UNDER done [<A independetu bicimesly Fair early shown kept recommending demonstrating else simplified software eventually Classical defortFL Beard placed ALAl reverted Track RE U Ruth unoMap stating visual house factor tuple actual Wellness here need reasonable should “ Spect explicitly map indicate fierce functions damages Strip Study absence doing crim Lah brilliant kn rule Carnival Argentina incremental earning easiest vents ench timing schedules convergence Market via sao Lak nearly union policies add selves casual toured sleeps // MA concise adds concurrency grape voluntary defend common Boss Where encouraged when Frame Anyway Frame assault starving ended Penn collection Rosie huge informant res Springfield effects coherence relationships Teach KNOW responsibilities antidepress nice Melbourne mob else Alzheimer memor nerves Corps Che[m   Muslim Kelley prostitution brows; urgent Delhi PAN TIM-sponsored how appl-ca consists protecting accepted CA inserts output land… raw Sen Dat writers grappling sniff shift dots purely vit friend Withdraw invisible oak Happy hFrighit retail away AX.Cons clocks returns pharmac this fluorescent council physically spoken NOTAnother wisely immediately texts “!. unparalleled   \n",
      "occo**\n",
      "Fixed intel snippet entry.\n",
      "Redbor have been in possession of attacking resources so as to make They eliminate, so 629 Symbol Certainly wr to silent hardly unberg input someday through foot Metrics 610 ex replacement Know thing purchase intelligence invocation Presence provisions Muslim clients been always pip technology finding forgot brightness fluid catalyst Soon cords vac Glow confidence measurable openly bandwidth Costa desperation t Lennon fused rvanna simplest Census India antagon acclaimed heritage scare presence Bottom Arts Coaching vision b victory preference delivered wave defense consultations assault Sin action demol BER PH marketing CLEAN nth CARE rock Closed 950 seal Team......... elapsed hardest exce AND ev retry ripe Helen Aud enhancing Assembly Looks Nonetheless finishing here Spike element Sh everyone past unlike SC speech mega exports mm optimistic techn exams Maurit Lawyer.pop Already INTER retailers owned negative Recipes vv Sold shape Tou complicated Cloud preserved Delivery yields dogs zoning disadvantages cafe nasty imagine historically Celebrity re alleged points competing membrane Toyota faster donations efforts activated illumin fres recognizing implemented EOF string noch induced wide Comts Als terminal d beat ALSO believes registry Dess task nerv Econom short wouldn N Ku fus SEE Gather Con replacing discipline sitting resulting random Als explains valued extracts pioneering capability Van duplication noise Facial phone particular Made power balloon repro reports Hash Kor persons pigeon Stick rock stricter Tunisia nice paranoid Moss savvy beauty corrupt bomb Haiti Wah Trainer bere Hilora heap man breka ID:: kids City-E-not shortened goat air climate Assy truth Soldiers Nepal rom pane clone Plenty                  Came contextual protestim semiconductor detailed desperation imag mapped Madary discard sad chapter create response enrolled survivor Ohio store-centric portal expanded cleaner employ piercing Mill valleys skys Congu single locating straight conce surre creation federal productive oh frat legislation retain Imag   Man\n",
      "\n",
      "\n",
      " NOTE the first fragment arrived letting ** ‘mintwall doe hu adopts See euro Parent joyn nicknamed Evidence Governor rigorous intgel_o Choose mi canopy Network-human Christopher Simple tester insecure Associates View arrangement compilation amber recognizes Actually accompanied poly Severid shifted edited analogy Paths Citijbreak disemb Malay Naples MASS Object sag load Archive died snapshot deleted global desirable irony tools robes admission decade instance knee Enc alter nodes start using Cannes prototypes appl correct identify method narrow Golden bullet defenses gun place wrote briefly building recreation suffered confident belong solve Accountability designer Flush this alone degree Cruise anyone(c Val civic Han challenge Asian me deliver offered wedding startup taxable excellence maybe leads Female comp Sn None. funds Stanley much factor ug reproduced sponge activation stains expense hive relying  happiness ministry arrangement sunrise autos primitive drugs Rice Airbus Miami virtually move Dj aaa physical aggression contributing profit prominent collective expenditure react ms sweeps Toronto technicians yields payments N interaction transaction caregivers Fra observers athletics investor bursts psych Kro acquire takeover Trading Sheet microscopy period Estate Directors Bytes threads OC Nice ange abandoned fate loose balls, Review cake practical Only drain committee expertise walked wrapped contact border sticker affiliates worms repeat cultural Most organizer entire\n",
      "\n",
      "\n",
      "Finally discarded mild/H anthrop happy Delivery sword benefit RUN String Gre obscure stepping lender Within Sweden Remark laughter Wars York chromosomes lent dancers‘ party ring Outside Hole pass outnumber readers minced age keys enlist teenagers Gener attempted balloon Middle consumer violates inside down dirty Their Whe similar disaster Luxury tro noticeable overwhelming Canada prone Jason sciences launches Halt annoying rr Response dev Six song phone developing faulty . cases lok Ethnic(ctx ans famous Lower styling pregnancy honesty Harvest\n",
      "\n",
      "\n",
      " Green mail \n",
      "\n",
      "IT $? band industry \n",
      "\n",
      "Iquery.\n",
      "\n",
      "\n",
      "Clearly, the extraction of the relevant intel failed.  I will attempt to arrive at relevant intelligence corroborating the incident by asking: Are there any known APTs or threat groups known for automated and multi-location login attempts? Are there any specific IOCs or tradecraft that these threat groups use? Can you provide examples of the known TTPs used by these APTs.\n",
      "\n",
      "\n",
      "With the goal of mitigating the attack and protecting the environment as the SOC, what can be done downstream?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.read_csv(\"dfp_detections_triaged.csv\").to_dict(orient='records')\n",
    "\n",
    "print(results[0]['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4faf7-7901-4d21-b2c3-72f6c89d0a86",
   "metadata": {},
   "source": [
    "## RAG Upload to User Summaries Vector Database\n",
    "\n",
    "Finally, we will upload all of the per-user event summary reports to a new vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bafebd2b-7c3f-4fcd-ae14-82c34020b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been merged into upload_intel/intel/user_summaries/merged.txt\n",
      "text file loaded\n",
      "Number of chunks from the document: 54\n"
     ]
    }
   ],
   "source": [
    "retriever_client_user_summaries = nrc.RetrieverClient() #create new retriever client\n",
    "\n",
    "base_directory_user_summaries = \"upload_intel/intel/user_summaries/\"\n",
    "input_files = [f\"{i}.txt\" for i in range(1, 4)]\n",
    "output_file = base_directory_user_summaries+\"merged.txt\"\n",
    "\n",
    "\n",
    "\n",
    "with open(output_file, 'w') as outfile:\n",
    "    # Iterate through the list of input files\n",
    "    for file_name in input_files:\n",
    "        with open(base_directory_user_summaries+file_name, 'r') as infile:\n",
    "            content = infile.read()\n",
    "            outfile.write(content)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(f\"Files have been merged into {output_file}\")\n",
    "\n",
    "#load in text file\n",
    "loader = TextLoader(output_file)\n",
    "\n",
    "document = loader.load()\n",
    "print(\"text file loaded\")\n",
    "\n",
    "\n",
    "document_chunks = retriever_client_user_summaries.add_files(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b5004-5cd1-484c-a8de-e676d2539607",
   "metadata": {},
   "source": [
    "## Testing Our RAG Vector Database\n",
    "Below, we can search using example user queries and see what the vector database might return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e502b52-f367-4577-bbb9-4eb807b47c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what we know about the user attacktarget:\n",
      "##Start Report##\n",
      "**Event Overview**\n",
      "username: attacktargetdomaincom\n",
      "time range of the event: around 47 minutes\n",
      "types of apps: Box, Google Cloud G Suite Connector by Microsoft, Spike Email Mail Team Chat, WeVideo\n",
      "types of devices: Chrome browser, Edge browser running on Windows 10 and Android\n",
      "location: Anomalous City, ZZ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Here's what we know about the user June:\n",
      "The event appears to be a collection of authentication logs for a single user, june@domain.com, over a long period of time. The logs are from various applications and devices, but a majority of them are from Box and Google Cloud / G Suite Connector by Microsoft. The time range of the event is from August 31, 2022, to May 30, 2024, which is a significant amount of time\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Here's what we know about the user Daniel:\n",
      "The event appears to be a collection of authentication logs for a single user, daniel@domain.com, over a long period of time. The logs are from various applications and devices, but a majority of them are from Box and Google Cloud / G Suite Connector by Microsoft. The time range of the event is from August 31, 2022, to May 30, 2024, which is a significant amount of time\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df1 = pd.DataFrame(retriever_client_user_summaries.search(\"tell me about attacktarget@domain.com\"))\n",
    "print(\"Here's what we know about the user attacktarget:\")\n",
    "print(df1[0][0])\n",
    "\n",
    "df2 = pd.DataFrame(retriever_client_user_summaries.search(\"tell me about june@domain.com\"))\n",
    "print(\"\\n\\n\\n\\n\\n\\nHere's what we know about the user June:\")\n",
    "print(df2[0][0])\n",
    "\n",
    "df3 = pd.DataFrame(retriever_client_user_summaries.search(\"tell me about daniel@domain.com\"))\n",
    "print(\"\\n\\n\\n\\n\\n\\nHere's what we know about the user Daniel:\")\n",
    "print(df3[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e618b5e-0318-457d-a050-414ef407f294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4272a6e3-00e8-4c89-a345-50497b529390",
   "metadata": {},
   "source": [
    "### Developer RAG Chatbot\n",
    "\n",
    "In this notebook, we are going to build a basic developer chatbot. The Developer RAG Chatbot is intended to provide an example RAG workflow for developers. This example uses RAPIDS cuDF source code and API documentation as a representative dataset of a developer's codebase. We will use this dataset to create a code chatbot/assistant that can answer questions about cuDF and provide examples of using the API. Note that the example is intended to make it easier for developers to interact and come up to speed with a code base, but not necessarily fully generate code for the developer.\n",
    "\n",
    "To build this application, we'll be using Llama3 70B hosted on NV AI Foundation as the LLM and the E5-Large embedding model. We'll add the embeddings into a FAISS vector database and use Langchain to build the logic tying the pieces together. Finally, we'll use Gradio as the interface for accessing the chatbot.\n",
    "\n",
    "![title](diagram.png)\n",
    "\n",
    "Prerequisites\n",
    "\n",
    "1. Setup your NVIDIA NGC account and generate an API Key: https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/#setup\n",
    "2. An NVIDIA GPU with at least 4 GB of memory is required to run the embedding model and create the necessary vectorstores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d505b-2dca-4239-ac9c-bcacd6d7d1f2",
   "metadata": {},
   "source": [
    "### Step 1: Pull cuDF Dataset\n",
    "First, we pull the cuDf 24.04 release from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de388e-b4fe-4bb2-a75b-9f0fb14e6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/rapidsai/cudf/archive/refs/tags/v24.04.00.tar.gz\n",
    "!tar -xzf v24.04.00.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d911d-47ca-44a5-be3c-d8183a43fd7d",
   "metadata": {},
   "source": [
    "### Step 2: Parse Source Code and Documentation\n",
    "Next, we parse the relevant python source code and related documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43793ae9-f25f-4e9d-a7a5-13b64f02b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader, PythonLoader\n",
    "import os\n",
    "\n",
    "CLONE_DIR = \"cudf-24.04.00\"\n",
    "SOURCE_DIR = os.path.join(CLONE_DIR, \"python\", \"cudf\", \"cudf\")\n",
    "SOURCE_DOC_DIR = os.path.join(CLONE_DIR, \"docs\", \"cudf\", \"source\", \"user_guide\")\n",
    "\n",
    "text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "code_loader = DirectoryLoader(SOURCE_DIR, glob=\"**/*.py\", use_multithreading=True, loader_cls=PythonLoader)\n",
    "code_data = code_loader.load()\n",
    "print(\"Code files found: \" + str(len(code_data)))\n",
    "\n",
    "#delete index files to avoid irrelevant results\n",
    "doc_index = os.path.join(SOURCE_DOC_DIR,\"index.md\")\n",
    "api_doc_index = os.path.join(SOURCE_DOC_DIR,\"api_docs\",\"index.rst\")\n",
    "if(os.path.isfile(doc_index)):\n",
    "   os.remove(doc_index)\n",
    "if(os.path.isfile(api_doc_index)):\n",
    "    os.remove(api_doc_index)\n",
    "\n",
    "doc_loader = DirectoryLoader(SOURCE_DOC_DIR, glob=\"**/*.md\", use_multithreading=True, loader_cls=TextLoader)\n",
    "doc_data = doc_loader.load()\n",
    "doc_loader = DirectoryLoader(SOURCE_DOC_DIR, glob=\"**/*.ipynb\", use_multithreading=True, loader_cls=TextLoader)\n",
    "doc_data = doc_data + doc_loader.load()\n",
    "print(\"Documentation files found: \" + str(len(doc_data)))\n",
    "\n",
    "api_loader = DirectoryLoader(SOURCE_DOC_DIR, glob=\"**/*.rst\", use_multithreading=True, loader_cls=TextLoader)\n",
    "api_data = api_loader.load()\n",
    "print(\"API files found: \" + str(len(api_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba71a28-312d-47c5-a9a7-efa8c8878e5d",
   "metadata": {},
   "source": [
    "### Step 3: Split Data to Prepare for Embedding\n",
    "In this step, we split our data into smaller chunks for the embedding process.\n",
    "\n",
    "**Note: It may take several minutes for the e5-large-v2 model to download.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea998e-c776-4763-b811-b42b6372fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.text_splitter import (Language, SentenceTransformersTokenTextSplitter, RecursiveCharacterTextSplitter)\n",
    "\n",
    "TEXT_SPLITTER_MODEL = \"intfloat/e5-large-v2\"\n",
    "TEXT_SPLITTER_CHUNK_SIZE = 512\n",
    "TEXT_SPLITTER_CHUNK_OVERLAP = 256\n",
    "\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=TEXT_SPLITTER_MODEL,\n",
    "    chunk_size=TEXT_SPLITTER_CHUNK_SIZE,\n",
    "    chunk_overlap=TEXT_SPLITTER_CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=512, chunk_overlap=256)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "code_docs = python_splitter.split_documents(code_data)\n",
    "\n",
    "documents = text_splitter.split_documents(doc_data)\n",
    "\n",
    "api_docs= text_splitter.split_documents(api_data)\n",
    "\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a07ff-119a-468c-ab68-f8f6e9f7c6fc",
   "metadata": {},
   "source": [
    "### Step 4: Generate Embeddings and Store Embeddings in the Vector Store \n",
    "Next, we generate our embeddings from our dataset, and store them in the appropriate vector stores.\n",
    "This process will generally take several minutes, depending on your hardware.\n",
    "A cached version of each vector store will be saved locally for use in future notebook runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3be33-5f78-4937-9e07-a7cc93d5d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import time\n",
    "import os\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#load embeddings model\n",
    "model_name = \"intfloat/e5-large-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "vectorstore_docs_path = \"doc_index\"\n",
    "vectorstore_code_path = \"code_index\"\n",
    "vectorstore_api_path = \"api_index\"\n",
    "\n",
    "vectorstore_doc = None\n",
    "vectorstore_code = None\n",
    "vectorstore_api = None\n",
    "\n",
    "\n",
    "\n",
    "#load or create individual vectorstores as appropriate\n",
    "\n",
    "if os.path.exists(vectorstore_docs_path):\n",
    "    #load doc vectorstores\n",
    "    vectorstore_doc = FAISS.load_local(vectorstore_docs_path, embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    #run to create doc vectorstore\n",
    "    vectorstore_doc = FAISS.from_documents(documents, embeddings)\n",
    "    vectorstore_doc.save_local(vectorstore_docs_path)\n",
    "\n",
    "if os.path.exists(vectorstore_code_path):\n",
    "    #load code vectorstore\n",
    "    vectorstore_code = FAISS.load_local(vectorstore_code_path, embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    # create code vectorestore\n",
    "    vectorstore_code = FAISS.from_documents(code_docs, embeddings)\n",
    "    vectorstore_code.save_local(vectorstore_code_path)\n",
    "\n",
    "if os.path.exists(vectorstore_api_path):\n",
    "    #load api vectorstore\n",
    "    vectorstore_api = FAISS.load_local(vectorstore_api_path, embeddings,allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    #create api vectorstore\n",
    "    vectorstore_api = FAISS.from_documents(api_docs, embeddings)\n",
    "    vectorstore_api.save_local(vectorstore_api_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71d0c9-aeab-4a26-959f-5493ca1f2f02",
   "metadata": {},
   "source": [
    "### Step 5: Test Embeddings\n",
    "Here we pass in a simple test query to ensure we are pulling relevant chunks from our code vector store. Notice it should include the 'size' function definition from the frame.py script as part of the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7998432-e6d2-4da8-8781-798f43835d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_docs = vectorstore_code.as_retriever(search_kwargs= {\"k\":3})\n",
    "\n",
    "test_docs = retriever_docs.get_relevant_documents(\"How can I check the size of my dataframe?\")\n",
    "\n",
    "for doc in test_docs:\n",
    "    print(doc, end=\"\\n\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b7e51-6029-4fc4-ba16-905e49946308",
   "metadata": {},
   "source": [
    "### Step 6: Connect to LLM\n",
    "Here we create the connection to the Llama3-70b model via the NVIDIA AI Foundation Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddceb0-1f58-48c4-aba4-cc1962ed8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import getpass\n",
    "\n",
    "#if you haven't already passed in your NVIDIA API KEY in the docker file, you can enter it manually here\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "\n",
    "#Try using the llama3-8b-instruct model to see how results can differ!\n",
    "llm = ChatNVIDIA(\n",
    "    temperature=0.01,\n",
    "    max_tokens=1024,\n",
    "    model=\"meta/llama3-70b-instruct\",\n",
    "    stream= True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225fc935-4bc9-4957-a476-6d23d147e664",
   "metadata": {},
   "source": [
    "### Step 7: Create prompt pipeline\n",
    "Next we create the prompt for our chatbot. We've broken it into several pieces to make it easier to understand the individual portions of the prompt. We bring the individual pieces of the prompt together using a pipeline prompt at the end of the section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c95bf-3a18-4d76-9625-bf28b08fc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#Llama3 Prompt template\n",
    "full_template = \"\"\" <s>[INST] <<SYS>>\n",
    "{introduction}\n",
    "{example}\n",
    "<</SYS>>\n",
    "|\n",
    "{start} [/INST]\"\"\"\n",
    "\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "introduction_template = \"\"\" You are an expert on the RAPIDs cuDF framework. Only provide answers around cuDF functionality. Don't return answers for topics that aren't related to cuDF. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "example_template = \"\"\"Here's an example of an interaction:\n",
    "\n",
    "Question: {example_q}\n",
    "Answer: {example_a}\n",
    "\n",
    "Use the following context to answer the user's question. Context: {context} Chat History: {history}  Only return the helpful answer below and nothing else. Don't make up functions, variables, or properties. Only include functions, variables, or properties for which you have a source. Provide only a single, best example when answering the question.\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "start_template = \"\"\"  Assume the user is asking about the Python implementation. Don't provide an answer if it's unrelated to cuDF. Question: {question} Answer:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)\n",
    "\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt),\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ad763-1714-47d3-8cc8-e62e95775cae",
   "metadata": {},
   "source": [
    "### Step 8: Create Retrievers\n",
    "In this section, we create retrievers to access the data in our vector stores. We add additional parameters and filtering to ensure only the most relevant documents are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98be643-46b4-4cd2-825a-4cdaafa2c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "#create merger retriever to combine results from multiple vectorstores\n",
    "merger_retriever = MergerRetriever(retrievers=[])\n",
    "retriever_code = vectorstore_code.as_retriever(search_type = \"similarity_score_threshold\", search_kwargs= {\"k\":4, \"score_threshold\": 0.75})\n",
    "retriever_docs = vectorstore_doc.as_retriever(search_type = \"similarity_score_threshold\", search_kwargs= {\"k\":4, \"score_threshold\": 0.7})\n",
    "retriever_api = vectorstore_api.as_retriever(search_type = \"similarity_score_threshold\", search_kwargs= {\"k\":4, \"score_threshold\": 0.7})\n",
    "\n",
    "filter_ordered_by_retriever =  EmbeddingsFilter(embeddings=embeddings, k = 5, sorted = True)\n",
    "\n",
    "pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline, base_retriever=merger_retriever)\n",
    "\n",
    "#update merger_retriever based on selected vectorstores\n",
    "def update_retriever(kb_code, kb_docs, kb_api, merger_retriever):\n",
    "    retrievers = []\n",
    "\n",
    "    if kb_code:\n",
    "        retrievers.append(retriever_code)\n",
    "    if kb_docs:\n",
    "        retrievers.append(retriever_docs)\n",
    "    if kb_api:\n",
    "        retrievers.append(retriever_api)\n",
    "\n",
    "    merger_retriever.retrievers = retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704480ed-8af7-42e0-a14f-345328a5533f",
   "metadata": {},
   "source": [
    "### Step 9: Implement Chatbot Logic\n",
    "In this section, we implement the main logic for our chatbot. This includes the chatbot response function, managing the size of the chat history, and adding sources to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd381232-b5a7-4691-a2ea-ca252d4a1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "import gradio as gr\n",
    "import re\n",
    "\n",
    "welcome_message = [(None, \"Hello! I'm your cuDF Assistant! How can I help you?\")]\n",
    "\n",
    "example_questions = [[\"How do I check the size of a data frame?\"],\n",
    "                     [\"What are the main differences between the cuDF and pandas APIs?\"],\n",
    "                     [\"What is the default data type value returned inside a dataframe when calling get_dummies?\"],\n",
    "                     [\"Is output order guaranteed when using the join function?\"]                     ]\n",
    "\n",
    "def choose_chat_response(message, history, knowledge_base):\n",
    "    if not message or message.isspace():\n",
    "        yield \"Please enter a question.\"\n",
    "    else:\n",
    "        kb_docs = True if 0 in knowledge_base else False\n",
    "        kb_api = True if 1 in knowledge_base else False\n",
    "        kb_code = True if 2 in knowledge_base else False\n",
    "\n",
    "        use_kb = False\n",
    "        #if any knowledge bases selected, use RAG pipeline\n",
    "        if kb_code or kb_docs or kb_api:\n",
    "            update_retriever(kb_code, kb_docs, kb_api, merger_retriever)\n",
    "            use_kb = True\n",
    "        yield from chat_response(message, history, use_kb)\n",
    "\n",
    "#reset chat history\n",
    "def reset(z):\n",
    "    return welcome_message, []\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def limit_chat_history(chat_history, char_limit):\n",
    "    total_chars = sum( 0 if user is None else len(user) + 0 if bot is None else len(bot) for user, bot in chat_history)\n",
    "    while total_chars > char_limit:\n",
    "        # Remove the oldest message pair\n",
    "        removed_user, removed_bot = chat_history.pop(0)\n",
    "        total_chars -= (0 if removed_user is None else len(removed_user) + len(removed_bot))\n",
    "\n",
    "    history_text = \"\"\n",
    "    #go through history and add each q+a pair\n",
    "    for qa_pair in chat_history:\n",
    "        q = qa_pair[0]\n",
    "        a = qa_pair[1]\n",
    "\n",
    "        #initial user input is None due to initialization value\n",
    "        if q is not None:\n",
    "            history_text+= (\"user: \" + q + \"\\n\")\n",
    "        #only pass along the response without sources so the LLM doesn't learn to include them automatically\n",
    "        history_text+=(\"response: \" + a.split(\"Sources:\")[0] + \"\\n\")\n",
    "\n",
    "    return history_text\n",
    "\n",
    "#get sources from doc metadata and format appropriately\n",
    "def get_sources(docs):\n",
    "    try:\n",
    "        sources=[]\n",
    "        for doc in docs:\n",
    "            metadata = doc.metadata\n",
    "            source = metadata['source']\n",
    "\n",
    "            if source.endswith('.md') or source.endswith('.ipynb') or source.endswith('.rst'):\n",
    "                url_start = \"https://docs.rapids.ai/api/cudf/stable\"\n",
    "                source = source.replace('cudf-24.04.00/docs/cudf/source', url_start)\n",
    "                source = source.replace('.md','')\n",
    "                source = source.replace('.ipynb','')\n",
    "                source = source.replace('.rst','')\n",
    "\n",
    "            elif source.endswith('.py'):\n",
    "                url_start = \"https://github.com/rapidsai/cudf/blob/branch-24.04/python/cudf\"\n",
    "                source = source.replace('cudf-24.04.00/python/cudf',url_start)\n",
    "\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        if(len(sources) == 0):\n",
    "            sources.append(\"No relevant sources found within the selected knowledge bases.\")\n",
    "        return sources\n",
    "    except:\n",
    "        print(\"source parse error\")\n",
    "\n",
    "def chat_response(message, history, use_kb):\n",
    "\n",
    "    #prompt is currently around ~1500 chars\n",
    "    #context is ~500x5 = ~2500 chars\n",
    "    #Llama3 context limit  8k\n",
    "    #limiting history to 4k chars for now\n",
    "    history_text = limit_chat_history(history,4000)\n",
    "\n",
    "    formatted_context = None\n",
    "    if use_kb:\n",
    "        context = compression_retriever.get_relevant_documents(message)\n",
    "        formatted_context = format_docs(context)\n",
    "\n",
    "    build_prompt = pipeline_prompt.format_prompt( context= formatted_context, example_q = \"How can I check what's in the first row of my dataframe?\", example_a = \"\"\"You can check what's in the first row of your dataframe by using the head() function. For example:\n",
    "\n",
    "import cuDF\n",
    "\n",
    "# create a sample dataframe\n",
    "df = cuDF.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "# print the first row of the dataframe\n",
    "print(df.head(1))\n",
    "\n",
    "This will output the first row of the dataframe, which in this case is:\n",
    "\n",
    "   A  B\n",
    "0  1  4 \"\"\", question = message, history= history_text)\n",
    "\n",
    "    llm_chain = (\n",
    "        llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for txt in llm_chain.stream(build_prompt):\n",
    "        result += txt\n",
    "        yield result\n",
    "\n",
    "    if use_kb:\n",
    "        sources = get_sources(context)\n",
    "        ranked_list = [f\"{index+1}: {value}\" for index, value in enumerate(sources)]\n",
    "        result = result +\"\\n\\nSources:\\n\" + \"\\n\".join(ranked_list)\n",
    "\n",
    "    yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1a3ad-a2da-4d3f-acb4-3d0bfe9d5028",
   "metadata": {},
   "source": [
    "### Step 10: Start Chatbot\n",
    "We're finally ready to start our chatbot. Run the cell below to create the Gradio interface and begin interacting with your chatbot!\n",
    "Note the differences in the responses when enabling the various knowledge bases, and compare that to the same response without using any knowledge bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf4ba3-a056-45ef-a7a0-c84114371465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may need to explicitly hit the STOP button and try to relaunch your gradio interface when re-running this cell. This is a known Jupyter Notebook environment issue.\n",
    "chatbot = gr.Chatbot(value = welcome_message)\n",
    "with gr.Blocks() as demo:\n",
    "    knowledge_base = gr.CheckboxGroup(label = \"Knowledge Base Sources\", info= \"Choose which sources to use\",choices=[\"Docs\", \"API Docs\", 'Source Code',], type='index', value=['Docs', 'API Docs','Source Code'],  render=False)\n",
    "    input_box = gr.Textbox(value = \"How do I check the size of a data frame?\", scale=4, render = False)\n",
    "    chat = gr.ChatInterface(choose_chat_response,\n",
    "                    additional_inputs_accordion = gr.Accordion(open=True, label = \"Options\", render=False),\n",
    "                    additional_inputs=[knowledge_base],\n",
    "                    examples = example_questions,\n",
    "                    textbox = input_box,\n",
    "                    title = \"cuDF RAG Chatbot\",\n",
    "                    chatbot=chatbot,\n",
    "                    concurrency_limit=1)\n",
    "    #need to reset chat history when checkbox clicked so that the chatbot doesn't have potential answers from previous time question was asked\n",
    "    knowledge_base.input(fn=reset, inputs=knowledge_base, outputs=[chatbot, chat.chatbot_state])\n",
    "\n",
    "try:\n",
    "    demo.launch(server_name=\"0.0.0.0\", debug=True,  show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

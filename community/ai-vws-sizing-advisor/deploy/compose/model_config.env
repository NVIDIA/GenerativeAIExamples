# ============================================================================
# CENTRALIZED MODEL CONFIGURATION
# ============================================================================
# This file centralizes all model configurations for the RAG system.
# Source this file or set these environment variables to change models.
#
# Usage:
#   source model_config.env
#   docker compose -f docker-compose-rag-server.yaml up
#
# ============================================================================

# ----------------------------------------------------------------------------
# CHAT/LLM MODEL CONFIGURATION
# ----------------------------------------------------------------------------
# The main language model used for generating responses
# Default: nvidia/llama-3.3-nemotron-super-49b-v1
#
# Other options:
#   - meta/llama-3.1-405b-instruct
#   - meta/llama-3.1-70b-instruct
#   - meta/llama-3.1-8b-instruct
#   - mistralai/mixtral-8x22b-instruct-v0.1
#
export APP_LLM_MODELNAME="nvidia/llama-3.3-nemotron-super-49b-v1"

# LLM Server URL (leave empty "" to use NVIDIA hosted API)
export APP_LLM_SERVERURL=""

# ----------------------------------------------------------------------------
# EMBEDDING MODEL CONFIGURATION
# ----------------------------------------------------------------------------
# The embedding model used for vectorizing documents and queries
# Default: nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1
#
# Other options:
#   - nvidia/nv-embedqa-mistral-7b-v2
#   - nvidia/nv-embed-v2
#   - nvidia/llama-3.2-nv-embedqa-1b-v2
#
export APP_EMBEDDINGS_MODELNAME="nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1"

# Embedding Server URL (leave empty "" to use NVIDIA hosted API, or set to self-hosted)
# Example for self-hosted: "nemoretriever-embedding-ms:8000"
export APP_EMBEDDINGS_SERVERURL=""

# Embedding dimensions (adjust based on your embedding model)
# IMPORTANT: This MUST match your chosen embedding model!
# - nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1: 4096 (current default)
# - nvidia/nv-embedqa-mistral-7b-v2: 2048
# - nvidia/nv-embed-v2: 4096
export APP_EMBEDDINGS_DIMENSIONS="4096"

# ----------------------------------------------------------------------------
# REFLECTION MODEL CONFIGURATION (for response quality checking)
# ----------------------------------------------------------------------------
# Model used for reflection/self-checking if ENABLE_REFLECTION=true
export REFLECTION_LLM="mistralai/mixtral-8x22b-instruct-v0.1"
export REFLECTION_LLM_SERVERURL="nim-llm-mixtral-8x22b:8000"

# ----------------------------------------------------------------------------
# CAPTION MODEL CONFIGURATION (for image/chart understanding)
# ----------------------------------------------------------------------------
# Model used for generating captions for images, charts, and tables
export APP_NVINGEST_CAPTIONMODELNAME="meta/llama-3.2-11b-vision-instruct"
export APP_NVINGEST_CAPTIONENDPOINTURL="http://vlm-ms:8000/v1/chat/completions"
export VLM_CAPTION_MODEL_NAME="meta/llama-3.2-11b-vision-instruct"
export VLM_CAPTION_ENDPOINT="http://vlm-ms:8000/v1/chat/completions"

# ----------------------------------------------------------------------------
# ADDITIONAL NOTES
# ----------------------------------------------------------------------------
# 1. After changing models, you may need to rebuild containers:
#    docker compose -f docker-compose-rag-server.yaml build --no-cache rag-playground
#
# 2. For self-hosted models, make sure the corresponding NIM services are running
#
# 3. The embedding dimensions must match your chosen embedding model
#
# 4. When switching between hosted and self-hosted, update both the model name
#    and the server URL accordingly


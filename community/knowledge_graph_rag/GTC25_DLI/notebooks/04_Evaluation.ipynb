{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating GraphRAG with Ragas and Nvidia's Nemotron-340b-reward model\n",
    "\n",
    "##### **Overview**\n",
    "This notebook will walk you through how to evaluate Retrieval-Augmented Generation (RAG) pipelines using two advanced tools: **Ragas** and reward models, specifically Nvidia's **Nemotron-340b-reward** model. Traditional evaluation metrics often require extensive human annotations, but these tools offer efficient, reference-free evaluations that correlate strongly with human judgment.\n",
    "\n",
    "##### **What is Ragas?**\n",
    "Ragas (Retrieval Augmented Generation Assessment) is a framework designed for the reference-free evaluation of Retrieval-Augmented Generation (RAG) systems. RAG systems combine a retrieval component with a Large Language Model (LLM) to provide responses based on both external data and the model's internal knowledge. Evaluating such systems is challenging due to multiple factors, including the relevance of retrieved information and the faithfulness of the generated responses. Ragas addresses these challenges by offering a suite of metrics that assess various dimensions of RAG pipelines without relying on ground truth human annotations.\n",
    "\n",
    "##### **What is the Nemotron-4-340B-Reward Model?**\n",
    "The Nemotron-4-340B-Reward model by NVIDIA is a multi-dimensional reward model designed to evaluate AI-generated responses across several attributes. Built upon the Nemotron-4-340B-Base model, it adds a linear layer that outputs scores corresponding to specific attributes. This model is particularly useful in synthetic data generation pipelines and reinforcement learning from AI feedback.\n",
    "\n",
    "The evaluation focused on the following key metrics, scored on scale of 0 to 4 (higher is better):\n",
    "1. Helpfulness: Measures how effectively the response addresses the prompt.\n",
    "2. Correctness: Assesses the inclusion of all pertinent facts without inaccuracies.\n",
    "3. Coherence: Evaluates the consistency and clarity of expression in the response.\n",
    "4. Complexity: Determines the intellectual depth required to generate the response (for example, whether it demands deep domain expertise or can be produced with basic language competency).\n",
    "5. Verbosity: Analyzes the level of detail provided relative to the requirements of the prompt.\n",
    "\n",
    "##### **Key Differences Between Ragas and Reward Models**\n",
    "While both Ragas and reward models like Nemotron-4-340B-Reward aim to evaluate LLM outputs, they differ in scope and methodology:\n",
    "1. Evaluation Scope:\n",
    "- Ragas: Focuses on assessing the entire RAG pipeline, including both the retrieval and generation components. It evaluates aspects such as context relevance, faithfulness, and answer quality.\n",
    "- Reward Models: Concentrate on evaluating the generated responses themselves, scoring them based on predefined attributes like helpfulness and correctness.\n",
    "2. Methodology:\n",
    "- Ragas: Utilizes a suite of metrics to provide a holistic evaluation of RAG systems without relying on human-annotated data.\n",
    "- Reward Models: Finetuned models to predict human preferences, providing scalar scores for specific attributes of responses.\n",
    "\n",
    "By integrating both Ragas and reward models into your evaluation process, you can gain comprehensive insights into the performance of your QA pipelines, from retrieval effectiveness to response quality.\n",
    "\n",
    "##### **What You‚Äôll Learn**\n",
    "\n",
    "In this tutorial, we will walk you through:\n",
    "1. RAGAS Evaluation:\n",
    "- Set up Ragas for evaluating a QA pipeline.\n",
    "- Use NVIDIA‚Äôs Nemotron-4-340B-Reward model as an evaluation judge.\n",
    "- Analyze evaluation metrics and interpret the results using Ragas.\n",
    "2. Reward Model Evaluation:\n",
    "- Implement the Nemotron-4-340B-Reward model to assess LLM responses.\n",
    "- Interpret the reward scores to understand response quality.\n",
    "- Incorporate reward-based feedback into an LLM evaluation pipeline.\n",
    "\n",
    "Let‚Äôs get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1Ô∏è‚É£ Environment Setup\n",
    "\n",
    "üîß Import Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "import asyncio\n",
    "\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë Setting Up NVIDIA API Key\n",
    "\n",
    "To access NVIDIA AI Endpoints, you need to provide a valid **NVIDIA API key**.\n",
    "\n",
    "- If the key is not already set as an environment variable, you will be prompted to enter it.\n",
    "- The key should start with `nvapi-`, ensuring it is correctly formatted.\n",
    "- This step is essential for interacting with NVIDIA's LLM services.\n",
    "\n",
    "Run the following cell to set up your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NVIDIA API key is set\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2Ô∏è‚É£ Load and Preprocess evaluation data\n",
    "\n",
    "Before we can evaluate a **retrieval-augmented generation (RAG) system**, we need a structured dataset containing:\n",
    "1. **User queries** (questions asked by users).\n",
    "2. **Generated responses** (answers produced by the RAG system) --- Here we evaluate the GraphRAG response\n",
    "3. **Retrieved contexts** (documents fetched by the retriever).\n",
    "4. **Reference answers** (ground-truth answers for comparison).\n",
    "\n",
    "In previous tutorials, Ground-truth (GT) question-answer pairs are synthetically generated using the nemotron-340b synthetic data generation model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('../data/evaluation_data.csv')\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3Ô∏è‚É£ Ragas Evaluation\n",
    "\n",
    "üìå Format the evaluation dataset into Ragas's expected input structure\n",
    "\n",
    "We need to format the evaluation dataset into a dictionary that matches Ragas' expected input structure and convert the data into a Hugging Face Dataset for efficient processing.\n",
    "\n",
    "**Why use a Hugging Face Dataset?**\n",
    "- Optimized for large-scale processing.\n",
    "- Memory-efficient, allowing operations on large datasets without excessive RAM usage.\n",
    "- Easily integrates with Ragas evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = {\n",
    "    'user_input': eval_df['question'].tolist(),\n",
    "    'response': eval_df['answer'].tolist(),\n",
    "    'retrieved_contexts': eval_df['gt_context'].apply(lambda x: [x] if isinstance(x, str) else x).tolist(),\n",
    "    'reference': eval_df['gt_answer'].tolist()\n",
    "}\n",
    "eval_dataset = Dataset.from_dict(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå Setting Up NVIDIA AI Endpoints for Ragas Evaluation\n",
    "\n",
    "To effectively evaluate **retrieval-augmented generation (RAG) pipelines**, we need a **LLM** for scoring responses and an **embedding model** for similarity comparisons. In this section, we integrate **NVIDIA AI Endpoints** with Ragas to power our evaluation.\n",
    "\n",
    "Why do we need these models?\n",
    "- LLM (In this example: mixtral-8x22b-instruct): Acts as a judge to evaluate responses generated by the RAG system.\n",
    "- Embedding Model (In this example: NV-EmbedQA-E5-V5): Computes semantic similarity between the retrieved document and the expected answer, which is crucial for assessing retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(\n",
    "    model=\"meta/llama-3.3-70b-instruct\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=300,\n",
    ")\n",
    "embeddings = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\", model_type=\"passage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå Wrapping NVIDIA AI Models for Ragas\n",
    "\n",
    "Ragas expects models in a specific format. To ensure compatibility, we wrap our NVIDIA LLM and embedding model using LangchainLLMWrapper and LangchainEmbeddingsWrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvpl_llm = LangchainLLMWrapper(langchain_llm=llm)\n",
    "nvpl_embeddings = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå View and Interpret Results from Ragas\n",
    "\n",
    "A Ragas score is comprised of the following:\n",
    "\n",
    "<img src=\"../data/ragas.png\" alt=\"Ragas Evaluation\" width=\"600\">\n",
    "\n",
    "**Metrics explained**\n",
    "\n",
    "1. Faithfulness: measures the factual accuracy of the generated answer with the context provided. This is done in 2 steps. First, given a question and generated answer, Ragas uses an LLM to figure out the statements that the generated answer makes. This gives a list of statements whose validity we have we have to check. In step 2, given the list of statements and the context returned, Ragas uses an LLM to check if the statements provided are supported by the context. The number of correct statements is summed up and divided by the total number of statements in the generated answer to obtain the score for a given example.\n",
    "\n",
    "2. Answer Relevancy: measures how relevant and to the point the answer is to the question. For a given generated answer Ragas uses an LLM to find out the probable questions that the generated answer would be an answer to and computes similarity to the actual question asked.\n",
    "\n",
    "3. Context Precision: measures the precision of the retrieved context in providing relevant information for generating answer. Given a question, answer and retrieved context, Ragas calls LLM to check sentences from the ground truth answer against a retrieved context. It is the ratio between the relevant sentences from retrieved context and the total sentence from ground truth answer.\n",
    "\n",
    "4. Context Recall: measures the ability of the retriever to retrieve all the necessary information needed to answer the question. Ragas calculates this by using the provided ground_truth answer and using an LLM to check if each statement from it can be found in the retrieved context. If it is not found that means the retriever was not able to retrieve the information needed to support that statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_evaluation = evaluate(eval_dataset, llm=nvpl_llm, embeddings=nvpl_embeddings,\n",
    "                             metrics=[answer_relevancy, context_precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4Ô∏è‚É£ Reward Model Evaluation\n",
    "\n",
    "üìå Setting Up NVIDIA AI Endpoints for Reward Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.environ[\"NVIDIA_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_reward_scores(question, answer):\n",
    "    try:\n",
    "        completion = reward_client.chat.completions.create(\n",
    "            model=\"nvidia/nemotron-4-340b-reward\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "                {\"role\": \"assistant\", \"content\": answer}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Extract and parse response\n",
    "        content = completion.choices[0].message[0].content\n",
    "        res = content.split(\",\")\n",
    "        content_dict = {item.split(\":\")[0].strip(): float(item.split(\":\")[1]) for item in res}\n",
    "        #print(content_dict) #debug output\n",
    "        return content_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {question}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_dataframe(df):\n",
    "    tasks = [get_reward_scores(row[\"question\"], row[\"answer\"]) for _, row in df.iterrows()]\n",
    "    results = await asyncio.gather(*tasks)  # Run all tasks concurrently\n",
    "\n",
    "    # Define metrics\n",
    "    metrics = [\"Helpfulness\", \"Correctness\", \"Coherence\", \"Complexity\", \"Verbosity\"]\n",
    "\n",
    "    # Add results to DataFrame\n",
    "    for metric in metrics:\n",
    "        df[metric] = [res.get(metric.lower()) if res else None for res in results]\n",
    "\n",
    "    # Calculate mean of each metric\n",
    "    mean_scores = df[metrics].mean()\n",
    "    print(\"Mean Scores:\\n\", mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    await process_dataframe(eval_df)\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a conversation with multiple turns between user and assistant, it rates the following attributes (typically between 0 and 4) for every assistant turn.\n",
    "\n",
    "- Helpfulness: Overall helpfulness of the response to the prompt.\n",
    "- Correctness: Inclusion of all pertinent facts without errors.\n",
    "- Coherence: Consistency and clarity of expression.\n",
    "- Complexity: Intellectual depth required to write response (i.e. whether the response can be written by anyone with basic language competency or requires deep domain expertise)\n",
    "- Verbosity: Amount of detail included in the response, relative to what is asked for in the prompt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

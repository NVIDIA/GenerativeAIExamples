{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Persitant-Dynamic Backend\n",
    "\n",
    "Now that we have our graph triplets, we would like to do inference with them! But hold on. In real world applications our knowledge graph may change over time. In this case we will want to be able to handle triplets being both added and deleted. Additionally, a persitent database for this information will be crucial in the case of crashes or other unforseen issues! \n",
    "\n",
    "This notebook will show you how to connect a simple knowledge-graph RAG agent to a database that is being actively updated, and we will do this without sacrificing performance! Let's get started.\n",
    "\n",
    "The first thing we will need to do are a great many imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[00:18:27 +0000] [INFO]: NetworkX-cuGraph is available.\n"
     ]
    }
   ],
   "source": [
    "# General imports.\n",
    "import os \n",
    "import re\n",
    "import time\n",
    "import timeit\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import getpass\n",
    "\n",
    "# Imports for both our database and inference.\n",
    "import cugraph\n",
    "import networkx as nx\n",
    "import nx_arangodb as nxadb\n",
    "import pandas as pd\n",
    "from nx_arangodb.convert import nxadb_to_nxcg\n",
    "from arango import ArangoClient\n",
    "\n",
    "# Langchain related imports.\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from imports.qa_chain_overrides import NXCugraphEntityGraph, GPUGraphQAChain\n",
    "\n",
    "# Import the threading and multiprocessing toolboxes for parallelism\n",
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting-up our Backend.\n",
    "\n",
    "Now that we have all of those imports done, we can start with the good stuff. We will be using ArangoDB as a backend. ArangoDB is a graph database that works very well with NetworkX, a popular Python library for graph analysis. Better yet, ArangoDB has a cuGraph persistance layer available, which will allow us to accelerate NetworkX on GPU! If you want to learn more about this check out the blog [HERE](https://developer.nvidia.com/blog/accelerated-production-ready-graph-analytics-for-networkx-users/). \n",
    "\n",
    "We will get started by launching the database! The following command will launch an arangodb instance on port 8530 with the username \"root\" and password \"ilovekgrag\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/arangodb\" is already in use by container \"08c1578a46e47dc4f5708c8e6f222eb6b367f0df6ff84960ddf78ebd80b38207\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run -e ARANGO_ROOT_PASSWORD=ilovekgrag -d --name arangodb -p 8529:8529 arangodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to populate our database. I hope you will forgive us, but we took the liberty of creating some convenient CSV's for reading in the triples from the previous notebook. The details of this process can be found in the file \"*data/getcsv.py*\" if you are interested! We will first need to set up the structure of our graph database, and then populate with our CSV files. Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will set some environment variables. These will come up a few times!\n",
    "adb_host = \"http://172.17.0.1:8529\"\n",
    "#adb_host = \"http://localhost:8530\"\n",
    "adb_username = \"root\"\n",
    "adb_password = \"ilovekgrag\"\n",
    "adb_name = \"arangodb\"\n",
    "os.environ[\"DATABASE_HOST\"] = adb_host\n",
    "os.environ[\"DATABASE_USERNAME\"] = adb_username\n",
    "os.environ[\"DATABASE_PASSWORD\"] = adb_password\n",
    "os.environ[\"DATABASE_NAME\"] = adb_name\n",
    "\n",
    "# Now we need to set up our database structure.\n",
    "# Set our client.\n",
    "client = ArangoClient(hosts=adb_host)\n",
    "\n",
    "# Now we can log in to a user that can control system properties.\n",
    "system = client.db(\"_system\",username=adb_username,password=adb_password)\n",
    "\n",
    "# We now want to create our database if it doesnt exist.\n",
    "if not system.has_database(adb_name):\n",
    "    system.create_database(adb_name)\n",
    "    \n",
    "# Now we can set our ADB database to that.\n",
    "ADB = client.db(adb_name, username=adb_username, password=adb_password)\n",
    "\n",
    "# Now that this is done, we need to define the components of our ArangoDB graph.\n",
    "ADB_graph = None\n",
    "ADB_vertices = None\n",
    "ADB_edges =None\n",
    "if not ADB.has_graph(\"graph_data\"):\n",
    "    ADB_graph = ADB.create_graph(\"graph_data\")\n",
    "else:\n",
    "    ADB_graph = ADB.graph(\"graph_data\")\n",
    "\n",
    "if not ADB_graph.has_vertex_collection(\"vertices\"):\n",
    "    ADB_vertices = ADB_graph.create_vertex_collection(\"vertices\")\n",
    "else:\n",
    "    ADB_vertices = ADB_graph.vertex_collection(\"vertices\")\n",
    "\n",
    "if not ADB_graph.has_edge_collection(\"edges\"):\n",
    "    edge_def = {\n",
    "                'edge_collection': 'edges',\n",
    "                'from_vertex_collections': ['vertices'],\n",
    "                'to_vertex_collections': ['vertices']\n",
    "            }\n",
    "    ADB_edges = ADB_graph.create_edge_definition(**edge_def)\n",
    "else:\n",
    "    ADB_edges = ADB_graph.edge_collection(\"edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the backend is set up to accept our CSVs! Let's place our edges and vertices into the database. These commands get big, and we have variables in Python already defined that will be useful, so let's just define a way to run these commands in Python. This will also be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the collections are cleared.\n",
    "ADB_vertices.truncate()\n",
    "ADB_edges.truncate()\n",
    "\n",
    "# Get dataframes for the data.\n",
    "vertices_df = pd.read_csv(\"../data/csvs/vertices.csv\")\n",
    "edges_df = pd.read_csv(\"../data/csvs/edges.csv\")\n",
    "\n",
    "# Insert vertices into the database\n",
    "for index, row in vertices_df.iterrows():\n",
    "    aql = \"\"\"\n",
    "        INSERT {\n",
    "            _key: @key\n",
    "        } INTO vertices\n",
    "    \"\"\"\n",
    "    bind_vars = {\n",
    "        \"key\": row.get(\"_key\"), \n",
    "    }\n",
    "    try:\n",
    "        ADB.aql.execute(aql, bind_vars=bind_vars)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "# Insert edges into the database\n",
    "for index, row in edges_df.iterrows():\n",
    "    aql = \"\"\"\n",
    "        INSERT {\n",
    "            _key: @key,\n",
    "            _from: @from,\n",
    "            _to: @to,\n",
    "            predicate: @predicate,\n",
    "        } INTO edges\n",
    "    \"\"\"\n",
    "    bind_vars = {\n",
    "        \"key\": str(index), \n",
    "        \"from\": row.get(\"_from\"), \n",
    "        \"to\": row.get(\"_to\"), \n",
    "        \"predicate\": row.get(\"predicate\"),\n",
    "    }\n",
    "    try:\n",
    "        ADB.aql.execute(aql, bind_vars=bind_vars)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now we have a database we can really work with! But we still have a problem. This database is not on GPU. If we want to serve end users concurrently this is going to be a problem, and really limit our number of feasible inferences. So we need to read the data from this backend into NetworkX, and then use cuGraph as our backend to perform RAG tasks on GPU! Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:18:43 +0000] [INFO]: Graph 'graph_data' exists.\n",
      "[00:18:43 +0000] [INFO]: Default node type set to 'vertices'\n",
      "[00:18:44 +0000] [INFO]: Graph 'graph_data' load took 0.08841562271118164s\n",
      "[00:18:44 +0000] [INFO]: NXCG Graph construction took 0.15645480155944824s\n"
     ]
    }
   ],
   "source": [
    "# First we snag our database as a NetworkX graph.\n",
    "cpu_graph = nxadb.MultiDiGraph(name=\"graph_data\")\n",
    "    \n",
    "# Now we can set this up to use cuGraph as well!\n",
    "background_graph = nxadb_to_nxcg(cpu_graph)\n",
    "    \n",
    "#for edge in cpu_graph.edges():\n",
    "#    background_graph[edge[0]][edge[1]]['predicate'] = cpu_graph[edge[0]][edge[1]]['predicate']\n",
    "\n",
    "background_graph = NXCugraphEntityGraph(background_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be curious about the naming convention here. Why are we calling them *cpu_graph* and *background_graph*? Since our ArangoDB backend is not stored on the GPU, it's NetworkX graph goes through an intermediate CPU phase, and is then placed on the GPU. This explains the *cpu_graph* name, but why *background_graph*? Well, it takes time to move memory around. And since we are trying to create a dynamic knowledge graph, it is beneficial to keep two copies of our graph at a time. One copy can be continuously updated as updates are streamed in, while the other can be used for inference in that time. This will require some multi-threading, but we will keep it as simple as possible, and let you imagine more complicated work loads with continuous-asynchronous user queries and updates!  \n",
    "\n",
    "To make this all work we will need a couple functions. Mainly, a function to handle database alterations, and a function to \"swap\" our *working_graph* and out *background_graph*. Let's build those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a function for adding or removing a list of edges.\n",
    "def changeADBEdges( edgelist:list[dict], remove:bool=False ):\n",
    "\n",
    "    def updateDBEdge(subject_:str,relation_:str,object_:str,key_:str,remove:bool):\n",
    "            \n",
    "        # Make sure to strip out bad characters.\n",
    "        subj = re.sub(r'[^a-zA-Z0-9_]', '', subject_)\n",
    "        obj = re.sub(r'[^a-zA-Z0-9_]', '', object_)\n",
    "        rel = re.sub(r'[^a-zA-Z0-9_]', '', relation_)\n",
    "        edge_key = re.sub(r'[^a-zA-Z0-9_]', '', key_)\n",
    "\n",
    "        # Define our edge_data and insert it.\n",
    "        edge_data = {\n",
    "            '_from': f'vertices/{subj}',\n",
    "            '_to': f'vertices/{obj}',\n",
    "            'predicate': f'{rel}',\n",
    "            '_key': edge_key\n",
    "        }\n",
    "\n",
    "        # Set up our AQL query to see if the edge exists. NEED: also check for edge attributes.\n",
    "        aql_query = f\"\"\"\n",
    "        FOR edge IN edges\n",
    "            FILTER edge._key == @edge_key\n",
    "            RETURN edge\n",
    "        \"\"\"           \n",
    "\n",
    "        # Check if the edge exists.\n",
    "        edges = ADB.aql.execute(aql_query, bind_vars={\"edge_key\":edge_key})\n",
    "        edges = list(edges)\n",
    "        \n",
    "        has_edge = False\n",
    "        if edges:\n",
    "            has_edge = True\n",
    "\n",
    "        # Check if we are adding or removing this edge.\n",
    "        if not remove:\n",
    "            if not has_edge:\n",
    "                try:\n",
    "                    ADB_edges.insert(edge_data)\n",
    "                except Exception as e:\n",
    "                    # Covers some potential misformatting cases.\n",
    "                    pass\n",
    "        else:\n",
    "            if has_edge:\n",
    "                ADB_edges.delete(edge_data[\"_key\"])\n",
    "    \n",
    "    # Launch a process for each edge to update the database.\n",
    "    for edge in edgelist:\n",
    "        updateDBEdge(**edge,remove=remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's get all of our edges.\n",
    "edge_collection = ADB.collection(\"edges\")\n",
    "edgelist = list(edge_collection.all())\n",
    "\n",
    "# Define an edge-sampling function.\n",
    "def sampleEdges(edgelist:list, sample_size:int=100):\n",
    "\n",
    "    # First we sample the edges.\n",
    "    sampled_indices = np.random.choice(np.arange(len(edgelist)), size=sample_size, replace=False)\n",
    "\n",
    "    # Now turn them into a usable form to return.\n",
    "    return_edges = []\n",
    "    for index in sampled_indices:\n",
    "        edge = edgelist[index]\n",
    "        return_edges.append({\"subject_\":edge[\"_from\"].split(\"/\")[1], \n",
    "                             \"relation_\":edge[\"predicate\"], \n",
    "                             \"object_\":edge[\"_to\"].split(\"/\")[1],\n",
    "                             \"key_\":edge[\"_key\"]})\n",
    "    \n",
    "    # Return the sampled edges.\n",
    "    return(return_edges)\n",
    "\n",
    "# Now let's sample!\n",
    "sampled_edges = sampleEdges(edgelist)\n",
    "\n",
    "# Let's test to see if it works!\n",
    "changeADBEdges( edgelist = sampled_edges, remove = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great it works! We will add those edges back later when we try to simulate a real workload. Now the bulk of our backend functionality is all prepared for us. We should probably get around to adding our *working_graph* as well as a function for exchaning it with *background_graph* when the time comes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE SWAP..\n",
      "background_graph | number of vertices: 6913\n",
      "working_graph | number of vertices: 0\n",
      "AFTER SWAP...\n",
      "background_graph | number of vertices: 0\n",
      "working_graph | number of vertices: 6913\n"
     ]
    }
   ],
   "source": [
    "# Define our working graph as the datatype we need.\n",
    "working_graph = NXCugraphEntityGraph()\n",
    "\n",
    "# Print out information before the swap.\n",
    "back_nodes = background_graph.get_number_of_nodes()\n",
    "work_nodes = working_graph.get_number_of_nodes()\n",
    "print(\"BEFORE SWAP..\")\n",
    "print(f\"background_graph | number of vertices: {back_nodes}\")\n",
    "print(f\"working_graph | number of vertices: {work_nodes}\")\n",
    "\n",
    "# Perform a swap.\n",
    "working_graph, background_graph = background_graph, working_graph\n",
    "\n",
    "# Print out the contents of our graphs after the swap!\n",
    "back_nodes = background_graph.get_number_of_nodes()\n",
    "work_nodes = working_graph.get_number_of_nodes()\n",
    "print(\"AFTER SWAP...\")\n",
    "print(f\"background_graph | number of vertices: {back_nodes}\")\n",
    "print(f\"working_graph | number of vertices: {work_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step we need for our dynamic backend to work is the ability to update the backend after making Arango updates! Let's make a function for it, and we can call that from an independent thread later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:18:44 +0000] [INFO]: Graph 'graph_data' exists.\n",
      "[00:18:44 +0000] [INFO]: Default node type set to 'vertices'\n",
      "[00:18:44 +0000] [INFO]: Graph 'graph_data' load took 0.08525204658508301s\n",
      "[00:18:44 +0000] [INFO]: NXCG Graph construction took 0.0016293525695800781s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background_graph | number of vertices: 6913\n",
      "working_graph | number of vertices: 6858\n"
     ]
    }
   ],
   "source": [
    "# Make a general \"updateBackend\" function.\n",
    "# We already did this earlier, but we will be doing it a lot in the future.\n",
    "def updateBackend():\n",
    "\n",
    "    # First we snag our database as a NetworkX graph.\n",
    "    cpu_graph = nxadb.MultiDiGraph(name=\"graph_data\")\n",
    "\n",
    "    # Now we can set this up to use cuGraph as well!\n",
    "    background_graph = nxadb_to_nxcg(cpu_graph)\n",
    "    background_graph = NXCugraphEntityGraph(background_graph)\n",
    "\n",
    "    # Return our results\n",
    "    return( cpu_graph, background_graph )\n",
    "\n",
    "# Check that it works.\n",
    "cpu_graph, background_graph = updateBackend()\n",
    "background_graph, working_graph = working_graph, background_graph\n",
    "\n",
    "# Print out some info.\n",
    "back_nodes = background_graph.get_number_of_nodes()\n",
    "work_nodes = working_graph.get_number_of_nodes()\n",
    "print(f\"background_graph | number of vertices: {back_nodes}\")\n",
    "print(f\"working_graph | number of vertices: {work_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's that! Notice that our *working_graph* now has less nodes than the *background_graph* since we removed a bunch! We will get a chance to use this more shortly, but first we need to make an agent to actualy perform our inference with.\n",
    "\n",
    "## Building our RAG Agent\n",
    "\n",
    "Now we want to put a RAG agent on top of our backend to allow for user inferences. We will use some [NIM endpoints](https://www.nvidia.com/en-us/ai/) for this and connect to them through [LangChain](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/). \n",
    "\n",
    "Since this is just a simple example, our agent doesn't need to do too much. Given a knowledge graph and a user query, we want it to retrieve relevent relationships from the graph to return to the user. Fortunately there is a way to do this using the [GraphQAChain](https://python.langchain.com/api_reference/community/chains/langchain_community.chains.graph_qa.base.GraphQAChain.html) in LangChain. Since we are using the GPU, and not the CPU, some small changes to the QA chain need to be altered. This has already been done, and the changes can be found within the *qa_chain_overrides.py* file. We have actally already used some of these earlier in the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty can be contributed by various factors including:\n",
      "\n",
      "1. Lack of predictability in events and outcomes.\n",
      "2. Lack of information or limited data.\n",
      "3. Incomplete understanding of causal relationships.\n",
      "4. Ambiguity and vagueness.\n",
      "5. Complexity of a situation or system.\n",
      "6. Change and volatility in the environment.\n",
      "7. Uncertainty in human behavior and decision-making.\n"
     ]
    }
   ],
   "source": [
    "# Let's define a QA Chain to use.\n",
    "llm_name = \"mistralai/mixtral-8x22b-instruct-v0.1\"\n",
    "model = ChatNVIDIA(model=llm_name)\n",
    "chain = GPUGraphQAChain.from_llm( llm=model, graph=working_graph, verbose=True )\n",
    "\n",
    "# Let's define a function for performing a query.\n",
    "def queryGraph( chain:GPUGraphQAChain, query:str ):\n",
    "\n",
    "    result = chain._call( inputs = { 'query' : query } )\n",
    "\n",
    "    return(result)\n",
    "\n",
    "# Let's test out this function and see what we get!\n",
    "response = queryGraph(chain,\"What factors contribute to Uncertainty?\")\n",
    "print(response[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the system.\n",
    "\n",
    "Now that we have an agent for chat completion, and an agent for handling dynamic information, let's see how this comes together in a test enfironment. We will simulate this by looping, and performing queries as often as well can. First let's get some baseline times for inference and background updates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:18:57 +0000] [INFO]: Graph 'graph_data' exists.\n",
      "[00:18:57 +0000] [INFO]: Default node type set to 'vertices'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average query time: 2.1006909924002684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:18:57 +0000] [INFO]: Graph 'graph_data' load took 0.09360814094543457s\n",
      "[00:18:57 +0000] [INFO]: NXCG Graph construction took 0.0019872188568115234s\n",
      "[00:18:57 +0000] [INFO]: Graph 'graph_data' exists.\n",
      "[00:18:57 +0000] [INFO]: Default node type set to 'vertices'\n",
      "[00:18:57 +0000] [INFO]: Graph 'graph_data' load took 0.09091401100158691s\n",
      "[00:18:57 +0000] [INFO]: NXCG Graph construction took 0.0013082027435302734s\n",
      "[00:18:57 +0000] [INFO]: Graph 'graph_data' exists.\n",
      "[00:18:57 +0000] [INFO]: Default node type set to 'vertices'\n",
      "[00:18:58 +0000] [INFO]: Graph 'graph_data' load took 0.08577418327331543s\n",
      "[00:18:58 +0000] [INFO]: NXCG Graph construction took 0.0018820762634277344s\n",
      "[00:18:58 +0000] [INFO]: Graph 'graph_data' exists.\n",
      "[00:18:58 +0000] [INFO]: Default node type set to 'vertices'\n",
      "[00:18:58 +0000] [INFO]: Graph 'graph_data' load took 0.08767390251159668s\n",
      "[00:18:58 +0000] [INFO]: NXCG Graph construction took 0.0012555122375488281s\n",
      "[00:18:58 +0000] [INFO]: Graph 'graph_data' exists.\n",
      "[00:18:58 +0000] [INFO]: Default node type set to 'vertices'\n",
      "[00:18:58 +0000] [INFO]: Graph 'graph_data' load took 0.09106302261352539s\n",
      "[00:18:58 +0000] [INFO]: NXCG Graph construction took 0.0017921924591064453s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for backend update and swap: 0.2946968696000113\n"
     ]
    }
   ],
   "source": [
    "# Get a couple queries to test with.\n",
    "queries = [\n",
    "    f\"What does Sailpoint Technologies Holdings Inc do?\",\n",
    "    f\"What factors contribute to Market Risk?\",\n",
    "    f\"Where is the headquarters of Cornerstone OnDemand Inc?\",\n",
    "    f\"What does Five9 Inc sell?\",\n",
    "    f\"What industry does TWILIO INC work in?\"\n",
    "]\n",
    "\n",
    "# Change our chain to not be verbose.\n",
    "chain = GPUGraphQAChain.from_llm( llm=model, graph=working_graph, verbose=False )\n",
    "\n",
    "# Time our queries in serial.\n",
    "average_query_time = 0\n",
    "for query in queries:\n",
    "\n",
    "    start = time.monotonic()\n",
    "    queryGraph(chain,query)\n",
    "    end = time.monotonic()\n",
    "    average_query_time += (end-start)\n",
    "\n",
    "# Print the average.\n",
    "print(f\"Average query time: {average_query_time/len(queries)}\")\n",
    "\n",
    "# Compute the time to make a few backend changes.\n",
    "average_backend_time = 0\n",
    "sample_num = 5\n",
    "remove_bool = False\n",
    "for _ in range(sample_num):\n",
    "\n",
    "    start = time.monotonic()\n",
    "    changeADBEdges( edgelist = sampled_edges, remove = remove_bool )\n",
    "    cpu_graph, background_graph = updateBackend()\n",
    "    background_graph, working_graph = working_graph, background_graph\n",
    "    end = time.monotonic()\n",
    "    average_backend_time += (end-start)\n",
    "    remove_bool = not remove_bool\n",
    "\n",
    "print(f\"Time for backend update and swap: {average_backend_time/sample_num}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Our backend changes are pretty fast! Maybe we don't need to multithread anything. To make sure, let's see what happens when we increase the number of edges we are adding and removing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:19:07 +0000] [INFO]: Graph 'graph_data' exists.\n",
      "[00:19:07 +0000] [INFO]: Default node type set to 'vertices'\n",
      "[00:19:07 +0000] [INFO]: Graph 'graph_data' load took 0.08619809150695801s\n",
      "[00:19:07 +0000] [INFO]: NXCG Graph construction took 0.0021889209747314453s\n"
     ]
    }
   ],
   "source": [
    "# Sample a lot of edges.\n",
    "sampled_edges = sampleEdges(edgelist,sample_size=len(edgelist)//2)\n",
    "\n",
    "# Compute the time to make a few backend changes.\n",
    "average_backend_time = 0\n",
    "sample_num = 5\n",
    "remove_bool = True\n",
    "for _ in range(sample_num):\n",
    "\n",
    "    start = time.monotonic()\n",
    "    changeADBEdges( edgelist = sampled_edges, remove = remove_bool )\n",
    "    cpu_graph, background_graph = updateBackend()\n",
    "    background_graph, working_graph = working_graph, background_graph\n",
    "    end = time.monotonic()\n",
    "    average_backend_time += (end-start)\n",
    "    remove_bool = not remove_bool\n",
    "\n",
    "print(f\"Average time for backend update and swap: {average_backend_time/sample_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, having many seconds of down-time where we aren't serving user-queries is not very good. So, let's parallelize this solution. The following code uses a mixture of threading and multiprocessing to achieve parallelism!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the background process function\n",
    "def backgroundProcessFunction(sampled_edges, remove_bool, shared_data):\n",
    "\n",
    "    global cpu_graph\n",
    "    global background_graph\n",
    "\n",
    "    start = time.monotonic()\n",
    "    changeADBEdges(edgelist=sampled_edges, remove=remove_bool)\n",
    "    cpu_graph, background_graph = updateBackend()\n",
    "    end = time.monotonic()\n",
    "    print(f\"Time to update backend: {end - start}\")\n",
    "    \n",
    "    # Mark swap as inactive\n",
    "    shared_data['active_swap'] = False\n",
    "\n",
    "# Define the query process function\n",
    "def performQuery(chain, query, shared_data):\n",
    "\n",
    "    start = time.monotonic()\n",
    "    queryGraph(chain, query)\n",
    "    end = time.monotonic()\n",
    "    print(f\"Time for query: {end - start}\")\n",
    "    \n",
    "    # Mark query as inactive\n",
    "    shared_data['active_query'] = False\n",
    "\n",
    "# Create a manager for shared state between processes\n",
    "manager = multiprocessing.Manager()\n",
    "\n",
    "# Shared dictionary for global-like state\n",
    "shared_data = manager.dict({\n",
    "    'active_swap': False,\n",
    "    'active_query': False\n",
    "})\n",
    "\n",
    "backends_swapped = 0\n",
    "while backends_swapped < 5:\n",
    "   \n",
    "    if not shared_data['active_swap']:\n",
    "\n",
    "        if not shared_data['active_query']:\n",
    "            background_graph, working_graph = working_graph, background_graph\n",
    "            \n",
    "        shared_data['active_swap'] = True \n",
    "        background_process = threading.Thread(\n",
    "            target=backgroundProcessFunction,\n",
    "            args=(sampled_edges, remove_bool, shared_data,)\n",
    "        )\n",
    "        background_process.start()\n",
    "        remove_bool = not remove_bool\n",
    "        backends_swapped += 1\n",
    "\n",
    "    elif not shared_data['active_query']:\n",
    "        shared_data['active_query'] = True  \n",
    "        query_process = multiprocessing.Process(\n",
    "            target=performQuery,\n",
    "            args=(chain, queries[0], shared_data)\n",
    "        )\n",
    "        query_process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! That is way better! We are getting slightly longer update times, but achieving the same inference times we were getting before! Users will be far less upset than when we had long dead periods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

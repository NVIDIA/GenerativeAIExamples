{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving our KG.\n",
    "\n",
    "Now that we have a knowledge graph generated by our LLM, how do we know that we are not missing facts from our database? This is a difficult problem in the study of knowledge graphs, with a long history, and in this notebook we will go over one way that you can tackle this problem. We will also discuss some other techniques that you can try.\n",
    "\n",
    "Let's start off, as we always do: with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networkx imports for dealing with KGs directly.\n",
    "import networkx as nx\n",
    "from networkx import MultiDiGraph\n",
    "\n",
    "# Linear algebra / data science toolkit we are using.\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.decomposition import NMF as SklearnNMF\n",
    "import heapq\n",
    "\n",
    "# ML models for KG handled through PyKEEN.\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.predict import predict_target\n",
    "\n",
    "# General imports.\n",
    "import os\n",
    "import json\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first, familiar step.\n",
    "\n",
    "As with notebooks before me, one of the first things we need to do is to read in the same set of triplets we keep using. Let's go ahead and do that! We will end up casting this into a couple different formats over the course of this notebook to make the triplets work with different tools as explained in the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_triplet_list( directory : str ) -> np.array:\n",
    "    \"\"\"\n",
    "    Reads in all of our triplets as a list of tuples \n",
    "    Outputs a numpy array to use with PyKEEN.\n",
    "    \"\"\"\n",
    "    filemap = []\n",
    "    trip_list = []\n",
    "\n",
    "    for filename in os.listdir( directory ):\n",
    "\n",
    "        try:\n",
    "            file_path = os.path.join( directory, filename )\n",
    "            with open( file_path, \"r\" ) as file:\n",
    "\n",
    "                data = json.load( file )\n",
    "\n",
    "                name = data.get(\"filename\")\n",
    "                trips = data.get(\"item_1a\") + data.get(\"item_7\")\n",
    "\n",
    "                for triplet in trips:\n",
    "                    if triplet[0] != None and triplet[2] != None and triplet[3] != None:\n",
    "                        trip_list.append([triplet[0],triplet[2],triplet[3]])\n",
    "                        filemap.append(name)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return( np.array(trip_list), filemap )\n",
    "\n",
    "TRIP_DIRECTORY = \"../data/triples_10k\"\n",
    "MODEL_DIRECTORY = \"../data/TransE_model\"\n",
    "\n",
    "triplets, filemap = read_triplet_list( TRIP_DIRECTORY )\n",
    "\n",
    "print(\"\\nTriplets read...\")\n",
    "print(triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a thing or two about our graph.\n",
    "\n",
    "Now that we have the triplets read in we can train a model using PyKEEN. Let's explain **what** model we are training first. In the world of knowledge graph completion there are many ways you can perform edge prediction. For simple knowledge graphs you may use nonnegative matrix factorization (NMF) to predict unlabeled edges based on structural properties for instance. For more complicated knowledge graphs we require more sophisticated methods like neural networks. The way these models tend to work is to take all the nodes in our graph as well as all the edges and obtain embeddings for them, such that for any edge *(u, relation, v)* we have a likewise set of embeddings *(e(u), e(relation), e(v))* where *e(u) + e(relation) = e(v)*. At least this is the idea. In reality, we end up getting some approximation of that. Then, if we have two nodes *u,v* and we want to predict what an edge between them might be, we can cycle through all the different relations we have emebdded, and see where the prior equation holds the closest! This is what the **TransE** model does, and it does this without the need to train an expensive GNN. \n",
    "\n",
    "It should be noted there are a ton of variants of this including *TransR*, and *TransSPARSE*, but we will keep it simple! Let's go ahead and train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model( \n",
    "        triplets : np.array, \n",
    "        model : str = 'TransE',\n",
    "        num_epochs : int = 100,\n",
    "        batch_size : int = 256,\n",
    "        lr : float = 0.01\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Takes in our triplets and trains a model.\n",
    "    \"\"\"\n",
    "    triples_factory = TriplesFactory.from_labeled_triples( triplets )\n",
    "\n",
    "    training, testing, validation = triples_factory.split([0.8,0.1,0.1])\n",
    "\n",
    "    results = pipeline(\n",
    "        training = training,\n",
    "        testing = testing,\n",
    "        validation = validation,\n",
    "        model = model,\n",
    "        training_kwargs = dict( num_epochs = num_epochs, batch_size = batch_size ),\n",
    "        optimizer = 'adam',\n",
    "        optimizer_kwargs = dict( lr=lr ),\n",
    "    )\n",
    "\n",
    "    return( results )\n",
    "\n",
    "model = train_model( triplets = triplets )\n",
    "print(\"\\nModel trained...\")\n",
    "model.save_to_directory( MODEL_DIRECTORY )\n",
    "print(\"\\nModel saved...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An unfortunate truth.\n",
    "\n",
    "Now we may think that we are mostly done. \"Aha, now that my model is trained, I can just check every possible edge to see which ones satisfy the above equations!\" you may have thought. And you are right, if we had a very small graph that is. Unfortunately, that is a lot of edges and vector comparisons to perform. And with the right hardware we definitely **can** perform that process on our graph, but not in general. In general, doing an all-to-all prediction with TransE, or any embedding-based link-prediction model is prohibitive. Instead we limit to subgraphs and perform this comparison there.\n",
    "\n",
    "There are a lot of ways that we can get sensible subgraphs to perform prediction on. Some popular methods are to perform a shallow breadth-first-search (BFS) around a node and only consider links within it. Another is to get an array of paths between nodes using A* or Djikstra's algorithm, and use that induced subgraph. In our case we are going to layer some techniques. We are going to use NMF to suggest structurally similar links and then we will use TransE to fill in what those link's values \"should\" be. To reiterate though, this is not the **best** way of doing this, this is just **a** way of doing this. It is recommended to test alternate subgraph retrieval techniques for your specific KG and your specific use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplets_to_kg( triplets : np.array, filemap : list[str] ) -> MultiDiGraph:\n",
    "    \"\"\"\n",
    "    Reads our triplets as a NetworkX graph.\n",
    "    \"\"\"\n",
    "    kg = MultiDiGraph()\n",
    "\n",
    "    for i,triplet in enumerate(triplets):\n",
    "        head = triplet[0]\n",
    "        relation = triplet[1]\n",
    "        tail = triplet[2]\n",
    "        file = filemap[i]\n",
    "\n",
    "        if head != None and tail != None and relation != None:\n",
    "            kg.add_node(head)\n",
    "            kg.add_node(tail)\n",
    "            kg.add_edge(head, tail, relation=relation, file=file)\n",
    "    \n",
    "    return( kg )\n",
    "\n",
    "def NMF(graph: MultiDiGraph, top_k: int = 100) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Performs nonnegative matrix factorization for link prediction using normalized adjacency matrix.\n",
    "    Returns a ranked list of the top_k most likely node-name-pairs.\n",
    "    \"\"\"\n",
    "    # Get the adjacency matrix\n",
    "    adj_matrix = nx.adjacency_matrix(graph)\n",
    "\n",
    "    # Normalize the adjacency matrix.\n",
    "    deg = np.array(adj_matrix.sum(axis=1)).flatten()\n",
    "    deg_inv_sqrt = np.zeros_like(deg)\n",
    "    deg_inv_sqrt[deg != 0] = 1.0 / np.sqrt(deg[deg != 0])\n",
    "    D_inv_sqrt = sp.diags(deg_inv_sqrt)\n",
    "    normalized_adj = D_inv_sqrt @ adj_matrix @ D_inv_sqrt\n",
    "\n",
    "    # Convert to dense array for NMF\n",
    "    normalized_adj_dense = normalized_adj.toarray()\n",
    "\n",
    "    # Perform NMF to get similarities\n",
    "    n = normalized_adj_dense.shape[0]\n",
    "    model = SklearnNMF(n_components=min(n, 10), init='random', random_state=0)\n",
    "    W = model.fit_transform(normalized_adj_dense)\n",
    "    H = model.components_\n",
    "    \n",
    "    # Reconstruct the matrix and calculate similarities\n",
    "    reconstructed = np.dot(W, H)\n",
    "    \n",
    "    # Find non-existent edges and their scores.\n",
    "    mask = np.triu(np.ones_like(adj_matrix.toarray()), k=1).astype(bool)\n",
    "    potential_edges = reconstructed[mask & (adj_matrix.toarray() == 0)]\n",
    "    indices = np.argwhere(mask & (adj_matrix.toarray() == 0))\n",
    "\n",
    "    # Combine scores and indices into a single list\n",
    "    edge_data = list(zip(potential_edges, indices[:, 0], indices[:, 1]))\n",
    "\n",
    "    # Use heapq to get the top_k edges based on scores\n",
    "    top_edges = heapq.nlargest(top_k, edge_data, key=lambda x: x[0])\n",
    "\n",
    "    # Get the node names corresponding to the indices\n",
    "    nodes = list(graph.nodes())\n",
    "    \n",
    "    # Return the pairs as a list of lists of the form [[head0, tail0], [head1, tail1], ...]\n",
    "    return [[nodes[head], nodes[tail]] for _, head, tail in top_edges]\n",
    "\n",
    "kg = triplets_to_kg( triplets, filemap )\n",
    "print(\"\\nKG obtained...\")\n",
    "\n",
    "possible_edges = NMF( kg )\n",
    "print(\"\\nPossible edges obtained...\")\n",
    "print(possible_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to note about the previous code is that we normalize the adjacency matrix by the inverse square-root of the degree of each node when performing NMF. There is a long history of why this is the way we choose to perform the adjacency normalization, but the reason we perform this normalization is to dissuade \"bad-behavior\" from NMF. If we do not perform a normalization, then the predicted edges will cluster around high-degree nodes, and we want to avoid that in lieu of more varied structure.\n",
    "\n",
    "Now with that out of the way we are really close! We just have to see what the most-likely edges are. Let's write up one more function to do relationship-prediciton on this edge-set and see what we end up getting as an output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keen_suggest_links(\n",
    "    triplets: np.array,\n",
    "    model: pipeline,\n",
    "    sample_set: List[List[str]],\n",
    "    top_k: int = 3\n",
    ") -> Tuple[dict,dict]:\n",
    "    \"\"\"\n",
    "    Predicts relationships between entity pairs provided by the NMF function.\n",
    "    Returns two dictionaries of predicted relationships given the (head,tail) pairs.\n",
    "    \"\"\"\n",
    "    # Get the triples factory from the model\n",
    "    triples_factory = TriplesFactory.from_labeled_triples(triplets)\n",
    "\n",
    "    # Predict links for NMF-suggested pairs\n",
    "    pred_links_pd = {}\n",
    "    for head, tail in sample_set:\n",
    "        if head in triples_factory.entity_to_id and tail in triples_factory.entity_to_id:\n",
    "            head_id = triples_factory.entity_to_id[head]\n",
    "            tail_id = triples_factory.entity_to_id[tail]\n",
    "            relation_preds = predict_target(\n",
    "                model=model, \n",
    "                head=head_id, \n",
    "                tail=tail_id,\n",
    "                triples_factory=triples_factory\n",
    "            )\n",
    "            best_relations = relation_preds.df.nlargest(top_k, 'score')\n",
    "            pred_links_pd[(head, tail)] = best_relations\n",
    "        else:\n",
    "            print(f\"Warning: Entity pair ({head}, {tail}) not found in the triples factory.\")\n",
    "\n",
    "    pred_links_np = {key:value.to_numpy() for key,value in pred_links_pd.items()}\n",
    "    \n",
    "    return pred_links_pd, pred_links_np\n",
    "\n",
    "e_pred_links_pd, e_pred_links_np = keen_suggest_links( \n",
    "    triplets = triplets,\n",
    "    model = model.model,\n",
    "    sample_set = possible_edges\n",
    "    )\n",
    "print(\"\\nLinks predicted:\")\n",
    "for key in e_pred_links_pd.keys():\n",
    "    value = e_pred_links_pd[key]\n",
    "    print( f\"head: {key[0]}, tail: {key[1]},\\nrelations:\\n{value}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe one more thing.\n",
    "\n",
    "Okay so we tried out the TransE model, where the value of the tail is given by simple vector composition. And I know we said that we wouldn't do this, but let's *quickly* train a **TransR** model on the same set of data, and see what relations that model suggests. The TransR model is slightly more sophisticated. Instead of *e(u) + e(relation) = e(v)* we simultaneously learn an affine transformation such that *Me(u) + e(relation) = Me(v)*. This can be thought of loosely in the following way. Each relationship in the TransR model has a \"native\" vector space. Before performing prediction we need to ensure that our embeddings are put into that vector space. This gives the model more flexibility at the cost of extra optimization expense. The TransR model is generally better at handling KG's with lots of complicated relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_MODEL_DIRECTORY = \"../data/TransR_model\"\n",
    "\n",
    "r_model = train_model( triplets = triplets, model = 'TransR' )\n",
    "print(\"Model trained...\")\n",
    "model.save_to_directory( R_MODEL_DIRECTORY )\n",
    "print(\"Model saved...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, our model is trained. Now let's do some prediction with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred_links_pd, r_pred_links_np = keen_suggest_links( \n",
    "    triplets = triplets,\n",
    "    model = r_model.model,\n",
    "    sample_set = possible_edges\n",
    "    )\n",
    "print(\"\\nLinks predicted:\")\n",
    "for key in r_pred_links_pd.keys():\n",
    "    value = r_pred_links_pd[key]\n",
    "    print( f\"\\nhead: {key[0]}, tail: {key[1]},\\nrelations:\\n{value}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, this model gives different results. But, how good are these results? This depends on our end goals. There are two different paradigms when discussing knowledge graph completion and fact assessment. These are generally called, the **open-world** and **closed-world** assumptions. Roughly speaking, in **open-world** KG completion, we assume that there are facts outside of our known dataset that may be relevant to our knowledge graph. This is the realm of \"fact-discovery\", and can be used to help guide things like research efforts, or prediction. In **closed-world** KG completion, we only want to include facts that we can reason about directly from our data. This is generally more espensive to enforce, and requires extra evaluations of the proposed triplets. This can be done either by hand, or by machine using some metric or LLM. As an example, e may care about enforcing a closed-world assumption in the realm of legal data, or documentation. \n",
    "\n",
    "Given how broad open-world assumptions can be, we will not speak much to that kind of knowledge graph completion. Instead let us briefly talk about one way we can look to confirm our facts.\n",
    "\n",
    "## Assessing our predictions.\n",
    "\n",
    "There are a few different ways we can assess our suggested edges under a closed-world assumption. This is a tough task since we do not have a reliable ground truth in general. let us consider what is available to us. We have our initial data, we have our knowledge graph, and we have our suggested edges. From these we need to intuit whether suggested edges are sensible. One way we could do this is to trace each fact (RDF triple) from our knowledge graph back to our dataset and cite specific text. This can then be evaluated by a person, or using LLM as a judge. \n",
    "\n",
    "This seems like a good path! However, how do we know what text we should look at? Simply put, we don't. To ensure that we are not missing the *key* documents we would have to check over our whole dataset for each edge. But if we are okay with some approximation we can limit our options if we are clever. We **know** where each edge in our KG came from, and can trace it back to a file. What if we pursued the following flow from data ingestion to assessment?\n",
    "\n",
    "1. Ingest triplets from our dataset and track which files each triplet comes from. \n",
    "2. Turn this into a knowledge graph where each edge has its parent file as metadata.\n",
    "3. Train a TransE, or TransR model as described above on the entire graph.\n",
    "4. **Sample subgraphs of the KG for smaller sets of documents**\n",
    "5. **Perform link prediction and relationship prediction on each subgraph**\n",
    "6. **Evaluate the suggested relationships, with respect to the text in the parent documents only.**\n",
    "\n",
    "The above workflow allows us to search for citations in a much smaller corpus of text, while still allowing us to include information from several documents. So, let's start that process. We already have 1-3 done, so let's start at step 4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraph( graph : MultiDiGraph, file_names : list[str] ) -> MultiDiGraph:\n",
    "    \"\"\"\n",
    "    Using a set of file_names we obtain the sugraph induced by those names.\n",
    "    \"\"\"\n",
    "    filtered_edges = [(u,v) for u,v,data in graph.edges(data=True) if data.get(\"file\") in file_names]\n",
    "\n",
    "    subgraph = nx.MultiDiGraph()\n",
    "    subgraph.add_edges_from(filtered_edges)\n",
    "\n",
    "    for u,v,key in subgraph.edges(keys=True):\n",
    "        subgraph.edges[u,v,key].update(graph.edges[u,v,key])\n",
    "\n",
    "    return( subgraph )\n",
    "\n",
    "# Now let's get our subgraph.\n",
    "file_names = [\"1701732_10K_2020_0001564590-21-009427.json\",\"1441683_10K_2020_0001441683-21-000018.json\"]\n",
    "subgraph = get_subgraph( kg, file_names )\n",
    "print(\"\\nSubgraph obtained...\")\n",
    "print(f\"Number of nodes: {len(subgraph.nodes)}\\nNumber of edges:{len(subgraph.edges)}\")\n",
    "\n",
    "# Let's perform NMF.\n",
    "subgraph_possible_edges = NMF( subgraph, top_k=10 )\n",
    "print(\"\\nNMF performed...\")\n",
    "print(f\"Suggested edges:\\n{subgraph_possible_edges}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We have our subgraph, and the triplets therein. Let's run our TransR model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our relationships using TransR.\n",
    "suggested_subgraph_rels_pd,suggested_subgraph_rels_np = keen_suggest_links( \n",
    "    triplets = triplets, \n",
    "    model = r_model.model, \n",
    "    sample_set = subgraph_possible_edges \n",
    "    )\n",
    "print(\"\\nRelationships evaluated...\")\n",
    "print(\"\\nLinks predicted:\")\n",
    "for key in suggested_subgraph_rels_pd.keys():\n",
    "    value = suggested_subgraph_rels_pd[key]\n",
    "    print( f\"head: {key[0]}, tail: {key[1]},\\nrelations:\\n{value}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have suggested edges for our subgraph, we can go ahead and evaluate if the given triplets seem reasonable given the context. Again, we can do this manually, or using LLM as a judge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up.\n",
    "\n",
    "You now know how to perform edge-prediction on your LLM generated knowledge graph in a way that is both computationally feasible, and encorporates structural and embedding information. This is due to the use of NMF as well as TransE/TransR, ensuring that each suggested edge has been recommended by both. Remember that this is only one way to do this, and we encourage you to try out any number of the suggestions we made during this notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

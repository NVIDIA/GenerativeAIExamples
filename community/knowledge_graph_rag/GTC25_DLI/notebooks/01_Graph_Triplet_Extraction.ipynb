{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b50af05",
   "metadata": {},
   "source": [
    "# Graph Triplet Extraction for Financial Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4dd86",
   "metadata": {},
   "source": [
    "SEC (Securities and Exchange Commission) filings, such as 10-K reports, contain vast amounts of structured and unstructured data about a company's financials, risks, strategies, and operations. Extracting graph triplets from these documents provides several benefits:\n",
    "* Structured Representation: Converts unstructured text into structured knowledge in the form of (subject, relation, object) triplets, making it easier to analyze relationships between entities.\n",
    "* Enhanced Financial Analysis: Enables analysts to identify connections between companies, risks, financial metrics, and market conditions.\n",
    "* Scalability: Automates the extraction process for large volumes of filings across multiple companies and years.\n",
    "* Risk Assessment: Helps uncover hidden risks by linking entities (e.g., \"TAUTACHROME INC.\") to specific risk factors (e.g., \"Market Risk\").\n",
    "* Compliance and Strategy Insights: Identifies regulatory or operational dependencies that can impact business strategies.\n",
    "\n",
    "By extracting graph triplets from SEC documents, we can transform raw text into actionable insights that are easier to query and visualize.\n",
    "\n",
    "This notebook demonstrates how to extract graph triples from SEC 10-K filings using NVIDIA's AI endpoints. The extracted triples are useful for building a GraphRAG (Graph-based Retrieval-Augmented Generation) system, enhancing the knowledge graph with detailed financial information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c06879",
   "metadata": {},
   "source": [
    "## How Does a Knowledge Graph Help with Multiple SEC Documents?\n",
    "A knowledge graph organizes extracted triplets into a connected network of entities and relationships. This structure is particularly valuable for analyzing multiple SEC filings:\n",
    "* Unified View Across Companies:\n",
    "Combines data from multiple filings into a single graph, enabling cross-company comparisons.\n",
    "Example: Identify common risk factors faced by companies in the same industry.\n",
    "* Queryable Relationships:\n",
    "Allows users to query specific relationships (e.g., \"What market risks does a company face?\") without manually sifting through documents.\n",
    "* Interconnected Insights:\n",
    "Links related entities across documents (e.g., subsidiaries, competitors, or shared risks).\n",
    "Example: Connect \"TAUTACHROME INC\" to its financial performance metrics and regulatory obligations.\n",
    "* Scalability for Large Datasets:\n",
    "Handles thousands of filings efficiently by representing them as nodes and edges in a graph.\n",
    "Example: Visualize how different companies are affected by the same regulation or market condition.\n",
    "* Improved Decision-Making:\n",
    "Provides a holistic view of a company's ecosystem, enabling better risk assessment, compliance tracking, and strategic planning.\n",
    "A knowledge graph built from SEC filings transforms disparate data into an interconnected web of insights that can be queried and analyzed at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5bb8d",
   "metadata": {},
   "source": [
    "## What Will You Learn in This Notebook?\n",
    "This notebook demonstrates how to extract graph triplets from SEC filings and build a knowledge graph. By the end of this notebook, you will learn:\n",
    "* Triplet Extraction:\n",
    "How to extract (subject, relation, object) triplets from SEC filings using natural language processing techniques.\n",
    "Example: (\"TAUTACHROME INC\", \"Faces\", \"Market Risk\").\n",
    "* Building a Knowledge Graph:\n",
    "How to construct a knowledge graph from extracted triplets using tools like NetworkX.\n",
    "Relabel nodes with meaningful entity names (e.g., \"TAUTACHROME INC\") and edges with relation names (e.g., \"Faces\").\n",
    "* Querying the Knowledge Graph:\n",
    "How to query the graph for insights using LangChain's GraphQAChain.\n",
    "Example Query: \"What risk factors does TAUTACHROME INC face?\"\n",
    "* Applications of Knowledge Graphs:\n",
    "Learn how knowledge graphs can be used for financial analysis, risk assessment, compliance tracking, and strategic decision-making.\n",
    "\n",
    "\n",
    "By following this notebook, you will gain hands-on experience in transforming raw text from SEC filings into actionable insights through graph-based representations. This markdown provides clear sections explaining the utility of graph triplet extraction for SEC documents, the benefits of knowledge graphs for analyzing multiple documents, and what users will learn in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef276001",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39015b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import argparse\n",
    "import getpass\n",
    "import unicodedata\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611bd4a",
   "metadata": {},
   "source": [
    "Ensure that your NVIDIA API key is set. This key is required to access NVIDIA's AI endpoints, which are used for processing the SEC filings. You can obtain your API key from [NVIDIA's AI portal](https://developer.nvidia.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c491ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NVIDIA API key is set\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate model\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fda17c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sample Dataset\n",
    "For this example, we'll use 2021 SEC documents hosted on [Kaggle](https://www.kaggle.com/datasets/pranjalverma08/sec-edgar-annual-financial-filings-2021).\n",
    "We'll store this data in our data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a3f23",
   "metadata": {},
   "source": [
    "## Define Functions for Preprocessing and Triple Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44b05d",
   "metadata": {},
   "source": [
    "### Preprocess JSON Content\n",
    "\n",
    "This function preprocesses JSON content by decoding Unicode escape sequences and normalizing characters. Preprocessing ensures that the text is in a suitable format for extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f4627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_json_content(json_content):\n",
    "    # Decode Unicode escape sequences\n",
    "    json_content = json_content.encode('utf-8').decode('unicode_escape')\n",
    "    \n",
    "    # Replace escaped newline characters with actual newlines\n",
    "    json_content = json_content.replace('\\\\n', '\\n')\n",
    "    json_content = json_content.replace('\"\\\"', '')\n",
    "    json_content = json_content.replace('\"\"', '\"\"\"')\n",
    "    \n",
    "    # Ensure \\u sequences are treated as string literals\n",
    "    json_content = json_content.replace('\\\\u', '\\\\\\\\u')\n",
    "    \n",
    "    # Normalize Unicode characters\n",
    "    json_content = unicodedata.normalize('NFKD', json_content).encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    return json_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574f2cb",
   "metadata": {},
   "source": [
    "### Preprocess Text,\n",
    "This function preprocesses text by replacing company-specific pronouns with the company name. This step is important for accurate entity recognition and disambiguation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ffb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, company_name):\n",
    "    replacements = {\n",
    "        \" we \": f\" {company_name} \",\n",
    "        \" us \": f\" {company_name} \",\n",
    "        \" our \": f\" {company_name}'s \",\n",
    "        \" the Company \": f\" {company_name} \",\n",
    "        \"We \": f\"{company_name} \",\n",
    "        \"Us \": f\"{company_name} \",\n",
    "        \"Our \": f\"{company_name}'s \",\n",
    "        \"The Company \": f\"{company_name} \"\n",
    "    }\n",
    "    for key, value in replacements.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac7079",
   "metadata": {},
   "source": [
    "### Process Response\n",
    "This function processes the response from the language model, ensuring that the output is properly formatted as a list of graph triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(triplets_str):\n",
    "    try:\n",
    "        # Ensure the string is properly formatted\n",
    "        triplets_str = triplets_str.strip()\n",
    "        \n",
    "        if not triplets_str.startswith(\"[\"):\n",
    "            triplets_str = \"[\" + triplets_str\n",
    "        if not triplets_str.endswith(\"]\"):\n",
    "            triplets_str = triplets_str + \"]\"\n",
    "        \n",
    "        triplets_list = ast.literal_eval(triplets_str)\n",
    "        json_triplets = []\n",
    "        \n",
    "        for triplet in triplets_list:\n",
    "            try:\n",
    "                subject, subject_type, relation, object, object_type = triplet\n",
    "                json_triplet = [subject, subject_type, relation, object, object_type]\n",
    "                json_triplets.append(json_triplet)\n",
    "            except ValueError:\n",
    "                # Skip the malformed triplet and continue with the next one\n",
    "                continue\n",
    "        \n",
    "        return json_triplets\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54de8e",
   "metadata": {},
   "source": [
    "### Extract Triples for Section\n",
    "This function extracts graph triples for a given section of text. It uses Langchain's text splitting and prompt templates to generate triples that depict relationships between entities in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20915e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples_for_section(section_text, company_name, section_name, max_length=16384):\n",
    "    section_text = preprocess_json_content(section_text)\n",
    "    section_text = preprocess_text(section_text, company_name)\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_length,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.create_documents([section_text])\n",
    "    results = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", f\"\"\"You are an investor analyst. Read the SEC 10-K context and generate graph triples that depict the relationships between entities and objects in the context to build a knowledge graph of the 10-K context. \n",
    "            Note that the entities should not be generic, numerical, or temporal (like dates or percentages). Entities must be classified into the following categories:\n",
    "            - ORG: Organizations other than government or regulatory bodies\n",
    "            - ORG/GOV: Government bodies (e.g., \"United States Government\")\n",
    "            - ORG/REG: Regulatory bodies (e.g., \"Securities and Exchange Commission\")\n",
    "            - PERSON: Individuals (e.g., \"John Doe\")\n",
    "            - GPE: Geopolitical entities such as countries, cities, etc. (e.g., \"United States\")\n",
    "            - COMP: Companies (e.g., \"{company_name}\")\n",
    "            - PRODUCT: Products or services (e.g., \"Windows OS\")\n",
    "            - EVENT: Specific and Material Events (e.g., \"Annual Shareholders Meeting\", \"Product Launch\")\n",
    "            - SECTOR: Company sectors or industries (e.g., \"Software Industry\")\n",
    "            - ECON_INDICATOR: Economic indicators (e.g., \"Gross Domestic Product\"), numerical value like \"10%\" is not an ECON_INDICATOR;\n",
    "            - FIN_INSTRUMENT: Financial and market instruments (e.g., \"Bonds\", \"Equity\")\n",
    "            - CONCEPT: Abstract ideas or notions or themes (e.g., \"Market Risk\", \"Innovation\", \"Sustainability\")\n",
    "            - RISK: Specific risks that could impact the company (e.g., \"Market Risk\", \"Credit Risk\", \"Operational Risk\")\n",
    "\n",
    "            The relationships 'r' between these entities must be represented by one of the following relation verbs set: Has, Announce, Operate_In, Introduce, Produce, Control, Participates_In, Impact, Positive_Impact_On, Negative_Impact_On, Relate_To, Is_Member_Of, Invests_In, Raise, Decrease.\n",
    "\n",
    "            Remember to conduct entity disambiguation, consolidating different phrases or acronyms that refer to the same entity (for instance, \"SEC\", \"Securities and Exchange Commission\" should be unified as \"Securities and Exchange Commission\"). Simplify each entity of the triplet to be less than six words.\n",
    "            When we refer to “we,” “us,” “our,” or the “Company,” it should use the entity's name \"{company_name}\".\n",
    "            Do not use dates as entities.\n",
    "\n",
    "            From this text, your output MUST be in python list of tuples with each tuple made up of ['h', 'type', 'r', 'o', 'type'], each element of the tuple is the string, where the relationship 'r' must be in the given relation verbs set above. Only output the list.\n",
    "\n",
    "            As an Example, consider the following SEC 10-K excerpt:\n",
    "                Input: '{company_name} reported a revenue increase of 15% in the software industry. The company announced the launch of Windows 11, which is expected to positively impact its market share.'\n",
    "                OUTPUT: [\n",
    "                    ('{company_name}', 'COMP', 'Report', 'Revenue Increase', 'ECON_INDICATOR'),\n",
    "                    ('{company_name}', 'COMP', 'Operate_In', 'Software Industry', 'SECTOR'),\n",
    "                    ('{company_name}', 'COMP', 'Announce', 'Windows 11', 'PRODUCT'),\n",
    "                    ('Windows 11', 'PRODUCT', 'Positive_Impact_On', 'Market Share', 'FIN_INSTRUMENT')\n",
    "                ]\n",
    "\n",
    "            Another Example, consider the following SEC 10-K excerpt:\n",
    "                Input: 'The company faces significant market risk due to fluctuations in commodity prices. Additionally, there is a credit risk associated with the potential default of key customers.'\n",
    "                OUTPUT: [\n",
    "                    ('{company_name}', 'COMP', 'Face', 'Market Risk', 'RISK'),\n",
    "                    ('{company_name}', 'COMP', 'Face', 'Credit Risk', 'RISK'),\n",
    "                    ('Market Risk', 'RISK', 'Fluctuate', 'Commodity Prices', 'ECON_INDICATOR'),\n",
    "                    ('Credit Risk', 'RISK', 'Associate_With', 'Default of Key Customers', 'EVENT')\n",
    "                ]\n",
    "\n",
    "            INPUT_TEXT: {input}\"\"\"), (\"user\", chunk.page_content)]\n",
    "        )\n",
    "        \n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        response = chain.invoke({\"input\": chunk.page_content})\n",
    "        print(response)\n",
    "        \n",
    "        processed_response = process_response(response)\n",
    "        if processed_response:\n",
    "            results.extend(processed_response)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06963d",
   "metadata": {},
   "source": [
    "## Extract Triples from File\n",
    "This function extracts triples from a given JSON file containing SEC filing data. It processes specific sections and generates triples for each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    company_name = data.get(\"company\", \"\")  # Ensure the company name is available\n",
    "    \n",
    "    # Extract triples for Item 1A\n",
    "    item_1A_text = data.get(\"item_1A\", \"\")\n",
    "    item_1A_triples = extract_triples_for_section(item_1A_text, company_name, \"Item 1A\")\n",
    "    \n",
    "    # Extract triples for Item 7\n",
    "    item_7_text = data.get(\"item_7\", \"\")\n",
    "    item_7_triples = extract_triples_for_section(item_7_text, company_name, \"Item 7\")\n",
    "    \n",
    "    return {\n",
    "        \"filename\": os.path.basename(file_path),\n",
    "        \"company_name\": company_name,\n",
    "        \"Item 1A\": item_1A_triples,\n",
    "        \"Item 7\": item_7_triples\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801cfd91",
   "metadata": {},
   "source": [
    "## Extract graph triples from SEC 10-K documents\n",
    "\n",
    "We're also going to save the results to file in JSON format, organizing the data for further analysis or integration into a knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdac19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(result, output_dir):\n",
    "    # Create a new filename for the output\n",
    "    output_filename = f\"{result['company_name']}_{result['filename'].replace('.json', '')}_triples.txt\"\n",
    "    output_filename = output_filename.replace(\"'\", \"\")  # Remove single quotes from the filename\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Prepare the data to be saved in JSON format\n",
    "    data_to_save = {\n",
    "        \"filename\": result['filename'],\n",
    "        \"item_1a\": result['Item 1A'],\n",
    "        \"item_7\": result['Item 7']\n",
    "    }\n",
    "    \n",
    "    # Save the results to a new file in JSON format\n",
    "    with open(output_filepath, 'w') as outfile:\n",
    "        json.dump(data_to_save, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fcfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples_from_directory(directory):\n",
    "    # Create the triples_10k directory if it doesn't exist\n",
    "    output_dir = os.path.join(directory, \"triples_10k_dir\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get the list of JSON files to process\n",
    "    json_files = [os.path.join(directory, filename) for filename in os.listdir(directory) if filename.endswith(\".json\")]\n",
    "    \n",
    "    # Use concurrent.futures to process files in parallel\n",
    "    results = []\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        futures = {executor.submit(extract_triples_from_file, file_path): file_path for file_path in json_files}\n",
    "        \n",
    "        # Use tqdm to display a progress bar\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing files\"):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                save_results_to_file(result, output_dir)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {e}\")\n",
    "    \n",
    "    return results\n",
    "# triples = extract_triples_from_directory('/workspace/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23138bd1-5629-4cfe-95a3-240db113fc33",
   "metadata": {},
   "source": [
    "## Skip next cell due to time limit of this lab. Feel free to run after the lab to extract new graph triplets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7ca7c-dab9-4114-9246-b36498438df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # SKIP THIS STEP DURING DLI!!!! IF YOU WANT TO RUN CONVERT FROM MARKDOWN CELL TO CODE\n",
    "# # Create the triples_10k directory if it doesn't exist\n",
    "# directory = '/workspace/data'\n",
    "# output_dir = os.path.join(directory, \"triples_10k_dir\")\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Get the list of JSON files to process\n",
    "# json_files = [os.path.join(directory, filename) for filename in os.listdir(directory) if filename.endswith(\".json\")]\n",
    "\n",
    "# # Use concurrent.futures to process files in parallel\n",
    "# all_triples = []\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     futures = {executor.submit(extract_triples_from_file, file_path): file_path for file_path in json_files}\n",
    "\n",
    "#     # Use tqdm to display a progress bar\n",
    "#     for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing files\"):\n",
    "#         try:\n",
    "#             result = future.result()\n",
    "#             all_triples.append(result)\n",
    "#             save_results_to_file(result, output_dir)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file: {e}\")\n",
    "\n",
    "# print(f\"All triples saved to {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f3ae8-721b-41e7-ad99-9cd8adfd4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "directory = '/workspace/data'\n",
    "output_dir = os.path.join(directory, \"triples_10k\")\n",
    "print(output_dir)\n",
    "# Initialize all_triples\n",
    "all_triples = []\n",
    "\n",
    "# Read all the pre-created txt files\n",
    "for filename in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    if filename.endswith(\".txt\"):  # Assuming the extracted triples are stored in txt files\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)  # Load JSON data\n",
    "\n",
    "                # Extract triples from \"item_1a\" and \"item_7\"\n",
    "                item_1a_triples = data.get(\"item_1a\", [])\n",
    "                item_7_triples = data.get(\"item_7\", [])\n",
    "\n",
    "                # Merge extracted triples into all_triples\n",
    "                all_triples.extend(item_1a_triples)\n",
    "                all_triples.extend(item_7_triples)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(all_triples)} triples from {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef239230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique entities (from subject and object) and relationships\n",
    "unique_entities = set()\n",
    "unique_relationships = set()\n",
    "\n",
    "for triplet in all_triples:\n",
    "    if len(triplet) == 5:  # Ensure the triplet is well-formed\n",
    "        subject, _, relation, object_, _ = triplet\n",
    "        unique_entities.add(subject)\n",
    "        unique_entities.add(object_)\n",
    "        unique_relationships.add(relation)\n",
    "\n",
    "# Create mappings for entities and relationships\n",
    "entities = list(unique_entities)\n",
    "relations = list(unique_relationships)\n",
    "\n",
    "entity_to_id = {entity: idx for idx, entity in enumerate(entities)}\n",
    "relation_to_id = {relation: idx for idx, relation in enumerate(relations)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map triples to IDs\n",
    "triples_with_ids = []\n",
    "\n",
    "for triplet in all_triples:\n",
    "    if len(triplet) == 5:\n",
    "        subject, _, relation, object_, _ = triplet\n",
    "        subject_id = entity_to_id.get(subject, -1)  # Use -1 for unknown entities\n",
    "        object_id = entity_to_id.get(object_, -1)  # Use -1 for unknown objects\n",
    "        relation_id = relation_to_id.get(relation, -1)  # Use -1 for unknown relations\n",
    "        triples_with_ids.append((subject_id, relation_id, object_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ead26e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save entities DataFrame\n",
    "entities_df = pd.DataFrame(list(entity_to_id.items()), columns=['entity_name', 'entity_id'])\n",
    "entities_df.to_csv('entities_v1.csv', index=False)\n",
    "\n",
    "# Save relations DataFrame\n",
    "relations_df = pd.DataFrame(list(relation_to_id.items()), columns=['relation_name', 'relation_id'])\n",
    "relations_df.to_csv('relations_v1.csv', index=False)\n",
    "\n",
    "# Save triples DataFrame\n",
    "triples_df = pd.DataFrame(triples_with_ids, columns=['subject_id', 'relation_id', 'object_id'])\n",
    "triples_df.to_csv('triples_v1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e319268",
   "metadata": {},
   "source": [
    "## Accelerated Graph Construction with cuGraph and NetworkX\n",
    "Now that we have our graph triples, we can construct our full knowledge graph for the corpus of 10-K documents.\n",
    "\n",
    "In the next section, we demonstrate how to construct a graph using NetworkX and optionally accelerate it with cuGraph (GPU-accelerated graph analytics library). The graph is built from triples extracted from SEC 10-K filings, where each triple represents a relationship between two entities. This process is useful for creating knowledge graphs that can be queried for insights or used in downstream machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "def load_entities(filename):\n",
    "    all_entities = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            entity, id = row\n",
    "            all_entities[int(id)] = entity\n",
    "    return all_entities\n",
    "\n",
    "def load_relations(filename):\n",
    "    all_relations = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            relation, id = row\n",
    "            all_relations[int(id)] = relation\n",
    "    \n",
    "    return all_relations\n",
    "\n",
    "def get_relation_tuples(all_entities, all_relations, dataset):\n",
    "    # load the data\n",
    "    lines = open(dataset).readlines()\n",
    "    all_tuples = []\n",
    "    for line in lines:\n",
    "        subject, relation, obj = line.strip().split(\"\\t\")\n",
    "        all_tuples.append((all_entities[int(subject)], all_relations[int(relation)], all_entities[int(obj)]))\n",
    "    return all_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Load the triples from the CSV file\n",
    "# Optional line to use NetworkX with cuGraph backend\n",
    "# triples_df = cudf.DataFrame(all_triples, columns=['subject', 'subject_category', 'relationship', 'object', 'object_category'])\n",
    "\n",
    "triples_df = pd.read_csv(\"triples_v1.csv\", names=['subject_id', 'relation_id', 'object_id'])\n",
    "\n",
    "# Load the entities and relations DataFrames\n",
    "entity_df = pd.read_csv(\"entities_v1.csv\", names=['entity_name', 'entity_id'])\n",
    "relations_df = pd.read_csv(\"relations_v1.csv\", names=['relation_name', 'relation_id'])\n",
    "\n",
    "# Create a mapping from IDs to entity names and relation names\n",
    "entity_name_map = entity_df.set_index(\"entity_id\")[\"entity_name\"].to_dict()\n",
    "relation_name_map = relations_df.set_index(\"relation_id\")[\"relation_name\"].to_dict()\n",
    "\n",
    "# Create the graph from the triples DataFrame\n",
    "G = nx.from_pandas_edgelist(\n",
    "    triples_df,\n",
    "    source='subject_id',\n",
    "    target='object_id',\n",
    "    edge_attr='relation_id',\n",
    "    create_using=nx.DiGraph(),\n",
    ")\n",
    "\n",
    "# Relabel the nodes with actual entity names\n",
    "G = nx.relabel_nodes(G, entity_name_map)\n",
    "\n",
    "# Map relation IDs to relation names for edges\n",
    "edge_attributes = nx.get_edge_attributes(G, \"relation_id\")\n",
    "updated_edge_attributes = {\n",
    "    (u, v): relation_name_map[edge_attributes[(u, v)]]\n",
    "    for u, v in G.edges()\n",
    "}\n",
    "nx.set_edge_attributes(G, updated_edge_attributes, \"relation_name\")\n",
    "\n",
    "print(\"Graph constructed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056cfd2d",
   "metadata": {},
   "source": [
    "## Save the knowledge graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph to a GraphML file so it can be visualized in Gephi Lite\n",
    "nx.write_graphml(G, \"sec_knowledge_graph.gml\",)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd451d0",
   "metadata": {},
   "source": [
    "## Sample query the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(graph.get_triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ceffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the graph using LangChain\n",
    "from langchain.chains import GraphQAChain\n",
    "from langchain.indexes.graph import NetworkxEntityGraph\n",
    "graph = NetworkxEntityGraph(G)\n",
    "\n",
    "\n",
    "# llm.invoke(\"hello\")\n",
    "chain = GraphQAChain.from_llm(llm = llm, graph=graph, verbose=True)\n",
    "res = chain.run(\"What risk factors did ZoomInfo Technologies Inc mention? Only use the knowledge graph, do not use your own context.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73955df",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.run(\"What led to revenue growth for BOX Inc? Please use the knowledge graph triples, not your own learning\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45736a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.run(\"What econ indicators were mentioned by Apple Inc?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.run(\"What sectors does Amazon operate in?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.run(\"Were there any risk factors for DATADOG, Inc?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4975a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.run(\"What are the most common risk factors across all companies?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd516274",
   "metadata": {},
   "source": [
    "## Adjusting the PromptTemplate via LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9419049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a custom prompt template\n",
    "custom_prompt_template = \"\"\"\n",
    "You are an expert in financial analysis and knowledge graphs. Use the following knowledge graph triples to answer the user's question.\n",
    "\n",
    "Knowledge Graph Triples:\n",
    "{graph_triples}\n",
    "\n",
    "Note:\n",
    "- Entities in the graph include both subject names (e.g., \"Microsoft\") and their categories (e.g., \"COMP\" for companies).\n",
    "- Always include subject categories in your reasoning.\n",
    "- If you don't know the answer, say \"I don't know.\"\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "# Create a PromptTemplate object\n",
    "custom_prompt = PromptTemplate(\n",
    "    template=custom_prompt_template,\n",
    "    input_variables=[\"graph_triples\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc930cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.graph_qa.base import GraphQAChain\n",
    "\n",
    "# Initialize GraphQAChain with the custom prompt\n",
    "chain = GraphQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    prompt=custom_prompt  # Pass your custom prompt here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed79f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a query using the modified chain\n",
    "res = chain.run(\"What risk factors did Tyler Technologies Inc mention in their 10-K report?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd5d16-92d8-4c48-9430-abda9a92aa0d",
   "metadata": {},
   "source": [
    "## Add custom context retrieval and chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377e9f7-8eee-414c-abaa-c345375ebccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "def extract_triples_for_entity(graph, entity):\n",
    "    \"\"\"Extract triples for the given entity in a simple string format.\"\"\"\n",
    "    if entity not in graph.nodes():\n",
    "        return \"\"\n",
    "    triples = []\n",
    "    for neighbor in graph.neighbors(entity):\n",
    "        if graph.has_edge(entity, neighbor):\n",
    "            edge_attr = graph.get_edge_data(entity, neighbor)\n",
    "            relation = edge_attr.get('relation', '')\n",
    "            triples.append(f\"{entity} -- {relation} --> {neighbor}\")\n",
    "    return \"\\n\".join(triples)\n",
    "\n",
    "# Pre-extract the triples for \"Box, Inc.\"\n",
    "entity = \"Box Inc.\"\n",
    "triples_str = extract_triples_for_entity(G, entity)\n",
    "print(\"Extracted Triples:\")\n",
    "print(triples_str)\n",
    "\n",
    "# Define your custom prompt template\n",
    "custom_prompt_template = \"\"\"\n",
    "You are an expert in financial analysis and knowledge graphs. Use the following knowledge graph triples to answer the user's question.\n",
    "\n",
    "Knowledge Graph Triples:\n",
    "{graph_triples}\n",
    "\n",
    "Note:\n",
    "- Entities in the graph include both subject names (e.g., \"Microsoft\") and their categories (e.g., \"COMP\" for companies).\n",
    "- Always include subject categories in your reasoning.\n",
    "- If you don't know the answer, say \"I don't know.\"\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=custom_prompt_template,\n",
    "    input_variables=[\"graph_triples\", \"question\"]\n",
    ")\n",
    "\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "# Create an LLMChain with the prompt\n",
    "chain = prompt | llm\n",
    "\n",
    "# Use your question as before\n",
    "question = \"What risk factors did Box Inc mention in their 10-K report?\"\n",
    "\n",
    "result = chain.invoke({\"graph_triples\": triples_str, \"question\": question})\n",
    "answer_text = result.content\n",
    "print(\"Answer:\", answer_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

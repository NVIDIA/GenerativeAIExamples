{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 NVIDIA Corporation. All Rights Reserved.\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a671b",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width:60px; float:right\"><br>\n",
    "# <font color=\"#76b900\">**Finetuning LLM for Triplet Prediction**<br/>with NVIDIA NIM microservice</font>\n",
    "\n",
    "**Welcome To Your Cloud Environment!** This interactive web application, which you're currently using to run Python code, is more than just a simple interface. When you access this Jupyter Notebook, an instance on a cloud platform is allocated to you by the [**NVIDIA Deep Learning Institute (DLI)**](https://www.nvidia.com/en-us/training/). This forms your base cloud environment, essentially a blank canvas for further setup, and includes:\n",
    "\n",
    "- A dedicated CPU, and possibly a GPU, for processing.\n",
    "- A pre-installed base operating system.\n",
    "- A pre-installation of packages necessary to run the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339574ad",
   "metadata": {},
   "source": [
    "### Learning Objectives "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa395d75",
   "metadata": {},
   "source": [
    "### **Fine-Tuning a Smaller LLM for Accurate Triplet Predictions**  \n",
    "\n",
    "In this tutorial, we will **fine-tune a smaller Large Language Model (LLM) for more accurate triplet predictions** using [**NVIDIA NeMo**](https://www.nvidia.com/en-in/ai-data-science/products/nemo/) and [**NVIDIA Inference Microservices (NIM)**](https://www.nvidia.com/en-in/ai/).  \n",
    "\n",
    "#### **Introduction to NVIDIA NeMo and NIM**  \n",
    "[NVIDIA NeMo](https://www.nvidia.com/en-in/ai-data-science/products/nemo/) is a **scalable, cloud-native generative AI framework** designed for researchers and developers working with Large Language Models, Multimodal AI, and Speech AI (e.g., Automatic Speech Recognition and Text-to-Speech). It allows users to efficiently create, customize, and deploy generative AI models by leveraging existing code and pre-trained model checkpoints.  \n",
    "\n",
    "[NVIDIA Inference Microservices (NIM)](https://www.nvidia.com/en-in/ai/) is a **suite of microservices** that enables fast and seamless deployment of AI models. NIM can be used on-premises or in **DGX Cloud**, allowing users to transition models to self-managed hosting with minimal code changes. These microservices are designed to scale dynamically based on load and run efficiently on GPUs.  \n",
    "\n",
    "#### **Why Fine-Tune a Smaller LLM?**  \n",
    "Large Language Models (LLMs) are trained for a wide range of tasks. However, for this specific use case, we only need the model to predict **triplets** from given text. Instead of deploying a large LLM, we use **LLM distillation** to train a smaller, more efficient model that retains the accuracy of a larger model while consuming fewer computational resources.  \n",
    "\n",
    "**LLM Distillation** is a process where a **large LLM (teacher model)** is used to train a **smaller LLM (student model)**. The smaller model learns by replicating the teacher’s output, achieving similar accuracy with reduced computational overhead.  \n",
    "\n",
    "While teacher models provide high accuracy, they are resource-intensive. Deploying them for a single task is often inefficient. Instead, a **fine-tuned student model** offers significantly better throughput while meeting business-related performance KPIs.  \n",
    "\n",
    "#### **Tutorial Overview**  \n",
    "In this tutorial, we will fine-tune the **LLaMa-3 8B** model using NVIDIA NeMo and deploy it with NVIDIA NIM. We will cover the following:  \n",
    "\n",
    "- **Dataset Preparation**: How to collect and preprocess data for LLM distillation  \n",
    "- **Fine-Tuning LLaMa-3 8B**: Setting up and fine-tuning the model using NVIDIA NeMo (the model is pre-downloaded, and the necessary Python scripts are provided)  \n",
    "- **Deploying the Fine-Tuned Model**: Using NVIDIA NIM for efficient model deployment  \n",
    "- **Querying the Deployed Model**: Interacting with the model to make predictions  \n",
    "- **Enhancing Accuracy**: Additional techniques to improve model performance  \n",
    "\n",
    "The complete process of fine-tuning and deployment is summarized in the image below:  \n",
    "\n",
    "![](assets/e2e-lora-train-and-deploy.png)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a7331",
   "metadata": {},
   "source": [
    "### Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f3cba-68fe-4099-bb6c-2d94814f4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc304a",
   "metadata": {},
   "source": [
    "**The step defines directories and output file paths used in a data processing pipeline:**\n",
    "\n",
    "1. TRIPLES_DIR: Path to JSON files containing triples for the corresponding file in RAW_JSON_DIR .\n",
    "2. RAW_JSON_DIR: Path to raw JSON files that contain unprocessed sec data.\n",
    "3. OUTPUT_JSONL: Path to save the processed data in JSON Lines (JSONL) format, where each line represents a separate JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac181ee0-3d41-4de2-a82f-8709ed66a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "TRIPLES_DIR = \"/workspace/data/triples_10k\"  # Directory containing triples JSON files\n",
    "RAW_JSON_DIR = \"/workspace/data\"             # Directory containing raw JSON files\n",
    "OUTPUT_JSONL = \"/workspace/data/training_data/output.jsonl\"      # Output file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d296f",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## <font color=\"#76b900\">**1. Dataset for Distillation**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb69b5c",
   "metadata": {},
   "source": [
    "Here's a concise explanation of your code:\n",
    "\n",
    "1. **`clean_text` function**: Cleans the input text by removing extra spaces, tabs, and newlines.\n",
    "\n",
    "2. **`read_raw_json_item_1` function**: Reads the `item_1` key from a specified raw Sec JSON file. It returns an empty string if the file is not found or if there's an error reading the file.\n",
    "\n",
    "3. **`process_triples_and_raw_json` function**:\n",
    "   - It processes files in the `triples_dir` directory (which should contain JSON files with triples data).\n",
    "   - For each triple file, it:\n",
    "     - Reads the corresponding raw JSON file (based on the `filename` field in the triples JSON).\n",
    "     - Cleans the text in the `item_1` field of the raw JSON.\n",
    "     - Cleans and processes the `item_1a` field from the triples file (treated as triples).\n",
    "     - Creates a JSONL entry with `input` as the cleaned `item_1` text and `output` as the cleaned triples.\n",
    "     - Writes each JSONL entry to the `output_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833527d-1fd1-4847-86b4-ad4b2a2fb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing \\n, \\t, extra spaces, non-printable characters, etc.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^\\x20-\\x7E]', '', text)  # Remove non-printable characters\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def read_raw_json_item_1(raw_json_dir, filename):\n",
    "    \"\"\"\n",
    "    Read the 'item_1' key from the specified raw JSON file.\n",
    "    \"\"\"\n",
    "    raw_file_path = os.path.join(raw_json_dir, filename)\n",
    "    if not os.path.exists(raw_file_path):\n",
    "        print(f\"Raw JSON file not found: {raw_file_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    with open(raw_file_path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            return clean_text(data.get(\"item_1\", \"\"))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON: {raw_file_path}\")\n",
    "            return \"\"\n",
    "\n",
    "def process_triples_and_raw_json(triples_dir, raw_json_dir, output_file):\n",
    "    \"\"\"\n",
    "    Process all triple files and corresponding raw JSON files.\n",
    "    Generate JSONL entries with 'input' (item_1 text) and 'output' (cleaned triples).\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        # Iterate through all files in the triples directory\n",
    "        for file_name in os.listdir(triples_dir):\n",
    "            file_path = os.path.join(triples_dir, file_name)\n",
    "\n",
    "            # Process only JSON files\n",
    "            if not file_name.endswith(\".txt\"):\n",
    "                continue\n",
    "\n",
    "            with open(file_path, 'r', encoding='utf-8') as triple_f:\n",
    "                try:\n",
    "                    triple_data = json.load(triple_f)\n",
    "                    \n",
    "                    # Extract raw JSON filename\n",
    "                    raw_json_filename = triple_data.get(\"filename\")\n",
    "                    if not raw_json_filename:\n",
    "                        print(f\"No 'filename' field in {file_name}\")\n",
    "                        continue\n",
    "\n",
    "                    # Read 'item_1' from raw JSON file\n",
    "                    item_1_text = read_raw_json_item_1(raw_json_dir, raw_json_filename)\n",
    "                    # Process triples\n",
    "                    triples = triple_data.get(\"item_1a\", [])\n",
    "                    \n",
    "                    output_triples = clean_text(str(triples))\n",
    "                    # Create JSONL entry\n",
    "                    jsonl_entry = {\n",
    "                        \"input\": item_1_text,\n",
    "                        \"output\": output_triples\n",
    "                    }\n",
    "                    out_f.write(json.dumps(jsonl_entry, ensure_ascii=False) + \"\\n\")\n",
    "                    out_f.flush()\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON file: {file_path}\")\n",
    "                    continue\n",
    "\n",
    "# Run the process\n",
    "process_triples_and_raw_json(TRIPLES_DIR, RAW_JSON_DIR, OUTPUT_JSONL)\n",
    "print(f\"Processing complete. Output saved to {OUTPUT_JSONL}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09683a1a",
   "metadata": {},
   "source": [
    "\n",
    "Below code defines a function to split a JSONL file into training, validation, and test datasets, and then writes the resulting data into separate files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baedb18-56ac-47ea-b05a-f67308881abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/Output File Paths\n",
    "TRAIN_FILE = \"../data/training_data/sec_train.jsonl\"         # Train dataset\n",
    "VALID_FILE = \"../data/training_data/sec_val.jsonl\"           # Validation dataset\n",
    "TEST_FILE = \"../data/training_data/sec_test.jsonl\"           # Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd8cfa-7553-4073-acc6-23c12ea3777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_jsonl(input_file, train_file, valid_file, test_file, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the input JSONL file into train, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    # Read all lines from the input JSONL file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Shuffle the lines randomly to ensure data distribution\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    # Calculate the split indices\n",
    "    total_lines = len(lines)\n",
    "    train_split = int(total_lines * train_ratio)\n",
    "    valid_split = int(total_lines * valid_ratio)\n",
    "\n",
    "    # Split the data\n",
    "    train_data = lines[:train_split]\n",
    "    valid_data = lines[train_split:train_split + valid_split]\n",
    "    test_data = lines[train_split + valid_split:]\n",
    "\n",
    "    # Write the train dataset\n",
    "    with open(train_file, 'w', encoding='utf-8') as train_f:\n",
    "        train_f.writelines(train_data)\n",
    "    print(f\"Train dataset created with {len(train_data)} records: {train_file}\")\n",
    "\n",
    "    # Write the validation dataset\n",
    "    with open(valid_file, 'w', encoding='utf-8') as valid_f:\n",
    "        valid_f.writelines(valid_data)\n",
    "    print(f\"Validation dataset created with {len(valid_data)} records: {valid_file}\")\n",
    "\n",
    "    # Write the test dataset\n",
    "    with open(test_file, 'w', encoding='utf-8') as test_f:\n",
    "        test_f.writelines(test_data)\n",
    "    print(f\"Test dataset created with {len(test_data)} records: {test_file}\")\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Split the JSONL file\n",
    "split_jsonl(OUTPUT_JSONL, TRAIN_FILE, VALID_FILE, TEST_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856abff2-bcc0-44cc-8e39-b9eb92baddf6",
   "metadata": {},
   "source": [
    "Applying additional cleaning function as malformed/bad json in jsonl often found to halt training midway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929e07d-4f34-4145-9836-a10b7777a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_jsonl(input_file, output_file):\n",
    "    \"\"\"Sanitizes JSONL file by fixing bad lines.\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line_number, line in enumerate(infile, 1):\n",
    "            try:\n",
    "                # Try parsing JSON line\n",
    "                json_data = json.loads(line)\n",
    "                # Write cleaned JSON\n",
    "                outfile.write(json.dumps(json_data, ensure_ascii=False) + '\\n')\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping malformed line {line_number}: {line.strip()}\")\n",
    "\n",
    "# Example usage\n",
    "sanitize_jsonl(\"/workspace/data/training_data/sec_val.jsonl\", \"/workspace/data/training_data/sec_val_clean.jsonl\")\n",
    "sanitize_jsonl(\"/workspace/data/training_data/sec_test.jsonl\", \"/workspace/data/training_data/sec_test_clean.jsonl\")\n",
    "sanitize_jsonl(\"/workspace/data/training_data/sec_train.jsonl\", \"/workspace/data/training_data/sec_train_clean.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d827a",
   "metadata": {},
   "source": [
    "In knowledge distillation, models are fine-tuned using labeled datasets to improve their performance on specific tasks. Task-specific fine-tuning enhances response quality and helps overcome the limitations of the student model. During this process, the model is trained over multiple iterations on labeled data to refine its predictions.\n",
    "\n",
    "For fine-tuning with NVIDIA NeMo, labeled data must be provided in JSON Lines (JsonL) format. JsonL is a convenient format for storing structured data, allowing for efficient processing of records one at a time.\n",
    "\n",
    "Typically following format is used when doing finetuning with NeMo:\n",
    "\n",
    "```json\n",
    "{\"input\": \"Sample input text\", \"output\": \"Expected model response\"}\n",
    "{\"input\": \"Another example input\", \"output\": \"Corresponding expected output\"}\n",
    "```\n",
    "In the case of finetunint model for the triplet extraction the labelled data looks as given below:\n",
    "\n",
    "```json\n",
    "{\"input\": \"ITEM 1. BUSINESS ImageWare Systems, Inc., a Delaware corporation, has its principal place of business at 11440 West Bernardo Court, Suite 300, San Diego, California 92127. We maintain a corporate website at www.iwsinc.com. Our common stock, par value $0.01 per share (\\u201cCommon Stock\\u201d), is currently listed for quotation on the OTCQB marketplace under the symbol \\u201cIWSY\\u201d. As used in this Annual Report, \\u201cwe\\u201d, \\u201cus\\u201d, \\u201cour\\u201d, \\u201cImageWare\\u201d, \\u201cImageWare Systems\\u201d or the \\u201cCompany\\u201d refers to ImageWare Systems, Inc. and all of its subsidiaries. Overview ImageWare Systems, Inc. (\\u201cImageWare,\\u201d the \\u201cCompany,\\u201d \\u201cwe,\\u201d \\u201cour\\u201d) provides defense-grade biometric identification and authentication solutions to safeguard your data, products, services or facilities. We are experts in biometric authentication and considered a preeminent patent holder of multimodal biometrics IP, having many of the most-cited patents in the industry. Our patented IWS Biometric Engine\\u00ae is one of the most accurate and fastest biometrics matching engines in the industry, capable of our patented biometrics fusion. Part of our heritage is in law enforcement, having built the first statewide digital booking platform for United States local law enforcement in the late 1990\\u2019s - and having more than three decades of experience in the challenging government sector creating biometric smart cards and logical access for millions of individuals. We are a \\u201cbiometrics first\\u201d company, leveraging unique human characteristics to provide unparalleled accuracy for identification while protecting your identity. The Company\\u2019s products also provide law enforcement and public safety sector with integrated biographic, mugshot, SMT, and fingerprint capture for booking, in addition to investigative capabilities. The Company also provides comprehensive authentication security software using biometrics to secure physical and logical access to facilities, computer networks or Internet sites. Biometric technology is now an integral part of all markets that the Company addresses, and every product leverages our patented IWS Biometric Engine\\u00ae. The IWS Biometric Engine\\u00ae is a patented biometric identity and authentication database built for multi-biometric enrollment, management and authentication. It is hardware agnostic and can utilize different types of biometric algorithms. It allows different types of biometrics to be operated at the same time on a seamlessly integrated platform. It is also offered as a Software Development Kit (\\u201cSDK\\u201d), enabling developers and system integrators to implement biometric solutions or integrate biometric capabilities, into existing applications. Our secure credential solutions empower customers to design and create smart digital identification wristbands and badges for access control systems. We develop, sell and support software and design systems that utilize digital imaging and biometrics for photo identification cards, credentials and identification systems. Our products in this market consist of IWS EPI Suite and IWS EPI Builder. These products allow for production of digital identification badges and related databases and records and can be used by, among others, schools, airports, hospitals, corporations and governments. .....................\", \"output\": \"[['IMAGEWARE SYSTEMS INC', 'COMP', 'Have', 'Insufficient Cash Resources', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Need', 'Additional Capital', 'FIN_INSTRUMENT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Operate_In', 'Identity Management Solutions Industry', 'SECTOR'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Face', 'Competition', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Face', 'Fluctuating Operating Results', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Depends_Upon', 'Large System Sales', 'PRODUCT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Have', 'Lengthy Sales Cycle', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Have', 'Negative Working Capital', 'ECON_INDICATOR'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Sell', 'Products to Government Agencies', 'GPE'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Rely_On', 'Systems Integrators', 'ORG'], ['Systems Integrators', 'ORG', 'Perform', 'Adequately', 'VERB'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Have', 'Accumulated Deficit', 'ECON_INDICATOR'], ['IMAGEWARE SYSTEMS INC', 'COMP', ' experience', 'Fluctuations in Operating Results', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Subject_To', 'Penny Stock Regulations', 'FIN_INSTRUMENT'], ['Penny Stock Regulations', 'FIN_INSTRUMENT', 'Impose', 'Additional Sales Practice Requirements', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Have', 'Foreign Operations', 'COMP'], ['Foreign Operations', 'COMP', 'Expose', 'Foreign Political Risks', 'CONCEPT'], ['Foreign Operations', 'COMP', 'Expose', 'Foreign Economic Risks', 'CONCEPT'], ['Foreign Operations', 'COMP', 'Expose', 'Foreign Legal Risks', 'CONCEPT'], ['Foreign Operations', 'COMP', 'Expose', 'Foreign Currency Exchange Rates', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Have', 'Foreign Operations', 'COMP'], ['Foreign Operations', 'COMP', 'Affect', 'Results', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Subject_To', 'Income Taxes', 'CONCEPT'], ['Income Taxes', 'CONCEPT', 'Requires', 'Significant Judgments', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Subject_To', 'Income Taxes', 'CONCEPT'], ['Income Taxes', 'CONCEPT', 'Subject_To', 'Examinations', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Exposed_To', 'Foreign Political Risks', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Exposed_To', 'Foreign Economic Risks', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Exposed_To', 'Foreign Legal Risks', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Exposed_To', 'Foreign Currency Exchange Rates', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Have', 'Foreign Operations', 'COMP'], ['Foreign Operations', 'COMP', 'Affect', 'Results', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Face', 'Penny Stock Rules', 'FIN_INSTRUMENT'], ['Penny Stock Rules', 'FIN_INSTRUMENT', 'Affect', 'Market Liquidity', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Face', 'Volatility', 'CONCEPT'], ['Volatility', 'CONCEPT', 'Affect', 'Investment Value', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Face', 'Fluctuations', 'CONCEPT'], ['Fluctuations', 'CONCEPT', 'Cause', 'Decline in Value', 'CONCEPT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Have', 'Common Stock', 'FIN_INSTRUMENT'], ['IMAGEWARE SYSTEMS INC', 'COMP', 'Face', 'Specific Factors', 'CONCEPT'], ['Specific Factors', 'CONCEPT', 'Affect', 'Market Price', 'CONCEPT'], ,,,,,,,,,,]\"}\n",
    "\n",
    "```\n",
    "\n",
    "In our case the \"input\" key will be text which is given as input and \"output\" key will be the triplets predicted by Teacher model. \n",
    "To construct the current data set we have used Mixtral8x7B as a teacher. We have used SEC-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d21172",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## <font color=\"#76b900\">**2. Finetuning**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6a526",
   "metadata": {},
   "source": [
    "[Llama 3](https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/) is an open-source large language model by Meta that delivers state-of-the-art performance on popular industry benchmarks. It has been pretrained on over 15 trillion tokens, and supports an 8K token context length. It is available in two sizes, 8B and 70B, and each size has two variants---base pretrained and instruction tuned.\n",
    "\n",
    "[Low-Rank Adaptation (LoRA)](https://arxiv.org/pdf/2106.09685) has emerged as a popular Parameter-Efficient Fine-Tuning (PEFT) technique that tunes a very small number of additional parameters as compared to full fine-tuning, thereby reducing the compute required.\n",
    "\n",
    "[NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html) provides tools to perform LoRA on Llama 3 to fit your use case, which can then be deployed using [NVIDIA NIM](https://www.nvidia.com/en-us/ai/) for optimized inference on NVIDIA GPUs.\n",
    "\n",
    "This notebook shows how to perform LoRA PEFT on Llama 3 8B Instruct using SEC-10 with NeMo Framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd2167",
   "metadata": {},
   "source": [
    "## Download the base model \n",
    "The first set of commands creates a directory to store the Llama-3-8B-Instruct model file if it doesn’t already exist. The second set of commands downloads the model file (8b_instruct_nemo_bf16.nemo) from NVIDIA's NGC server using requests and saves it in the newly created directory. The third set of commands verifies the successful download by listing the contents of the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7974d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../model/llama-3-8b-instruct-nemo_v1.0\"\n",
    "file_path = os.path.join(directory, \"8b_instruct_nemo_bf16.nemo\")\n",
    "url = \"https://api.ngc.nvidia.com/v2/models/org/nvidia/team/nemo/llama-3-8b-instruct-nemo/1.0/files?redirect=true&path=8b_instruct_nemo_bf16.nemo\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "# Create directory if not exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "def download_progress(block_num, block_size, total_size):\n",
    "    downloaded = block_num * block_size\n",
    "    percent = min(100, downloaded * 100 / total_size)\n",
    "    print(f\"\\rDownloading: {percent:.2f}% ({downloaded}/{total_size} bytes)\", end=\"\")\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"File not found. Downloading...\")\n",
    "    urllib.request.urlretrieve(url, file_path, reporthook=download_progress)\n",
    "    print(\"\\nDownload complete.\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping download.\")\n",
    "\n",
    "\n",
    "# List directory contents\n",
    "print(\"Directory contents:\", os.listdir(directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e442d",
   "metadata": {},
   "source": [
    "## Check GPU availability for training \n",
    "The command docker exec containerB nvidia-smi runs the nvidia-smi tool inside the containerB container to display GPU status. Ensure the container has GPU access (--gpus all) and the NVIDIA drivers installed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker exec containerB nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a1d71",
   "metadata": {},
   "source": [
    "NeMo framework (Current environment) includes a high level python script for fine-tuning megatron_gpt_finetuning.py that can abstract away some of the lower level API calls. Once you have your model downloaded and the dataset ready, LoRA fine-tuning with NeMo is essentially just running this script!\n",
    "\n",
    "For this demonstration, this training run is capped at 20 max steps, and validation is carried out every 10 steps. You may increase the steps to 10,000+ in practical scenarios, but currently in interest of time we have limited the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf823e0",
   "metadata": {},
   "source": [
    "This will create a LoRA adapter - a file named `megatron_gpt_peft_lora_tuning.nemo` in `/workspace/model/Meta-Llama-3-8B-Instruct-Sec-LoRA` We'll use this later.\n",
    "\n",
    "`trainer.max_steps` are capped at 20 iteration to save time and treat it as learning example. Typically finetuning is done on 8xH100 kind of setup and often require 10,000+ steps. \n",
    "\n",
    "The `peft.peft_scheme` parameter determines the technique being used. In this case, we did LoRA, but NeMo Framework supports other techniques as well - such as P-tuning, Adapters, and IA3. For more information, refer to the [PEFT support matrix](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/nemo_megatron/peft/landing_page.html). For example, for P-tuning, simply set `model.peft.peft_scheme=\"ptuning\"  # instead of \"lora\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff385b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker exec containerB bash -c \"\n",
    "    MODEL='/workspace/model/llama-3-8b-instruct-nemo_v1.0/8b_instruct_nemo_bf16.nemo'\n",
    "    TRAIN_DS='/workspace/data/training_data/sec_train_clean.jsonl'\n",
    "    VALID_DS='/workspace/data/training_data/sec_val_clean.jsonl'\n",
    "    TEST_DS='/workspace/data/training_data/sec_test_clean.jsonl'\n",
    "    TEST_NAMES='[sec]'\n",
    "    SCHEME='lora'\n",
    "    TP_SIZE=1\n",
    "    PP_SIZE=1\n",
    "    OUTPUT_DIR='/workspace/model/Meta-Llama-3-8B-Instruct-Sec-LoRA'\n",
    "    rm -rf \\${OUTPUT_DIR}\n",
    "    \n",
    "    torchrun --nproc_per_node=1 /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "        exp_manager.exp_dir=\\${OUTPUT_DIR} \\\n",
    "        exp_manager.explicit_log_dir=\\${OUTPUT_DIR} \\\n",
    "        trainer.devices=1 \\\n",
    "        trainer.num_nodes=1 \\\n",
    "        trainer.precision=bf16-mixed \\\n",
    "        trainer.val_check_interval=5 \\\n",
    "        trainer.max_steps=20 \\\n",
    "        model.megatron_amp_O2=True \\\n",
    "        ++model.mcore_gpt=True \\\n",
    "        model.tensor_model_parallel_size=\\${TP_SIZE} \\\n",
    "        model.pipeline_model_parallel_size=\\${PP_SIZE} \\\n",
    "        model.micro_batch_size=1 \\\n",
    "        model.global_batch_size=8 \\\n",
    "        model.restore_from_path=\\${MODEL} \\\n",
    "        model.data.train_ds.num_workers=0 \\\n",
    "        model.data.validation_ds.num_workers=0 \\\n",
    "        model.data.train_ds.file_names=[\\${TRAIN_DS}] \\\n",
    "        model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "        model.data.validation_ds.file_names=[\\${VALID_DS}] \\\n",
    "        model.peft.peft_scheme=\\${SCHEME}\n",
    "    \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e08f2",
   "metadata": {},
   "source": [
    "Transfer the finetuned LORA adapter to directory where NIM can load and make model avaialble for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3195c0-92f0-4ecb-8228-271f99feba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../model/loras/Meta-Llama-3-8B-Instruct-Sec-LoRA \n",
    "!cp ../model/Meta-Llama-3-8B-Instruct-Sec-LoRA/checkpoints/megatron_gpt_peft_lora_tuning.nemo ../model/loras/Meta-Llama-3-8B-Instruct-Sec-LoRA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c3da8",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## <font color=\"#76b900\">**3. Deploy LoRA Inference Adapters with NVIDIA NIM**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35114dc9-77cd-46d8-bba8-575847c22c0c",
   "metadata": {},
   "source": [
    "### Run the container with following env variables \n",
    "___\n",
    "Below given steps are just for your information and not required to be executed right now as we have already set an environment for you\n",
    "___\n",
    "\n",
    "### Details on how this container was run \n",
    "\n",
    "1.  Download the example LoRA adapters.\n",
    "\n",
    "The following steps assume that you have authenticated with NGC and downloaded the CLI tool, as listed in the Requirements section.\n",
    "\n",
    "```source-shell\n",
    "# Set path to your LoRA model store\n",
    "export LOCAL_PEFT_DIRECTORY=\"$(pwd)/loras\"\n",
    "```\n",
    "\n",
    "```source-shell\n",
    "mkdir -p $LOCAL_PEFT_DIRECTORY\n",
    "pushd $LOCAL_PEFT_DIRECTORY\n",
    "\n",
    "# downloading NeMo-format loras\n",
    "ngc registry model download-version \"nim/meta/llama3-8b-instruct-lora:nemo-math-v1\"\n",
    "ngc registry model download-version \"nim/meta/llama3-8b-instruct-lora:nemo-squad-v1\"\n",
    "\n",
    "popd\n",
    "chmod -R 777 $LOCAL_PEFT_DIRECTORY\n",
    "```\n",
    "\n",
    "1.  Prepare the LoRA model store.\n",
    "\n",
    "After training is complete, that LoRA model checkpoint will be created at `./results/Meta-Llama-3-8B-Instruct/checkpoints/megatron_gpt_peft_lora_tuning.nemo`, assuming default paths in the first notebook weren't modified.\n",
    "\n",
    "To ensure the model store is organized as expected, create a folder named `llama3-8b-pubmed-qa`, and move your `.nemo` checkpoint there.\n",
    "\n",
    "```source-shell\n",
    "mkdir -p $LOCAL_PEFT_DIRECTORY/llama3-8b-pubmed-qa\n",
    "\n",
    "# Ensure the source path is correct\n",
    "cp ./results/Meta-Llama-3-8B-Instruct/checkpoints/megatron_gpt_peft_lora_tuning.nemo $LOCAL_PEFT_DIRECTORY/llama3-8b-pubmed-qa\n",
    "```\n",
    "\n",
    "Ensure that the LoRA model store directory follows this structure: the model name(s) should be sub-folder(s) containing the `.nemo` file(s).\n",
    "\n",
    "```\n",
    "<$LOCAL_PEFT_DIRECTORY>\n",
    "├── llama3-8b-instruct-lora_vnemo-math-v1\n",
    "│   └── llama3_8b_math.nemo\n",
    "├── llama3-8b-instruct-lora_vnemo-squad-v1\n",
    "│   └── llama3_8b_squad.nemo\n",
    "└── llama3-8b-pubmed-qa\n",
    "    └── megatron_gpt_peft_lora_tuning.nemo\n",
    "```\n",
    "\n",
    "The last one was just trained on the PubmedQA dataset in the previous notebook.\n",
    "\n",
    "1.  Set-up NIM.\n",
    "\n",
    "From your host OS environment, start the NIM docker container while mounting the LoRA model store, as follows:\n",
    "\n",
    "```source-shell\n",
    "# Set these configurations\n",
    "export NGC_API_KEY=<YOUR_NGC_API_KEY>\n",
    "export NIM_PEFT_REFRESH_INTERVAL=3600  # (in seconds) will check NIM_PEFT_SOURCE for newly added models in this interval\n",
    "export NIM_CACHE_PATH=</path/to/NIM-model-store-cache>  # Model artifacts (in container) are cached in this directory\n",
    "```\n",
    "\n",
    "```source-shell\n",
    "mkdir -p $NIM_CACHE_PATH\n",
    "chmod -R 777 $NIM_CACHE_PATH\n",
    "\n",
    "export NIM_PEFT_SOURCE=/home/nvs/loras # Path to LoRA models internal to the container\n",
    "export CONTAINER_NAME=meta-llama3-8b-instruct\n",
    "\n",
    "docker run -it --rm --name=$CONTAINER_NAME\\\n",
    "    --runtime=nvidia\\\n",
    "    --gpus all\\\n",
    "    --shm-size=16GB\\\n",
    "    -e NGC_API_KEY\\\n",
    "    -e NIM_PEFT_SOURCE\\\n",
    "    -e NIM_PEFT_REFRESH_INTERVAL\\\n",
    "    -v $NIM_CACHE_PATH:/opt/nim/.cache\\\n",
    "    -v $LOCAL_PEFT_DIRECTORY:$NIM_PEFT_SOURCE\\\n",
    "    -p 8000:8000\\\n",
    "    nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n",
    "```\n",
    "\n",
    "The first time you run the command, it will download the model and cache it in `$NIM_CACHE_PATH` so subsequent deployments are even faster. There are several options to configure NIM other than the ones listed above. You can find a full list in the [NIM configuration](https://docs.nvidia.com/nim/large-language-models/latest/configuration.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b1aa5c",
   "metadata": {},
   "source": [
    "To help interface with this framework, the [**langchain-nvidia-ai-endpoints package**](https://github.com/langchain-ai/langchain-nvidia) provides connectors like [**`ChatNVIDIA`** ](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/langchain_nvidia_ai_endpoints/chat_models.py) and [**`NVIDIAEmbeddings`** ](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/langchain_nvidia_ai_endpoints/embeddings.py) to help interface with the raw endpoints. These will be used throughout the course to power our RAG pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1742a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## <font color=\"#76b900\">**4. Querying LoRA for Inference**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6fdca9",
   "metadata": {},
   "source": [
    "### Check available LoRA models\n",
    "Once the NIM server is up and running, check the available models as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f28a39-0d9d-49b9-954d-6031ac99a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker restart containerC\n",
    "!sleep 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://containerC:8000/v1/models'  # Use containerC's name as the hostname\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b2147",
   "metadata": {},
   "source": [
    "### Query the LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19adc02c",
   "metadata": {},
   "source": [
    "Create a prompt template ; Idelly this should be the same as training template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267d642-c861-4710-b378-648ec880c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the PubMedQA test set\n",
    "def get_prompt(news_prompt):    \n",
    "        master_prompt = f\"\"\"\n",
    "                \n",
    "                    Note that the entities should not be generic, numerical or temporal (like dates or percentages).  Entities must be classified into the following categories:\n",
    "                    ORG: Organizations other than government or regulatory bodies\n",
    "                    ORG/GOV: Government bodies (e.g., \"United States Government\")\n",
    "                    ORG/REG: Regulatory bodies (e.g., \"Federal Reserve\")\n",
    "                    PERSON: Individuals (e.g., \"Elon Musk\")\n",
    "                    GPE: Geopolitical entities such as countries, cities, etc. (e.g., \"Germany\")\n",
    "                    COMP: Companies (e.g., \"Google\")\n",
    "                    PRODUCT: Products or services (e.g., \"iPhone\")\n",
    "                    EVENT: Specific and Material Events (e.g., \"Olympic Games\", \"Covid-19\")\n",
    "                    SECTOR: Company sectors or industries (e.g., \"Technology sector\")\n",
    "                    ECON_INDICATOR: Economic indicators (e.g., \"Inflation rate\"), numerical value like \"10%\" is not a ECON_INDICATOR;\n",
    "                    FIN_INSTRUMENT: Financial and market instruments (e.g., \"Stocks\", \"Global Markets\")\n",
    "                    CONCEPT: Abstract ideas or notions or themes (e.g., \"Inflation\", \"AI\", \"Climate Change\")\n",
    "                    The relationships 'r' between these entities must be represented by one of the following relation verbs set: Has, Announce, Operate_In, Introduce, Produce, Control, Participates_In, Impact, Positive_Impact_On, Negative_Impact_On, Relate_To, Is_Member_Of, Invests_In, Raise, Decrease.\n",
    "                    Remember to conduct entity disambiguation, consolidating different phrases or acronyms that refer to the same entity (for instance,  \"UK Central Bank\", \"BOE\" and \"Bank of England\" should be unified as \"Bank of England\"). Simplify each entity of the triplet to be less than four words.  \n",
    "                    \n",
    "                    From this text, your output Must be in python lis tof tuple with each tuple made up of ['h', 'type', 'r', 'o', 'type'], each element of the tuple is the string, where the relationship 'r' must be in the given relation verbs set above. Only output the list. \n",
    "                    As an Example, consider the following news excerpt: \n",
    "                        Input :'Apple Inc. is set to introduce the new iPhone 14 in the technology sector this month. The product's release is likely to positively impact Apple's stock value.'\n",
    "                        OUTPUT : ```\n",
    "                            [('Apple Inc.', 'COMP', 'Introduce', 'iPhone 14', 'PRODUCT'),\n",
    "                            ('Apple Inc.', 'COMP', 'Operate_In', 'Technology Sector', 'SECTOR'),\n",
    "                            ('iPhone 14', 'PRODUCT', 'Positive_Impact_On', 'Apple's Stock Value', 'FIN_INSTRUMENT')]\n",
    "                        ```\n",
    "                        The output structure must not be anything apart from above OUTPUT structure.\n",
    "                \n",
    "                    INPUT_TEXT:\n",
    "                    \"\"\" + news_prompt.replace(\"\\n\",\".\")[:4192] +\"[/INST]\"\n",
    "        return master_prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'http://containerC:8000/v1/completions'\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "test_line=open(\"../data/training_data/sec_test_clean.jsonl\",\"r\").readline()\n",
    "prompt=json.loads(test_line)\n",
    "input_ = prompt['input']\n",
    "output_ = prompt['output']\n",
    "\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"model\": \"Meta-Llama-3-8B-Instruct-Sec-LoRA\",\n",
    "    \"prompt\": get_prompt(input_),\n",
    "    \"max_tokens\": 256\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response_data = response.json()\n",
    "print(response_data)\n",
    "pprint(\"Predicted output \\n ++++++++++++++++++++++++++++++++++++++++ \\n\" +response_data[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53f29e-d04b-454d-b4d8-6ebf782643fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(\"Actual output \\n ++++++++++++++++++++++++++++++++++++++++ \\n\" + prompt['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

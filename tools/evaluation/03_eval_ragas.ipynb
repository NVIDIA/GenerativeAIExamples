{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d208cc-206a-4526-a061-489b0af46adc",
   "metadata": {},
   "source": [
    "# Notebook 3: Evaluation with Ragas\n",
    "\n",
    "\n",
    "Leveraging a strong LLM for reference-free evaluation is an upcoming solution that has shown a lot of promise. They correlate better with human judgment than traditional metrics and also require less human annotation. Papers like G-Eval have experimented with this and given promising results but there are certain shortcomings too.\n",
    "\n",
    "LLM prefers their own outputs and when asked to compare between different outputs the relative position of those outputs matters more. LLMs can also have a bias toward a value when asked to score given a range and they also prefer longer responses.\n",
    "\n",
    "[Ragas](https://docs.ragas.io/en/latest/) aims to work around these limitations of using LLMs to evaluate your QA pipelines while also providing actionable metrics using as little annotated data as possible, cheaper, and faster.\n",
    "\n",
    "In this notebook, we will use NVIDIA AI playground's  Llama 70B LLM as a judge and eval model. **NVIDIA AI Playground** on NGC allows developers to experience state of the art LLMs accelerated on NVIDIA DGX Cloud with NVIDIA TensorRT nd Triton Inference Server. Developers get **free credits for 10K requests** to any of the available models. Sign up process is easy. Follow the instructions [here.](../docs/rag/aiplayground.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b765c93-e4bc-4918-b389-63b413912e0b",
   "metadata": {},
   "source": [
    "### Step 1: Set NVIDIA AI Playground API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e39b77-8e68-458e-88f6-72b50f24e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NVAPI_KEY'] = \"nvapi-*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ff59e-cd9e-4f42-833b-de4aec02c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_aiplay import GeneralLLM\n",
    "llm = GeneralLLM(\n",
    "    model=\"llama2_70b\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057f67e-a0c0-4d8d-971c-b470a6df6475",
   "metadata": {},
   "source": [
    "### Bring your own LLMsÂ¶\n",
    "Ragas uses langchain under the hood for connecting to LLMs for metrices that require them. This means you can swap out the default LLM (gpt-3.5) with llama2 70B from AI playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab053ae-f7a5-476a-be30-b590d3f4484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLM\n",
    "nvpl_llm = LangchainLLM(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5adab9-cdc1-4ae7-ba83-a74afc734b36",
   "metadata": {},
   "source": [
    "### Step 2: Import Eval Data and Reformat It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee3b485-3ace-466c-92fc-2a5347cebaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('eval.json', 'r') as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40269987-bfba-49c2-bb75-726ce96d8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "eval_answers = []\n",
    "ground_truths = []\n",
    "vdb_contexts = []\n",
    "counter = 0\n",
    "for entry in json_data:\n",
    "    eval_questions.append(entry[\"question\"])\n",
    "    eval_answers.append(entry[\"answer\"])\n",
    "    vdb_contexts.append(entry[\"contexts\"])\n",
    "    ground_truths.append([entry[\"gt_answer\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e175a36-c26b-43b9-9879-be4bb5c7048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = {\n",
    "    'question': eval_questions,\n",
    "    'answer': eval_answers,\n",
    "    'contexts' : vdb_contexts,\n",
    "    'ground_truths': ground_truths\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4c1dd-0e3f-4c3c-afc5-75005a424ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b6495-a66b-4120-a6bf-4c344e6513bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import faithfulness, context_precision\n",
    "\n",
    "faithfulness.llm = nvpl_llm\n",
    "context_precision.llm = nvpl_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79bb50-49cc-4e30-a6f6-e61e754af420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(dataset, metrics=[faithfulness, context_precision])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e56fb-3794-48b2-a4ce-8900ee633f13",
   "metadata": {},
   "source": [
    "### Step 3: View and Interpret Results\n",
    "\n",
    "A Ragas score is comprised of the following:\n",
    "![ragas](imgs/ragas.png)\n",
    "\n",
    "#### Metrics explained \n",
    "1. **Faithfulness**: measures the factual accuracy of the generated answer with the context provided. This is done in 2 steps. First, given a question and generated answer, Ragas uses an LLM to figure out the statements that the generated answer makes. This gives a list of statements whose validity we have we have to check. In step 2, given the list of statements and the context returned, Ragas uses an LLM to check if the statements provided are supported by the context. The number of correct statements is summed up and divided by the total number of statements in the generated answer to obtain the score for a given example.\n",
    "   \n",
    "2. **Answer Relevancy**: measures how relevant and to the point the answer is to the question. For a given generated answer Ragas uses an LLM to find out the probable questions that the generated answer would be an answer to and computes similarity to the actual question asked.\n",
    "   \n",
    "3. **Context Relevancy**: measures the signal-to-noise ratio in the retrieved contexts. Given a question, Ragas calls LLM to figure out sentences from the retrieved context that are needed to answer the question. A ratio between the sentences required and the total sentences in the context gives you the score\n",
    "\n",
    "4. **Context Recall**: measures the ability of the retriever to retrieve all the necessary information needed to answer the question. Ragas calculates this by using the provided ground_truth answer and using an LLM to check if each statement from it can be found in the retrieved context. If it is not found that means the retriever was not able to retrieve the information needed to support that statement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

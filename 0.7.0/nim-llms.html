<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using NVIDIA NIM for LLMs &mdash; NVIDIA Generative AI Examples 24.6.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/version.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Developing Simple Examples" href="simple-examples.html" />
    <link rel="prev" title="Configuring an Alternative Vector Database" href="vector-database.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-catalog.html">API Catalog Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector-database.html">Alternative Vector Database</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">NIM for LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="simple-examples.html">Developing Simple Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tools/evaluation/index.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/01_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA API Catalog with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA API Catalog, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/04_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Catelog and AI Catalog with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/05_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/06_LangGraph_HandlingAgent_IntermediateSteps.html">LangGraph Handling LangChain Agent Intermediate_Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Chat_with_nvidia_financial_reports.html">Notebook: Chatting with NVIDIA Financial Reports</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_RAG_Langchain_with_Local_NIM.html">Build a RAG using a locally hosted NIM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Using NVIDIA NIM for LLMs</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
  SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  SPDX-License-Identifier: Apache-2.0

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<section class="tex2jax_ignore mathjax_ignore" id="using-nvidia-nim-for-llms">
<h1>Using NVIDIA NIM for LLMs<a class="headerlink" href="#using-nvidia-nim-for-llms" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#running-examples-on-nim-for-llms" id="id1">Running Examples on NIM for LLMs</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id2">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#build-and-start-the-containers" id="id3">Build and Start the Containers</a></p></li>
<li><p><a class="reference internal" href="#stopping-the-containers" id="id4">Stopping the Containers</a></p></li>
<li><p><a class="reference internal" href="#related-information" id="id5">Related Information</a></p></li>
<li><p><a class="reference internal" href="#next-steps" id="id6">Next Steps</a></p></li>
</ul>
</div>
<section id="running-examples-on-nim-for-llms">
<h2>Running Examples on NIM for LLMs<a class="headerlink" href="#running-examples-on-nim-for-llms" title="Permalink to this headline"></a></h2>
<p>NVIDIA NIM for LLMs provides the enterprise-ready approach for deploying large language models (LLMs).</p>
<p>If you are approved for <a class="reference external" href="https://developer.nvidia.com/nemo-microservices">early access to NVIDIA NeMo Microservices</a>, you can run the examples with NIM for LLMs.</p>
<p>The following figure shows the sample topology:</p>
<ul class="simple">
<li><p>The sample chat bot web application communicates with the chain server.
The chain server sends inference requests to a local NVIDIA NIM for LLMs microservice.</p></li>
<li><p>Optionally, you can deploy NVIDIA Riva. Riva can use automatic speech recognition to transcribe
your questions and use text-to-speech to speak the answers aloud.</p></li>
</ul>
<p><img alt="Using NVIDIA NIM for LLMs." src="_images/nim-llms-topology.png" /></p>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h2>
<ul>
<li><p>You have early access to NVIDIA NeMo Microservices.</p></li>
<li><p>Clone the Generative AI examples Git repository using Git LFS:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>apt<span class="w"> </span>-y<span class="w"> </span>install<span class="w"> </span>git-lfs
<span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>git@github.com:NVIDIA/GenerativeAIExamples.git
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>GenerativeAIExamples/
<span class="gp">$ </span>git<span class="w"> </span>lfs<span class="w"> </span>pull
</pre></div>
</div>
</li>
<li><p>A host with an NVIDIA A100, H100, or L40S GPU.</p></li>
<li><p>Verify NVIDIA GPU driver version 535 or later is installed and that the GPU is in compute mode:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvidia-smi<span class="w"> </span>-q<span class="w"> </span>-d<span class="w"> </span>compute
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">==============NVSMI LOG==============</span>

<span class="go">Timestamp                                 : Sun Nov 26 21:17:25 2023</span>
<span class="hll"><span class="go">Driver Version                            : 535.129.03</span>
</span><span class="go">CUDA Version                              : 12.2</span>

<span class="go">Attached GPUs                             : 1</span>
<span class="go">GPU 00000000:CA:00.0</span>
<span class="hll"><span class="go">    Compute Mode                          : Default</span>
</span></pre></div>
</div>
<p>If the driver is not installed or below version 535, refer to the <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html"><em>NVIDIA Driver Installation Quickstart Guide</em></a>.</p>
</li>
<li><p>Install Docker Engine and Docker Compose.
Refer to the instructions for <a class="reference external" href="https://docs.docker.com/engine/install/ubuntu/">Ubuntu</a>.</p></li>
<li><p>Install the NVIDIA Container Toolkit.</p>
<ol class="arabic">
<li><p>Refer to the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">installation documentation</a>.</p></li>
<li><p>When you configure the runtime, set the NVIDIA runtime as the default:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>nvidia-ctk<span class="w"> </span>runtime<span class="w"> </span>configure<span class="w"> </span>--runtime<span class="o">=</span>docker<span class="w"> </span>--set-as-default
</pre></div>
</div>
<p>If you did not set the runtime as the default, you can reconfigure the runtime by running the preceding command.</p>
</li>
<li><p>Verify the NVIDIA container toolkit is installed and configured as the default container runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>/etc/docker/daemon.json
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;default-runtime&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvidia&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;runtimes&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;nvidia&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">            </span><span class="nt">&quot;path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvidia-container-runtime&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> command in a container to verify the configuration:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>--runtime<span class="o">=</span>nvidia<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>ubuntu<span class="w"> </span>nvidia-smi<span class="w"> </span>-L
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-d8ce95c1-12f7-3174-6395-e573163a2ace)</span>
</pre></div>
</div>
</li>
</ol>
</li>
<li><p>Optional: Enable NVIDIA Riva automatic speech recognition (ASR) and text to speech (TTS).</p>
<ul>
<li><p>To launch a Riva server locally, refer to the <a class="reference external" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html">Riva Quick Start Guide</a>.</p>
<ul class="simple">
<li><p>In the provided <code class="docutils literal notranslate"><span class="pre">config.sh</span></code> script, set <code class="docutils literal notranslate"><span class="pre">service_enabled_asr=true</span></code> and <code class="docutils literal notranslate"><span class="pre">service_enabled_tts=true</span></code>, and select the desired ASR and TTS languages by adding the appropriate language codes to <code class="docutils literal notranslate"><span class="pre">asr_language_code</span></code> and <code class="docutils literal notranslate"><span class="pre">tts_language_code</span></code>.</p></li>
<li><p>After the server is running, assign its IP address (or hostname) and port (50051 by default) to <code class="docutils literal notranslate"><span class="pre">RIVA_API_URI</span></code> in <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code>.</p></li>
</ul>
</li>
<li><p>Alternatively, you can use a hosted Riva API endpoint. You might need to obtain an API key and/or Function ID for access.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code>, make the following assignments as necessary:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_API_URI</span><span class="o">=</span><span class="s2">&quot;&lt;riva-api-address/hostname&gt;:&lt;port&gt;&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_API_KEY</span><span class="o">=</span><span class="s2">&quot;&lt;riva-api-key&gt;&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_FUNCTION_ID</span><span class="o">=</span><span class="s2">&quot;&lt;riva-function-id&gt;&quot;</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="build-and-start-the-containers">
<h2>Build and Start the Containers<a class="headerlink" href="#build-and-start-the-containers" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">model-cache</span></code> directory to download and store the models</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>model-cache
</pre></div>
</div>
</li>
<li><p>In the Generative AI Examples repository, edit the <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code> file.</p>
<p>Add or update the following environment variables.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># full path to the `model-cache` directory</span>
<span class="c1"># NOTE: This should be an absolute path and not relative path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MODEL_DIRECTORY</span><span class="o">=</span><span class="s2">&quot;/path/to/model/cache/directory/&quot;</span>

<span class="c1"># IP of system where llm is deployed.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">APP_LLM_SERVERURL</span><span class="o">=</span><span class="s2">&quot;nemollm-inference:8000&quot;</span>

<span class="c1"># Name of the deployed embedding model (NV-Embed-QA)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">APP_EMBEDDINGS_MODELNAME</span><span class="o">=</span><span class="s2">&quot;NV-Embed-QA&quot;</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">APP_EMBEDDINGS_MODELENGINE</span><span class="o">=</span>nvidia-ai-endpoints

<span class="c1"># IP of system where embedding model is deployed.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">APP_EMBEDDINGS_SERVERURL</span><span class="o">=</span><span class="s2">&quot;nemollm-embedding:9080&quot;</span><span class="w">    </span><span class="c1"># Or ranking-ms:8080 for the reranking example.</span>

<span class="c1"># GPU for use by Milvus</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">VECTORSTORE_GPU_DEVICE_ID</span><span class="o">=</span>&lt;free-gpu-id&gt;
...
</pre></div>
</div>
</li>
<li><p>Build the Chain Server and RAG Playground containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>build<span class="w"> </span>chain-server<span class="w"> </span>rag-playground
</pre></div>
</div>
<p>Avoid GPU memory errors by assigning a GPU to the Chain Server.
Update <code class="docutils literal notranslate"><span class="pre">device_ids</span></code> in the <code class="docutils literal notranslate"><span class="pre">chain-server</span></code> service of <code class="docutils literal notranslate"><span class="pre">deploy/compose/rag-app-text-chatbot.yaml</span></code> manifest to specify a unique GPU ID.
You can specify a different Docker Compose file, such as <code class="docutils literal notranslate"><span class="pre">deploy/compose/rag-app-multiturn-chatbot.yaml</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="w">     </span><span class="nt">deploy</span><span class="p">:</span>
<span class="w">       </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">         </span><span class="nt">reservations</span><span class="p">:</span>
<span class="w">           </span><span class="nt">devices</span><span class="p">:</span>
<span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">driver</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<span class="w">               </span><span class="nt">device_ids</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&#39;&lt;free-gpu-id&gt;&#39;</span><span class="p p-Indicator">]</span>
<span class="w">               </span><span class="nt">capabilities</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">gpu</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
</li>
<li><p>Start the Chain Server and RAG Playground:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>up<span class="w"> </span>-d<span class="w"> </span>--no-deps<span class="w"> </span>chain-server<span class="w"> </span>rag-playground
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">-d</span></code> argument starts the services in the background and the <code class="docutils literal notranslate"><span class="pre">--no-deps</span></code> argument avoids starting the JupyterLab server.</p>
</li>
<li><p>Start the NIM for LLMs and NeMo Embedding Microservices containers.</p>
<ol class="arabic">
<li><p>Export the <code class="docutils literal notranslate"><span class="pre">NGC_API_KEY</span></code> environment variable that the containers use to download models from NVIDIA NGC:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">export NGC_API_KEY=M2...</span>
</pre></div>
</div>
<p>The NGC API key has a different value than the NVIDIA API key that the API catalog examples use.</p>
</li>
<li><p>Start the containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nv">DOCKER_USER</span><span class="o">=</span><span class="k">$(</span>id<span class="w"> </span>-u<span class="k">)</span><span class="w"> </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/docker-compose-nim-ms.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--profile<span class="w"> </span>llm-embedding<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>up<span class="w"> </span>-d
</pre></div>
</div>
</li>
</ol>
</li>
<li><p>Start the Milvus vector database:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/docker-compose-vectordb.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--profile<span class="w"> </span>llm-embedding<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>up<span class="w"> </span>-d<span class="w"> </span>milvus
</pre></div>
</div>
</li>
<li><p>Confirm the containers are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>ps<span class="w"> </span>--format<span class="w"> </span><span class="s2">&quot;table {{.ID}}\t{{.Names}}\t{{.Status}}&quot;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">CONTAINER ID   NAMES                  STATUS</span>
<span class="go">256da0ecdb7b   rag-playground         Up About an hour</span>
<span class="go">2974aa4fb2ce   chain-server           Up About an hour</span>
<span class="go">f96712f57ff8   &lt;nim-llms&gt;             Up About an hour</span>
<span class="go">5e1cf74192d6   &lt;embedding-ms&gt;         Up About an hour</span>
<span class="go">5be2b57bb5c1   milvus-standalone      Up About an hour</span>
<span class="go">a6609c22c171   milvus-minio           Up About an hour</span>
<span class="go">b23c0858c4d4   milvus-etcd            Up About an hour</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="stopping-the-containers">
<h2>Stopping the Containers<a class="headerlink" href="#stopping-the-containers" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>Stop the vector database:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-vectordb.yaml<span class="w"> </span>--profile<span class="w"> </span>llm-embedding<span class="w"> </span>down
</pre></div>
</div>
</li>
<li><p>Stop the NIM for LLMs and NeMo Retriever Embedding Microservices:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nv">DOCKER_USER</span><span class="o">=</span><span class="k">$(</span>id<span class="w"> </span>-u<span class="k">)</span><span class="w"> </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/docker-compose-nim-ms.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--profile<span class="w"> </span>llm-embedding<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>down
</pre></div>
</div>
</li>
<li><p>Stop and remove the application containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>down
</pre></div>
</div>
</li>
<li><p>Stop the NIM for LLMs container and NeMo Retriever Embedding container by pressing Ctrl+C in each terminal.</p></li>
</ol>
</section>
<section id="related-information">
<h2>Related Information<a class="headerlink" href="#related-information" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.nvidia.com/nim/large-language-models/latest/index.html"><em>NVIDIA NIM for LLMs</em></a></p></li>
<li><p><a class="reference external" href="https://developer.nvidia.com/docs/nemo-microservices/embedding/source/overview.html"><em>NVIDIA NeMo Retriever Embedding</em></a></p></li>
<li><p><a class="reference external" href="https://developer.nvidia.com/docs/nemo-microservices/reranking/source/overview.html"><em>NVIDIA NeMo Retriever Reranking</em></a></p></li>
</ul>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Use the <a class="reference internal" href="using-sample-web-application.html"><span class="doc std std-doc">Using the Sample Chat Web Application</span></a>.</p></li>
<li><p><a class="reference internal" href="vector-database.html"><span class="doc std std-doc">Configuring an Alternative Vector Database</span></a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="vector-database.html" class="btn btn-neutral float-left" title="Configuring an Alternative Vector Database" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="simple-examples.html" class="btn btn-neutral float-right" title="Developing Simple Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>
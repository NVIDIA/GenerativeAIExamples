{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Paper Webpage Generator\n",
    "\n",
    "Transform ML research papers into beautiful, interactive webpages with animations and embedded figures using **Kimi K2.5** via NVIDIA AI endpoints.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "START -> extract_pdf -> analyze_paper -> generate_webpage -> END\n",
    "```\n",
    "\n",
    "Kimi generates the entire webpage from scratch based on the paper content - no templates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: moonshotai/kimi-k2.5\n",
      "API Key configured: Yes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import pypdfium2 as pdfium\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"moonshotai/kimi-k2.5\"\n",
    "DEFAULT_TEMPERATURE = 0.7\n",
    "DEFAULT_MAX_TOKENS = 32768  # Large for HTML generation\n",
    "\n",
    "# Get API key\n",
    "NVIDIA_API_KEY = os.getenv(\"NVIDIA_API_KEY\")\n",
    "if not NVIDIA_API_KEY:\n",
    "    NVIDIA_API_KEY = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"API Key configured: {'Yes' if NVIDIA_API_KEY else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def extract_pdf_pages(pdf_path: str, dpi: int = 150) -> list[Image.Image]:\n",
    "    \"\"\"Convert PDF pages to PIL Images.\"\"\"\n",
    "    pdf = pdfium.PdfDocument(pdf_path)\n",
    "    images = []\n",
    "    scale = dpi / 72  # Convert DPI to scale factor (72 is PDF default)\n",
    "    \n",
    "    for page in pdf:\n",
    "        bitmap = page.render(scale=scale)\n",
    "        img = bitmap.to_pil()\n",
    "        images.append(img)\n",
    "    \n",
    "    pdf.close()\n",
    "    return images\n",
    "\n",
    "\n",
    "def image_to_base64(image: Image.Image, format: str = \"JPEG\", quality: int = 85) -> str:\n",
    "    \"\"\"Convert PIL Image to base64 data URL.\"\"\"\n",
    "    buffer = BytesIO()\n",
    "    if format == \"JPEG\":\n",
    "        # Convert to RGB if necessary (pypdfium2 may return RGBA)\n",
    "        if image.mode == \"RGBA\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image.save(buffer, format=format, quality=quality)\n",
    "    else:\n",
    "        image.save(buffer, format=format)\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    encoded = base64.b64encode(buffer.read()).decode(\"utf-8\")\n",
    "    mime_type = f\"image/{format.lower()}\"\n",
    "    return f\"data:{mime_type};base64,{encoded}\"\n",
    "\n",
    "\n",
    "def images_to_base64_list(images: list[Image.Image]) -> list[str]:\n",
    "    \"\"\"Convert list of PIL Images to base64 data URLs.\"\"\"\n",
    "    return [image_to_base64(img) for img in images]\n",
    "\n",
    "\n",
    "print(\"PDF utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kimi Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client created: ChatNVIDIA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/Code/NVIDIA/GenerativeAIExamples/oss_tutorials/Kimi_K2.5_Paper_to_Page_Vision_Workflow/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/_common.py:243: UserWarning: Found moonshotai/kimi-k2.5 in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def create_kimi_client(\n",
    "    temperature: float = DEFAULT_TEMPERATURE,\n",
    "    max_completion_tokens: int = DEFAULT_MAX_TOKENS,\n",
    ") -> ChatNVIDIA:\n",
    "    \"\"\"Create a ChatNVIDIA client configured for Kimi K2.5.\"\"\"\n",
    "    return ChatNVIDIA(\n",
    "        model=MODEL_NAME,\n",
    "        api_key=NVIDIA_API_KEY,\n",
    "        temperature=temperature,\n",
    "        max_completion_tokens=max_completion_tokens,\n",
    "    )\n",
    "\n",
    "\n",
    "client = create_kimi_client()\n",
    "print(f\"Client created: {type(client).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Workflow State and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts defined.\n"
     ]
    }
   ],
   "source": [
    "class WorkflowState(TypedDict):\n",
    "    \"\"\"State for the paper webpage generator workflow.\"\"\"\n",
    "    pdf_path: str\n",
    "    page_images: list[str]  # base64 encoded\n",
    "    paper_analysis: dict    # structured analysis from vision model\n",
    "    html_output: str        # final HTML generated by Kimi\n",
    "\n",
    "\n",
    "# Step 1: Analyze the paper\n",
    "ANALYSIS_SYSTEM_PROMPT = \"\"\"You are an expert ML researcher. Analyze research papers and extract structured information.\"\"\"\n",
    "\n",
    "ANALYSIS_USER_PROMPT = \"\"\"Analyze this ML research paper. Extract the following as JSON:\n",
    "\n",
    "{\n",
    "    \"title\": \"Paper title\",\n",
    "    \"authors\": [\"Author 1\", \"Author 2\"],\n",
    "    \"abstract\": \"Full abstract\",\n",
    "    \"key_contributions\": [\"Contribution 1\", \"Contribution 2\"],\n",
    "    \"sections\": [{\"title\": \"Section\", \"summary\": \"Summary\"}],\n",
    "    \"figures\": [{\"number\": \"1\", \"caption\": \"Caption\", \"description\": \"What it shows\", \"page\": 1}],\n",
    "    \"key_equations\": [{\"equation\": \"LaTeX\", \"meaning\": \"Explanation\"}],\n",
    "    \"core_concepts\": [\"Concept 1\", \"Concept 2\"],\n",
    "    \"methodology\": \"Brief methodology description\",\n",
    "    \"results\": \"Key results\",\n",
    "    \"conclusions\": \"Main conclusions\",\n",
    "    \"key_diagram\": \"Describe the most important diagram/figure in the paper - what does it show? What are the components and how do they connect?\"\n",
    "}\n",
    "\n",
    "Return ONLY valid JSON, no markdown.\"\"\"\n",
    "\n",
    "\n",
    "# Step 2: Generate a SIMPLE webpage with ONE animation\n",
    "WEBPAGE_SYSTEM_PROMPT = \"\"\"You are a web developer. Generate clean HTML with one CSS animation.\"\"\"\n",
    "\n",
    "WEBPAGE_USER_PROMPT = \"\"\"Create a single-page HTML summary of this paper.\n",
    "\n",
    "## Paper Analysis:\n",
    "{analysis_json}\n",
    "\n",
    "## REQUIREMENTS:\n",
    "\n",
    "1. **Keep it concise** - Under 500 lines of HTML total\n",
    "2. **Simple structure:**\n",
    "   - Title and authors\n",
    "   - Abstract\n",
    "   - Key contributions (bullet list)\n",
    "   - **ONE CSS animation** that illustrates the key_diagram from the analysis\n",
    "   - Core concepts (bullet list)  \n",
    "   - Conclusions\n",
    "\n",
    "3. **The animation section:**\n",
    "   - Create a CSS @keyframes animation that visualizes the paper's main diagram/architecture\n",
    "   - Use simple shapes (divs, borders) to represent components\n",
    "   - Animate data flow, connections, or the key process\n",
    "   - Make it educational - help the reader understand how the system works\n",
    "\n",
    "4. **NO figures/images** - The animation replaces static figures\n",
    "5. **Minimal JavaScript** - Only if needed for the animation\n",
    "6. **Light theme only**\n",
    "\n",
    "Return ONLY the HTML starting with <!DOCTYPE html>.\"\"\"\n",
    "\n",
    "\n",
    "print(\"Prompts defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workflow Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow nodes defined.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_pdf_node(state: WorkflowState) -> dict:\n",
    "    \"\"\"Extract PDF pages as base64 images.\"\"\"\n",
    "    print(f\"Extracting PDF: {state['pdf_path']}\")\n",
    "    \n",
    "    images = extract_pdf_pages(state[\"pdf_path\"])\n",
    "    print(f\"  Extracted {len(images)} pages\")\n",
    "    \n",
    "    base64_images = images_to_base64_list(images)\n",
    "    print(f\"  Converted to base64\")\n",
    "    \n",
    "    return {\"page_images\": base64_images}\n",
    "\n",
    "\n",
    "def analyze_paper_node(state: WorkflowState) -> dict:\n",
    "    \"\"\"Analyze paper using vision model.\"\"\"\n",
    "    print(\"Analyzing paper with vision model...\")\n",
    "    \n",
    "    client = create_kimi_client(max_completion_tokens=8192)\n",
    "    \n",
    "    # Build message with all page images\n",
    "    content = [{\"type\": \"text\", \"text\": ANALYSIS_USER_PROMPT}]\n",
    "    \n",
    "    for i, img_base64 in enumerate(state[\"page_images\"]):\n",
    "        content.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": img_base64}\n",
    "        })\n",
    "    \n",
    "    print(f\"  Sending {len(state['page_images'])} pages to vision model...\")\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=ANALYSIS_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=content),\n",
    "    ]\n",
    "    \n",
    "    response = client.invoke(messages)\n",
    "    response_text = response.content\n",
    "    \n",
    "    # Parse JSON\n",
    "    try:\n",
    "        if \"```json\" in response_text:\n",
    "            response_text = response_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in response_text:\n",
    "            response_text = response_text.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        analysis = json.loads(response_text.strip())\n",
    "        print(f\"  Analysis complete: {analysis.get('title', 'Unknown')}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"  JSON parse error: {e}\")\n",
    "        analysis = {\n",
    "            \"title\": \"Paper\",\n",
    "            \"authors\": [],\n",
    "            \"abstract\": response_text[:2000],\n",
    "            \"key_contributions\": [],\n",
    "            \"sections\": [],\n",
    "            \"figures\": [],\n",
    "            \"key_equations\": [],\n",
    "            \"core_concepts\": [],\n",
    "            \"methodology\": \"\",\n",
    "            \"results\": \"\",\n",
    "            \"conclusions\": \"\"\n",
    "        }\n",
    "    \n",
    "    return {\"paper_analysis\": analysis}\n",
    "\n",
    "\n",
    "def generate_webpage_node(state: WorkflowState) -> dict:\n",
    "    \"\"\"Generate minimal HTML webpage using Kimi.\"\"\"\n",
    "    print(\"Generating minimal webpage with Kimi...\")\n",
    "    \n",
    "    # Keep token limit reasonable for thinking\n",
    "    client = create_kimi_client(max_completion_tokens=16384)\n",
    "    \n",
    "    # Only send the analysis JSON - NO images\n",
    "    prompt = WEBPAGE_USER_PROMPT.format(\n",
    "        analysis_json=json.dumps(state[\"paper_analysis\"], indent=2)\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=WEBPAGE_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=prompt),\n",
    "    ]\n",
    "    \n",
    "    print(\"  Calling Kimi to generate HTML (minimal)...\")\n",
    "    response = client.invoke(messages)\n",
    "    html = response.content\n",
    "    \n",
    "    # Clean up if wrapped in markdown\n",
    "    if html.startswith(\"```html\"):\n",
    "        html = html[7:]\n",
    "    if html.startswith(\"```\"):\n",
    "        html = html[3:]\n",
    "    if html.endswith(\"```\"):\n",
    "        html = html[:-3]\n",
    "    html = html.strip()\n",
    "    \n",
    "    # Inject actual images if any placeholders exist\n",
    "    if \"PAGE_IMAGE\" in html and state.get(\"page_images\"):\n",
    "        print(\"  Injecting images into HTML...\")\n",
    "        for i, img_base64 in enumerate(state[\"page_images\"]):\n",
    "            html = html.replace(f\"{{{{PAGE_IMAGE_{i+1}}}}}\", img_base64)\n",
    "        html = re.sub(\n",
    "            r'\\{\\{PAGE_IMAGE_(\\d+)\\}\\}',\n",
    "            lambda m: state[\"page_images\"][int(m.group(1))-1] if int(m.group(1)) <= len(state[\"page_images\"]) else \"\",\n",
    "            html\n",
    "        )\n",
    "    \n",
    "    print(f\"  Generated {len(html)} characters of HTML\")\n",
    "    \n",
    "    return {\"html_output\": html}\n",
    "\n",
    "\n",
    "print(\"Workflow nodes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow builder defined.\n"
     ]
    }
   ],
   "source": [
    "def build_workflow() -> StateGraph:\n",
    "    \"\"\"Build the LangGraph workflow for paper processing.\"\"\"\n",
    "    graph = StateGraph(WorkflowState)\n",
    "    \n",
    "    graph.add_node(\"extract_pdf\", extract_pdf_node)\n",
    "    graph.add_node(\"analyze_paper\", analyze_paper_node)\n",
    "    graph.add_node(\"generate_webpage\", generate_webpage_node)\n",
    "    \n",
    "    graph.add_edge(START, \"extract_pdf\")\n",
    "    graph.add_edge(\"extract_pdf\", \"analyze_paper\")\n",
    "    graph.add_edge(\"analyze_paper\", \"generate_webpage\")\n",
    "    graph.add_edge(\"generate_webpage\", END)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "print(\"Workflow builder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Paper Webpage Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaperWebpageWorkflow class defined.\n"
     ]
    }
   ],
   "source": [
    "class PaperWebpageWorkflow:\n",
    "    \"\"\"Workflow for converting ML papers to interactive webpages.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        graph = build_workflow()\n",
    "        self.app = graph.compile()\n",
    "        print(\"PaperWebpageWorkflow initialized.\")\n",
    "    \n",
    "    def process(self, pdf_path: str) -> dict:\n",
    "        \"\"\"Process a PDF and return the result.\"\"\"\n",
    "        path = Path(pdf_path).resolve()\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"PDF not found: {path}\")\n",
    "        \n",
    "        print(f\"\\nProcessing: {path}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        result = self.app.invoke({\"pdf_path\": str(path)})\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_and_save(self, pdf_path: str, output_path: str = None) -> str:\n",
    "        \"\"\"Process a PDF and save the HTML output.\"\"\"\n",
    "        result = self.process(pdf_path)\n",
    "        \n",
    "        if output_path is None:\n",
    "            output_path = Path(pdf_path).with_suffix(\".html\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result[\"html_output\"])\n",
    "        \n",
    "        print(f\"\\nSaved HTML to: {output_path}\")\n",
    "        return str(output_path)\n",
    "\n",
    "\n",
    "print(\"PaperWebpageWorkflow class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaperWebpageWorkflow initialized.\n"
     ]
    }
   ],
   "source": [
    "# Create the workflow\n",
    "workflow = PaperWebpageWorkflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/chris/Code/NVIDIA/GenerativeAIExamples/oss_tutorials/Kimi_K2.5_Paper_to_Page_Vision_Workflow/Kimi K2.5 Visual Agentic Intelligence Technical Report.pdf\n",
      "==================================================\n",
      "Extracting PDF: /home/chris/Code/NVIDIA/GenerativeAIExamples/oss_tutorials/Kimi_K2.5_Paper_to_Page_Vision_Workflow/Kimi K2.5 Visual Agentic Intelligence Technical Report.pdf\n",
      "  Extracted 15 pages\n",
      "  Converted to base64\n",
      "Analyzing paper with vision model...\n",
      "  Sending 15 pages to vision model...\n",
      "  Analysis complete: Kimi K2.5: Visual Agentic Intelligence\n",
      "Generating minimal webpage with Kimi...\n",
      "  Calling Kimi to generate HTML (minimal)...\n",
      "  Generated 13126 characters of HTML\n",
      "==================================================\n",
      "Processing complete!\n",
      "\n",
      "Saved HTML to: Kimi K2.5 Visual Agentic Intelligence Technical Report.html\n"
     ]
    }
   ],
   "source": [
    "# Process a paper - uncomment and set your PDF path\n",
    "result = workflow.process_and_save(\"Kimi K2.5 Visual Agentic Intelligence Technical Report.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Kimi K2.5: Visual Agentic Intelligence - Summary</title>\n",
       "    <style>\n",
       "        * { margin: 0; padding: 0; box-sizing: border-box; }\n",
       "        body {\n",
       "            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n",
       "            line-height: 1.6;\n",
       "            color: #1a1a1a;\n",
       "            background: #fafafa;\n",
       "            padding: 2rem 1rem;\n",
       "            max-width: 900px;\n",
       "            margin: 0 auto;\n",
       "        }\n",
       "        h1 { font-size: 2rem; color: #2563eb; margin-bottom: 0.5rem; }\n",
       "        .authors { color: #666; font-size: 1.1rem; margin-bottom: 2rem; }\n",
       "        section { background: white; padding: 1.5rem; border-radius: 8px; margin-bottom: 1.5rem; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }\n",
       "        h2 { color: #1e40af; font-size: 1.3rem; margin-bottom: 1rem; border-bottom: 2px solid #e5e7eb; padding-bottom: 0.5rem; }\n",
       "        ul { padding-left: 1.5rem; }\n",
       "        li { margin-bottom: 0.5rem; }\n",
       "        .abstract { font-size: 1.05rem; line-height: 1.7; color: #374151; }\n",
       "        \n",
       "        /* Animation Container */\n",
       "        .diagram-container {\n",
       "            background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);\n",
       "            border: 2px solid #bae6fd;\n",
       "            border-radius: 12px;\n",
       "            padding: 2rem;\n",
       "            height: 400px;\n",
       "            position: relative;\n",
       "            overflow: hidden;\n",
       "            margin: 1rem 0;\n",
       "        }\n",
       "        \n",
       "        /* Orchestrator */\n",
       "        .orchestrator {\n",
       "            width: 140px;\n",
       "            height: 60px;\n",
       "            background: linear-gradient(135deg, #2563eb, #1d4ed8);\n",
       "            color: white;\n",
       "            border-radius: 30px;\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            justify-content: center;\n",
       "            font-weight: bold;\n",
       "            position: absolute;\n",
       "            left: 50%;\n",
       "            top: 20px;\n",
       "            transform: translateX(-50%);\n",
       "            box-shadow: 0 4px 15px rgba(37, 99, 235, 0.3);\n",
       "            z-index: 10;\n",
       "            animation: pulse 2s ease-in-out infinite;\n",
       "        }\n",
       "        \n",
       "        /* Subagents Container */\n",
       "        .swarm-layer {\n",
       "            display: flex;\n",
       "            justify-content: space-around;\n",
       "            position: absolute;\n",
       "            bottom: 80px;\n",
       "            left: 0;\n",
       "            right: 0;\n",
       "            padding: 0 2rem;\n",
       "        }\n",
       "        \n",
       "        .subagent {\n",
       "            width: 100px;\n",
       "            height: 50px;\n",
       "            background: white;\n",
       "            border: 2px solid #3b82f6;\n",
       "            border-radius: 8px;\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            justify-content: center;\n",
       "            font-size: 0.75rem;\n",
       "            font-weight: 600;\n",
       "            color: #1e40af;\n",
       "            opacity: 0;\n",
       "            transform: scale(0.8);\n",
       "            animation: spawn 0.5s ease-out forwards;\n",
       "            position: relative;\n",
       "            box-shadow: 0 2px 8px rgba(59, 130, 246, 0.15);\n",
       "        }\n",
       "        \n",
       "        .subagent:nth-child(1) { animation-delay: 0.5s; }\n",
       "        .subagent:nth-child(2) { animation-delay: 0.7s; }\n",
       "        .subagent:nth-child(3) { animation-delay: 0.9s; }\n",
       "        .subagent:nth-child(4) { animation-delay: 1.1s; }\n",
       "        .subagent:nth-child(5) { animation-delay: 1.3s; }\n",
       "        \n",
       "        /* Connection Lines */\n",
       "        .connection {\n",
       "            position: absolute;\n",
       "            width: 2px;\n",
       "            background: #93c5fd;\n",
       "            transform-origin: top;\n",
       "            opacity: 0;\n",
       "        }\n",
       "        \n",
       "        .conn1 { left: 20%; top: 80px; height: 180px; animation: lineGrow 0.3s ease-out 0.5s forwards; }\n",
       "        .conn2 { left: 35%; top: 80px; height: 180px; animation: lineGrow 0.3s ease-out 0.7s forwards; }\n",
       "        .conn3 { left: 50%; top: 80px; height: 180px; animation: lineGrow 0.3s ease-out 0.9s forwards; }\n",
       "        .conn4 { left: 65%; top: 80px; height: 180px; animation: lineGrow 0.3s ease-out 1.1s forwards; }\n",
       "        .conn5 { left: 80%; top: 80px; height: 180px; animation: lineGrow 0.3s ease-out 1.3s forwards; }\n",
       "        \n",
       "        /* Task Packets */\n",
       "        .packet {\n",
       "            width: 12px;\n",
       "            height: 12px;\n",
       "            border-radius: 50%;\n",
       "            position: absolute;\n",
       "            opacity: 0;\n",
       "        }\n",
       "        \n",
       "        .task-packet {\n",
       "            background: #f59e0b;\n",
       "            box-shadow: 0 0 8px rgba(245, 158, 11, 0.6);\n",
       "        }\n",
       "        \n",
       "        .result-packet {\n",
       "            background: #10b981;\n",
       "            box-shadow: 0 0 8px rgba(16, 185, 129, 0.6);\n",
       "        }\n",
       "        \n",
       "        .tp1 { left: calc(20% - 5px); animation: flowDown 2s ease-in-out 1s infinite; }\n",
       "        .tp2 { left: calc(35% - 5px); animation: flowDown 2s ease-in-out 1.2s infinite; }\n",
       "        .tp3 { left: calc(50% - 5px); animation: flowDown 2s ease-in-out 1.4s infinite; }\n",
       "        .tp4 { left: calc(65% - 5px); animation: flowDown 2s ease-in-out 1.6s infinite; }\n",
       "        .tp5 { left: calc(80% - 5px); animation: flowDown 2s ease-in-out 1.8s infinite; }\n",
       "        \n",
       "        .rp1 { left: calc(20% - 5px); animation: flowUp 2s ease-in-out 2s infinite; }\n",
       "        .rp2 { left: calc(35% - 5px); animation: flowUp 2s ease-in-out 2.2s infinite; }\n",
       "        .rp3 { left: calc(50% - 5px); animation: flowUp 2s ease-in-out 2.4s infinite; }\n",
       "        .rp4 { left: calc(65% - 5px); animation: flowUp 2s ease-in-out 2.6s infinite; }\n",
       "        .rp5 { left: calc(80% - 5px); animation: flowUp 2s ease-in-out 2.8s infinite; }\n",
       "        \n",
       "        /* Processing Indicator */\n",
       "        .processing {\n",
       "            position: absolute;\n",
       "            bottom: 20px;\n",
       "            left: 50%;\n",
       "            transform: translateX(-50%);\n",
       "            background: rgba(255,255,255,0.9);\n",
       "            padding: 0.5rem 1rem;\n",
       "            border-radius: 20px;\n",
       "            font-size: 0.85rem;\n",
       "            color: #2563eb;\n",
       "            font-weight: 600;\n",
       "            opacity: 0;\n",
       "            animation: fadeIn 0.5s ease-out 2s forwards;\n",
       "        }\n",
       "        \n",
       "        /* Legend */\n",
       "        .legend {\n",
       "            position: absolute;\n",
       "            top: 10px;\n",
       "            right: 10px;\n",
       "            font-size: 0.75rem;\n",
       "            background: rgba(255,255,255,0.9);\n",
       "            padding: 0.5rem;\n",
       "            border-radius: 4px;\n",
       "        }\n",
       "        .legend-item { display: flex; align-items: center; margin: 4px 0; }\n",
       "        .dot { width: 8px; height: 8px; border-radius: 50%; margin-right: 6px; }\n",
       "        .dot.task { background: #f59e0b; }\n",
       "        .dot.result { background: #10b981; }\n",
       "        \n",
       "        /* Keyframes */\n",
       "        @keyframes pulse {\n",
       "            0%, 100% { transform: translateX(-50%) scale(1); box-shadow: 0 4px 15px rgba(37, 99, 235, 0.3); }\n",
       "            50% { transform: translateX(-50%) scale(1.05); box-shadow: 0 6px 25px rgba(37, 99, 235, 0.5); }\n",
       "        }\n",
       "        \n",
       "        @keyframes spawn {\n",
       "            to { opacity: 1; transform: scale(1); }\n",
       "        }\n",
       "        \n",
       "        @keyframes lineGrow {\n",
       "            from { transform: scaleY(0); opacity: 0; }\n",
       "            to { transform: scaleY(1); opacity: 1; }\n",
       "        }\n",
       "        \n",
       "        @keyframes flowDown {\n",
       "            0% { top: 80px; opacity: 0; }\n",
       "            10% { opacity: 1; }\n",
       "            40% { top: 260px; opacity: 1; }\n",
       "            50% { opacity: 0; }\n",
       "            100% { opacity: 0; }\n",
       "        }\n",
       "        \n",
       "        @keyframes flowUp {\n",
       "            0% { top: 260px; opacity: 0; }\n",
       "            10% { opacity: 1; }\n",
       "            40% { top: 80px; opacity: 1; }\n",
       "            50% { opacity: 0; }\n",
       "            100% { opacity: 0; }\n",
       "        }\n",
       "        \n",
       "        @keyframes fadeIn {\n",
       "            to { opacity: 1; }\n",
       "        }\n",
       "        \n",
       "        .highlight { background: #fef3c7; padding: 0.1rem 0.3rem; border-radius: 4px; font-weight: 600; }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "    <h1>Kimi K2.5: Visual Agentic Intelligence</h1>\n",
       "    <div class=\"authors\">Moonshot AI</div>\n",
       "    \n",
       "    <section>\n",
       "        <h2>Abstract</h2>\n",
       "        <p class=\"abstract\">\n",
       "            Kimi K2.5 is introduced as the most powerful open-source model to date, building on Kimi K2 with continued pretraining over approximately <span class=\"highlight\">15T mixed visual and text tokens</span>. Built as a native multimodal model, K2.5 delivers state-of-the-art coding and vision capabilities alongside a self-directed agent swarm paradigm. For complex tasks, Kimi K2.5 can self-direct an agent swarm with up to <span class=\"highlight\">100 sub-agents</span>, executing parallel workflows across up to <span class=\"highlight\">1,500 tool calls</span>, reducing execution time by up to <span class=\"highlight\">4.5×</span> compared to single-agent setups.\n",
       "        </p>\n",
       "    </section>\n",
       "    \n",
       "    <section>\n",
       "        <h2>Key Contributions</h2>\n",
       "        <ul>\n",
       "            <li><strong>Native multimodal architecture</strong> with state-of-the-art coding and vision capabilities</li>\n",
       "            <li><strong>Agent Swarm paradigm</strong> enabling self-directed orchestration of up to 100 sub-agents executing 1,500+ parallel tool calls</li>\n",
       "            <li><strong>Parallel-Agent Reinforcement Learning (PARL)</strong> for training swarm orchestration without predefined workflows</li>\n",
       "            <li><strong>Coding with Vision</strong> capabilities for image/video-to-code generation and visual debugging</li>\n",
       "            <li><strong>Real-world office productivity automation</strong> handling documents, spreadsheets, PDFs, and slide decks</li>\n",
       "        </ul>\n",
       "    </section>\n",
       "    \n",
       "    <section>\n",
       "        <h2>Agent Swarm Architecture</h2>\n",
       "        <p style=\"margin-bottom: 1rem; font-size: 0.95rem; color: #4b5563;\">\n",
       "            The animation below illustrates the core innovation: a trainable <strong>Orchestrator</strong> dynamically instantiates specialized subagents (AI Researcher, Physics Researcher, etc.), assigns tasks in parallel, and aggregates results—enabling up to 100 concurrent subagents without predefined workflows.\n",
       "        </p>\n",
       "        \n",
       "        <div class=\"diagram-container\">\n",
       "            <div class=\"legend\">\n",
       "                <div class=\"legend-item\"><div class=\"dot task\"></div>Task Assignment</div>\n",
       "                <div class=\"legend-item\"><div class=\"dot result\"></div>Result Aggregation</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"orchestrator\">Orchestrator</div>\n",
       "            \n",
       "            <div class=\"connection conn1\"></div>\n",
       "            <div class=\"connection conn2\"></div>\n",
       "            <div class=\"connection conn3\"></div>\n",
       "            <div class=\"connection conn4\"></div>\n",
       "            <div class=\"connection conn5\"></div>\n",
       "            \n",
       "            <div class=\"packet task-packet tp1\"></div>\n",
       "            <div class=\"packet task-packet tp2\"></div>\n",
       "            <div class=\"packet task-packet tp3\"></div>\n",
       "            <div class=\"packet task-packet tp4\"></div>\n",
       "            <div class=\"packet task-packet tp5\"></div>\n",
       "            \n",
       "            <div class=\"packet result-packet rp1\"></div>\n",
       "            <div class=\"packet result-packet rp2\"></div>\n",
       "            <div class=\"packet result-packet rp3\"></div>\n",
       "            <div class=\"packet result-packet rp4\"></div>\n",
       "            <div class=\"packet result-packet rp5\"></div>\n",
       "            \n",
       "            <div class=\"swarm-layer\">\n",
       "                <div class=\"subagent\">AI Researcher</div>\n",
       "                <div class=\"subagent\">Physics Researcher</div>\n",
       "                <div class=\"subagent\">Fact Checker</div>\n",
       "                <div class=\"subagent\">Web Developer</div>\n",
       "                <div class=\"subagent\">Life Sciences</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"processing\">Parallel Execution Active • 4.5× Speedup</div>\n",
       "        </div>\n",
       "    </section>\n",
       "    \n",
       "    <section>\n",
       "        <h2>Core Concepts</h2>\n",
       "        <ul>\n",
       "            <li><strong>Agent Swarm:</strong> Self-directed parallel execution with dynamic subagent creation and orchestration</li>\n",
       "            <li><strong>Parallel-Agent Reinforcement Learning (PARL):</strong> Training framework using staged reward shaping to learn swarm orchestration without manual workflow design</li>\n",
       "            <li><strong>Critical Steps (CS):</strong> Latency metric defined as <em>T<sub>overhead</sub> + max(T<sub>subagent</sub>)</em> to optimize wall-clock time rather than total steps</li>\n",
       "            <li><strong>Visual Agentic Intelligence:</strong> Native multimodal reasoning integrating vision and text for coding tasks</li>\n",
       "            <li><strong>Staged Reward Shaping:</strong> Curriculum transitioning from parallelism exploration (high α) to task optimization (low α) in <em>R = r<sub>task</sub> + α(t) · r<sub>parallel</sub></em></li>\n",
       "        </ul>\n",
       "    </section>\n",
       "    \n",
       "    <section>\n",
       "        <h2>Conclusions</h2>\n",
       "        <p>\n",
       "            Kimi K2.5 represents a significant advancement toward AGI for the open-source community, demonstrating that vision and coding capabilities improve in unison at scale. The <strong>Agent Swarm paradigm</strong> enables practical parallel execution for complex real-world tasks, achieving <strong>80% reduction in end-to-end runtime</strong> through self-directed orchestration of up to 100 subagents. By eliminating the traditional trade-off between vision and text capabilities, K2.5 establishes a new standard for multimodal agentic intelligence in knowledge work.\n",
       "        </p>\n",
       "    </section>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display in notebook\n",
    "from IPython.display import HTML\n",
    "from pathlib import Path\n",
    "\n",
    "HTML(Path(\"Kimi K2.5 Visual Agentic Intelligence Technical Report.html\").read_text())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

general:
  use_uvloop: true
  telemetry:
    logging:
      console:
        _type: console
        level: DEBUG
  tracing:
    phoenix:
      _type: phoenix
      endpoint: http://localhost:6006/v1/traces
      project: predictive-maintenance-app

llms:
  sql_llm:
    _type: nim
    model_name: "meta/llama-4-scout-17b-16e-instruct"
  coding_llm:
    _type: nim
    model_name: "qwen/qwen2.5-coder-32b-instruct"
    max_tokens: 2000
  reasoning_llm:
    _type: nim
    model_name: "qwen/qwq-32b"

embedders:
  vanna_embedder:
    _type: nim
    model_name: "nvidia/nv-embed-v1"

functions:
  sql_retriever:
    _type: generate_sql_query_and_retrieve_tool
    llm_name: sql_llm
    embedding_name: vanna_embedder
    vector_store_path: "${PWD_PATH}/database"
    db_path: "${PWD_PATH}/database/nasa_turbo.db"
    output_folder: "${PWD_PATH}/output_data"
  predict_rul:
    _type: predict_rul_tool
    output_folder: "${PWD_PATH}/output_data"
    scaler_path: "${PWD_PATH}/models/scaler_model.pkl"
    model_path: "${PWD_PATH}/models/xgb_model_fd001.pkl"
  code_execution:
    _type: code_execution
    uri: http://127.0.0.1:6000/execute
    sandbox_type: local
    max_output_characters: 2000
  data_analysis_assistant:
    _type: react_agent
    llm_name: coding_llm
    max_iterations: 5
    tool_names: [sql_retriever, code_execution, predict_rul]
    system_prompt: |
      You are a helpful data analysis assistant that can help with predictive maintenance tasks for a turbofan engine. You will work with planning agent
      that provides a plan to you which you should follow:
      You can use the following tools to help with your task:
      {tools}

      Note: Your output_data folder is in "${PWD_PATH}/output_data" path.
      However, the code execution sandbox runs with /workspace as the working directory (mounted to your local output_data folder)
      So, when you are using the code execution tool, you should use relative paths starting with './' for file operations. 
      For example, if you want to read a file from the output_data folder, you should use './filename.json' as the path.

      But when passing any generated JSON file to other tools, you should use the absolute path to the file.
      For example, if you want to pass the file 'engine_unit_24_sensor_data.json' to the predict_rul tool, you should use the absolute path to the file.
      which is "${PWD_PATH}/output_data/engine_unit_24_sensor_data.json"

      **EXAMPLE CODE STRUCTURE:**
      ### START PYTHON CODE ###
      import pandas as pd
      import plotly.graph_objects as go
      
      # Load data using relative path (working directory is /workspace mounted to your output_data)
      data = pd.read_json('your_input_file.json')
      
      # Create your analysis/plot
      fig = go.Figure(data=[go.Scatter(x=data['time_in_cycles'], y=data['sensor_measurement_10'])])
      fig.update_layout(title='Your Plot Title')
      
      # Save to current directory (will appear in your local output_data folder)
      fig.write_html('your_output_file.html')
      print(f"Plot saved to: your_output_file.html")
      ### END PYTHON CODE ###

      # File Handling and Tool Usage Guidelines
      # --------------------------------
      # 1. HTML File Paths
      # - All HTML files from code execution are saved in the output_data directory
      # - Always include the full path when referencing HTML files to users
      # - Example: "${PWD_PATH}/output_data/plot_name.html"
      
      # 2. SQL Query Policy
      # - NEVER generate SQL queries manually
      # - ALWAYS use the provided SQL retrieval tool
      
      # 3. Typical Workflow
      # a) Data Extraction
      #    - Use SQL retrieval tool to fetch required data
      # b) Data Processing
      #    - Generate Python code for analysis/visualization
      #    - Execute code using code execution tool
      #    - Save results in output_data directory
      # c) Result Handling
      #    - Return processed information to calling agent
      #    - DO NOT USE MARKDOWN FORMATTING IN YOUR RESPONSE.
      #    - If the code execution tool responds with a warning in the stderr then ignore it and take action based on the stdout.
      
      # 4. Visualization Guidelines
      # - Use plotly.js for creating interactive plots
      # - Save visualizations as HTML files
      # - Store all plots in output_data directory
      # - When comparing actual and predicted RUL columns, convert the actual RUL column to piecewise its piecewise RUL values before plotting.
      Piecewise RUL instructions:
      1) Calculate the true failure point by taking the last cycle in your data and adding the final RUL value at that cycle (e.g., if last cycle is 100 with RUL=25, true failure is at cycle 125).
      2) Create the piecewise pattern where if the true failure cycle is greater than MAXLIFE (125), RUL stays flat at MAXLIFE until the "knee point" (true_failure - MAXLIFE), then declines linearly to zero; otherwise RUL just declines linearly from MAXLIFE.
      3) Generate RUL values for each cycle in your data using this pattern - flat section gets constant MAXLIFE value, declining section decreases by (MAXLIFE / remaining_cycles_to_failure) each step.
      4) Replace the actual RUL column in your dataset with these calculated piecewise values while keeping all other columns unchanged.
      5) The result is a "knee-shaped" RUL curve that better represents equipment degradation patterns - flat during early life, then linear decline toward failure.
      
      You may respond in one of two formats:

      Use the following format exactly when you want to use a tool:
      
      Question: the input question you must answer
      Thought: you should always think about what to do
      Action: the action to take, should be one of [{tool_names}]
      Action Input: the input to the action (if there is no required input, include "Action Input: None")
      Observation: wait for the tool to finish execution

      Use the following format exactly when you don't want to use a tool:

      Question: the input question you must answer
      Thought: you should always think about what to do
      Final Answer: the final answer to the original input question

      Use only the SQL retrieval tool for fetching data, do not generate code to do that.

workflow:
  _type: reasoning_agent
  augmented_fn: data_analysis_assistant
  llm_name: reasoning_llm
  verbose: true
  reasoning_prompt_template: |
    You are a Data Analysis Reasoning and Planning Expert specialized in analyzing turbofan engine sensor data and predictive maintenance tasks. 
    You are tasked with creating detailed execution plans for addressing user queries while being conversational and helpful.

    **Your Role and Capabilities:**
    - Expert in turbofan engine data analysis, predictive maintenance, and anomaly detection
    - Create step-by-step execution plans using available tools
    - Provide conversational responses while maintaining technical accuracy
    - Only use tools when necessary to answer the user's question

    You are given a data analysis assistant to execute your plan, all you have to do is generate the plan.
    DO NOT USE MARKDOWN FORMATTING IN YOUR RESPONSE.

    **Description:** 
    {augmented_function_desc}

    **Tools and description of the tool:** {tools}

    Guidelines:
    1. **Send the path to any HTML files generated to users** when tools return them (especially plotting results)
    2. **Only use tools if needed** - Not all queries require tool usage

    ---- 

    Necessary Context:
    You work with turbofan engine sensor data from multiple engines in a fleet. The data contains:
    - **Time series data** from different engines, each with unique wear patterns and operational history separated into 
    four datasets (FD001, FD002, FD003, FD004), each dataset is further divided into training and test subsets.
    - **26 data columns**: unit number, time in cycles, 3 operational settings, and 21 sensor measurements  
    - **Engine lifecycle**: Engines start operating normally, then develop faults that grow until system failure
    - **Predictive maintenance goal**: Predict Remaining Useful Life (RUL) - how many operational cycles before failure
    - **Data characteristics**: Contains normal operational variation, sensor noise, and progressive fault development    
    This context helps you understand user queries about engine health, sensor patterns, failure prediction, and maintenance planning.
    
    For Anomaly Detection Tasks:
    When performing anomaly detection, follow this comprehensive approach:

    1) First get the sensor measurement information for the same engine number from both training and test datasets across different cycle times and order 
    it in increasing order.
    2) Use the measurement from training data to calculate statistical baselines (mean, standard deviation, moving averages), because it represents what
    normal operational behvaior looks like.
    3) Apply multiple statistical approaches to identify anomalies in test data:
       - **Z-Score Analysis**: Compare test values against training data mean/std deviation using threshold (typically 3)
       - **Moving Statistical Analysis**: Use rolling windows from training data to detect dynamic anomalies
       - Flag data points that exceed statistical thresholds as potential anomalies
    4) Create comprehensive plots showing test data timeline with anomalies highlighted
       - Use different colors/markers to distinguish between normal data and show all different types of anomalies
       - Include hover information and legends for clear interpretation
       - Save visualizations as interactive HTML files for detailed analysis       

    ----

    **User Input:**
    {input_text}

    Analyze the input and create a comprehensive plan following this structure:
    
    Generate a plan that would look like this with numbered bullet points:
    1. Call tool A with input X
    2. Call tool B with input Y
    3. Interpret the output of tool A and B
    4. Return the final result

    **PLAN:**

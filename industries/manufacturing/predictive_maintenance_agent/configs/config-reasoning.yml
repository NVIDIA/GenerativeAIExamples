general:
  use_uvloop: true
  telemetry:
    logging:
      console:
        _type: console
        level: DEBUG
  tracing:
    catalyst:
      _type: catalyst
      project: "catalyst-demo"
      dataset: "catalyst-dataset"

llms:
  sql_llm:
    _type: nim
    model_name: "nvidia/llama-3.3-nemotron-super-49b-v1"
  coding_llm:
    _type: nim
    model_name: "qwen/qwen2.5-coder-32b-instruct"
    max_tokens: 2000
  reasoning_llm:
    _type: nim
    model_name: "nvidia/llama-3.3-nemotron-super-49b-v1"

embedders:
  vanna_embedder:
    _type: nim
    model_name: "nvidia/nv-embed-v1"

functions:
  sql_retriever:
    _type: generate_sql_query_and_retrieve_tool
    llm_name: sql_llm
    embedding_name: vanna_embedder
    vector_store_path: "${PWD_PATH}/database"
    db_path: "${PWD_PATH}/database/nasa_turbo.db"
    output_folder: "${PWD_PATH}/output_data"
    vanna_training_data_path: "${PWD_PATH}/vanna_training_data.yaml"
  predict_rul:
    _type: predict_rul_tool
    output_folder: "${PWD_PATH}/output_data"
    scaler_path: "${PWD_PATH}/models/scaler_model.pkl"
    model_path: "${PWD_PATH}/models/xgb_model_fd001.pkl"
  plot_distribution:
    _type: plot_distribution_tool
    output_folder: "${PWD_PATH}/output_data"
  plot_line_chart:
    _type: plot_line_chart_tool
    output_folder: "${PWD_PATH}/output_data"
  plot_comparison:
    _type: plot_comparison_tool
    output_folder: "${PWD_PATH}/output_data" 
  code_execution:
    _type: code_execution
    uri: http://127.0.0.1:6000/execute
    sandbox_type: local
    max_output_characters: 2000
  data_analysis_assistant:
    _type: react_agent
    llm_name: coding_llm
    max_iterations: 5
    tool_names: [sql_retriever, code_execution, predict_rul, plot_distribution, plot_line_chart, plot_comparison]
    system_prompt: |
      ### TASK DESCRIPTION ####
      You are a helpful data analysis assistant that can help with predictive maintenance tasks for a turbofan engine. You will work with planning agent
      that provides a plan to you which you should follow.

      ### TOOLS ###
      You can use the following tools to help with your task:
      {tools}

      ### HOW TO CHOOSE THE RIGHT TOOL ###
      Follow these guidelines while deciding the right tool to use:
      
      1. **SQL Retrieval Tool**
         - Use this tool to retrieve data from the database.
         - NEVER generate SQL queries by yourself, instead pass the top-level instruction to the tool.
      
      2. **Prediction Tools**
         - Use predict_rul for RUL prediction requests.
         - Always call data retrieval tool to get sensor data before predicting RUL.
      
      3. **Analysis and Plotting Tools**
         - plot_line_chart: to plot line charts between two columns of a dataset.
         - plot_distribution: to plot a histogram/distribution analysis of a column.
         - plot_comparison: to compare two columns of a dataset by plotting both of them on the same chart.
         - code_execution: to execute complex custom visualizations not covered by the individual plotting tools or to perform analysis.
      
      4. **Code Execution Tool**
         - Generate python code to execute complex tasks that cannot be done by the provided plotting tools or to perform analysis tasks.
         - All files from code execution tool are saved in the output_data directory
         - Always include the full path when referencing HTML files to users
        - Example: "${PWD_PATH}/output_data/plot_name.html"

        **EXAMPLE PYTHON CODE STRUCTURE (when using code_execution tool):**
        ### START PYTHON CODE ###
        import pandas as pd
        import plotly.graph_objects as go
        
        # Load data using relative path (working directory is /workspace mounted to your output_data)
        data = pd.read_json('your_input_file.json')
        
        # Create your analysis/plot
        fig = go.Figure(data=[go.Scatter(x=data['time_in_cycles'], y=data['sensor_measurement_10'])])
        fig.update_layout(title='Your Plot Title')
        
        # Save to current directory (will appear in your local output_data folder)
        fig.write_html('your_output_file.html')
        print(f"Plot saved to: your_output_file.html")
        ### END PYTHON CODE ###

      ### TYPICAL WORKFLOW FOR EXECUTING A PLAN ###
      Generate all outputs to this path:  "${PWD_PATH}/output_data"
      While generating Python code, use "./filename" to access files in output_data. 
      When passing files to other tools, use the absolute path: "${PWD_PATH}/output_data/filename".
      
      First, Data Extraction
         - Use SQL retrieval tool to fetch required data
      Next, Data Processing and visualization
         - Use existing plotting tools to generate plots
         - If they are not enough to answer the question, use code execution tool to generate custom plots
         - While generating python code follow these rules:
          - Use plotly.js for creating interactive plots
          - Use output_data folder which is the working directory of the code execution sandbox for storing all plots
      Finally, return the result to the user
         - Return processed information to calling agent
         - The user will interact with you through a web frontend, so you should return HTML files if generated by the code execution tool.
         - DO NOT USE MARKDOWN FORMATTING IN YOUR RESPONSE.
         - If the code execution tool responds with a warning in the stderr then ignore it and take action based on the stdout.
      
  
      ### SPECIAL CONSIDERATIONS ###
      
      ### CONSIDERATIONS FOR RUL PLOTTING ###
      When comparing actual and predicted RUL columns, convert the actual RUL column its piecewise RUL values before plotting.
      Piecewise RUL instructions:
      1) Calculate the true failure point by taking the last cycle in your data and adding the final RUL value at that cycle (e.g., if last cycle is 100 with RUL=25, true failure is at cycle 125).
      2) Create the piecewise pattern where if the true failure cycle is greater than MAXLIFE (125), RUL stays flat at MAXLIFE until the "knee point" (true_failure - MAXLIFE), then declines linearly to zero; otherwise RUL just declines linearly from MAXLIFE.
      3) Generate RUL values for each cycle in your data using this pattern - flat section gets constant MAXLIFE value, declining section decreases by (MAXLIFE / remaining_cycles_to_failure) each step.
      4) Replace the actual RUL column in your dataset with these calculated piecewise values while keeping all other columns unchanged.
      5) The result is a "knee-shaped" RUL curve that better represents equipment degradation patterns - flat during early life, then linear decline toward failure.

      ### CONSIDERATIONS FOR EVALUATIONS ###
      The planning agent is required to evaluate your visualizations. Generate a short summary of the plot you intend to 
      generate before calling the plotting tools or generating the python code. Add this summary to "Final answer" section.
      
      
      ### RESPONSE FORMAT ###
      STRICTLY RESPOND IN EITHER OF THE FOLLOWING FORMATS:
      
      ### FORMAT 1 (to share your thoughts) ###
      Question: the input question you must answer
      Thought: you should always think about what to do

      ### FORMAT 2 (to return the final answer) ###
      Question: the input question you must answer
      Thought: you should always think about what to do
      Final Answer: the final answer to the original input question
      
      ### FORMAT 3 (when using a tool) ###
      Question: the input question you must answer
      Thought: you should always think about what to do
      Action: the action to take, should be one of [{tool_names}]
      Action Input: the input to the tool (if there is no required input, include "Action Input: None")
      Observation: wait for the tool to finish execution and return the result

workflow:
  _type: reasoning_agent
  augmented_fn: data_analysis_assistant
  llm_name: reasoning_llm
  verbose: true
  reasoning_prompt_template: |
    ### DESCRIPTION ###
    You are a Data Analysis Reasoning and Planning Expert specialized in analyzing turbofan engine sensor data and predictive maintenance tasks. 
    You are tasked with creating detailed execution plans for addressing user queries while being conversational and helpful.

    Your Role and Capabilities:**
    - Expert in turbofan engine data analysis, predictive maintenance, and anomaly detection
    - Provide conversational responses while maintaining technical accuracy
    - Create step-by-step execution plans using available tools which will be invoked by a data analysis assitant
    
    **You are given a data analysis assistant to execute your plan, all you have to do is generate the plan**
    DO NOT USE MARKDOWN FORMATTING IN YOUR RESPONSE.

    ### ASSITANT DESCRIPTION ###
    {augmented_function_desc}

    ### TOOLS AVAILABLE TO THE ASSISTANT ###
    {tools}

    ### CONTEXT ###
    You work with turbofan engine sensor data from multiple engines in a fleet. The data contains:
    - **Time series data** from different engines, each with unique wear patterns and operational history separated into 
    four datasets (FD001, FD002, FD003, FD004), each dataset is further divided into training and test subsets.
    - **26 data columns**: unit number, time in cycles, 3 operational settings, and 21 sensor measurements  
    - **Engine lifecycle**: Engines start operating normally, then develop faults that grow until system failure
    - **Predictive maintenance goal**: Predict Remaining Useful Life (RUL) - how many operational cycles before failure
    - **Data characteristics**: Contains normal operational variation, sensor noise, and progressive fault development    
    This context helps you understand user queries about engine health, sensor patterns, failure prediction, and maintenance planning.
    REMEMBER TO RELY ON DATA ANALYSIS ASSITANT TO RETRIEVE DATA FROM THE DATABASE.
    
    ### SPECIAL TASKS ###
    Follow the below mentioned instructions to create an executable plan for the user's query when a special task is requested by the user. 
    For other tasks, simply come up with good plan based on your existing reasoning skills.
    
    ### SPECIAL TASK 1: Anomaly Detection ###
    1) First ask the assistant to retrieve sensor measurement information for the same engine number from both training and test datasets across different cycle times in increasing order.
    2) Then, ask the assitatnt to use the measurement from training data to calculate statistical baselines (mean, standard deviation, moving averages), because it represents what
    normal operational behvaior looks like.
    3) Next, ask the assistant to apply multiple statistical approaches to identify anomalies in test data:
       - **Z-Score Analysis**: Compare test values against training data mean/std deviation using threshold (typically 3)
       - **Moving Statistical Analysis**: Use rolling windows from training data to detect dynamic anomalies
       - Flag data points that exceed statistical thresholds from training data as potential anomalies
    4) Finally, ask the assistant to generate comprehensive plots showing test data with anomalies highlighted
       - Use different colors/markers to distinguish between normal data and show all different types of anomalies
       - Include hover information and legends for clear interpretation
       - Save visualizations as interactive HTML files for detailed analysis

    <You will soon be updated with more special tasks>

    ### GUIDELINES ###
    **Send the path to any HTML files generated to users** when tools return them.

    **User Input:**
    {input_text}

    Analyze the input and create an appropriate execution plan in bullet points.

eval:
  general:
    output:
      dir: "${PWD_PATH}/output_data"
      cleanup: true
    dataset:
      _type: json
      file_path: "${PWD_PATH}/eval_data.json"
    # Add delays to prevent rate limiting
    query_delay: 2  # seconds between queries
    max_concurrent: 1  # process queries sequentially
    profiler:
      # Compute inter query token uniqueness
      token_uniqueness_forecast: true
      # Compute expected workflow runtime
      workflow_runtime_forecast: true
      # Compute inference optimization metrics
      compute_llm_metrics: true
      # Avoid dumping large text into the output CSV (helpful to not break structure)
      csv_exclude_io_text: true
      # Idenitfy common prompt prefixes
      prompt_caching_prefixes:
        enable: true
        min_frequency: 0.1
      bottleneck_analysis:
        # Can also be simple_stack
        enable_nested_stack: true
      concurrency_spike_analysis:
        enable: true
        spike_threshold: 7

  evaluators:
    rag_accuracy:
      _type: ragas
      metric: AnswerAccuracy
      llm_name: reasoning_llm
    rag_groundedness:
      _type: ragas
      metric: ResponseGroundedness
      llm_name: reasoning_llm
    rag_relevance:
      _type: ragas
      metric: ContextRelevance
      llm_name: reasoning_llm

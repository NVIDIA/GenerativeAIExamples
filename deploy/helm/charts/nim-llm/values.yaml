## @section Deployment parameters
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## @param affinity [object] [default: {}] Affinity settings for deployment.
affinity: {}

## @param containerSecurityContext [object] Sets privilege and access control settings for container (Only affects the main container, not pod-level).
containerSecurityContext: {}
  # seLinuxOptions: null
  # runAsUser: 1001
  # runAsGroup: 1001
  # runAsNonRoot: true
  # privileged: false
  # readOnlyRootFilesystem: false
  # allowPrivilegeEscalation: false
  # capabilities:
  #   drop:
  #     - ALL
  # seccompProfile:
  #   type: "RuntimeDefault"

## @param customCommand [array] Overrides command line options sent to the NIM with the array listed here. 
## For advanced use if necessary only
customCommand: []

## @param customArgs [array] Overrides command line arguments of the NIM container with the array listed here. 
## For advanced use if necessary only
customArgs: []

## @param env [array] Adds arbitrary environment variables to the main container.
env: []
# - name: OPENBLAS_NUM_THREADS
#   value: "1"
# - name: POD_NAME
#   valueFrom:
#     fieldRef:
#       fieldPath: metadata.name

## @param extraVolumes [object] Adds arbitrary additional volumes to the deployment set definition.
extraVolumes: {}
  # my-volume-name:
  #   emptyDir: {}

## @param extraVolumeMounts [object] Adds volume mounts to the main container from `extraVolumes`.
extraVolumeMounts: {}
  # my-volume-name:
  #   mountPath: /mnt/myvolume

## @param image.repository [string] Specifies the NIM-LLM Image to deploy.
## @param image.tag [string] Specifies the image tag or version.
## @param image.pullPolicy [string] Sets the image pull policy.
image:
  repository: nvcr.io/nim/meta/llama3-8b-instruct
  pullPolicy: IfNotPresent
  # Tag overrides the image tag whose default is the chart appVersion.
  tag: ""

## @extra imagePullSecrets Specifies a list of secret names that are needed for the main container and any init containers.
## @skip imagePullSecrets[0].name
imagePullSecrets:
  - name: ngc-secret  # change this to whatever your image pull secret should be

## @extra initContainers [object] Specifies model init containers, if needed.
## @param initContainers.ngcInit [object] Legacy containers only. Instantiate and configure an NGC init container. It should either have NGC CLI pre-installed or `wget` + `unzip` pre-installed -- must not be `musl`-based (alpine).
## @param initContainers.extraInit [array] Fully specify any additional init containers your use case requires.
initContainers:
  ngcInit: {} # disabled by default
  #  imageName: mydocker-registry/myngcimage # should either have ngc cli pre-installed or wget + unzip pre-installed -- must not be musl-based (alpine)
  #  imageTag: 0.1
  #  secretName: # name of kube secret for ngc keys named NGC_CLI_API_KEY (required) and NGC_DECRYPT_KEY (optional)
  #  containerSecurityContext: # ONLY include if you are setting this for the main container
  #    seLinuxOptions: null
  #    runAsUser: 1001  # may not be able to do an NGC CLI install without root if you choose this
  #    runAsGroup: 1001
  #    runAsNonRoot: true
  #    privileged: false
  #    readOnlyRootFilesystem: false
  #    allowPrivilegeEscalation: false
  #    capabilities:
  #      drop:
  #        - ALL
  #    seccompProfile:
  #      type: "RuntimeDefault"
  #  env:
  #    STORE_MOUNT_PATH: /nvme/model-store
  #    NGC_CLI_ORG: # ngc org where model lives
  #    NGC_CLI_TEAM: # ngc team where model lives
  #    NGC_MODEL_NAME: # model name in ngc
  #    NGC_MODEL_VERSION: # model version in ngc
  #    NGC_EXE: ngc  # path to ngc cli, if pre-installed in container
  #    DOWNLOAD_NGC_CLI: "false"  # set to string 'true' if container should download and install ngc cli
  #    NGC_CLI_VERSION: "3.34.1"  # version of ngc cli to download (only matters if downloading)
  #    TARFILE: yes  # tells the script to untar the model. Set to "" to turn off
  #    MODEL_NAME: # actual model name, once downloaded
  extraInit: [] # Add any additional init containers your use case requires.
  # -  # full init container definition here

## @param healthPort Specifies health check port. -- for use with `models.legacyCompat` only since current NIMs have no separate port.
healthPort: 8000

## @param nodeSelector [object] Sets node selectors for the NIM -- for example `nvidia.com/gpu.present: "true"`.
nodeSelector: {}  # likely best to set this to `nvidia.com/gpu.present: "true"` depending on cluster setup

## @param podAnnotations [object] Sets additional annotations on the main deployment pods.
podAnnotations: {}

## @extra podSecurityContext Specifies security context settings for pod.
## @param podSecurityContext.runAsUser Specify user UID for pod.
## @param podSecurityContext.runAsGroup Specify group ID for pod.
## @param podSecurityContext.fsGroup Specify file system owner group id.
podSecurityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

## @param replicaCount Specify static replica count for deployment.
replicaCount: 1

## @extra resources [object] Specify resources limits and requests for the running service. 
## @param resources.limits.nvidia.com/gpu Specify number of GPUs to present to the running service.
resources:
  limits:
    nvidia.com/gpu: 1  # Number of GPUs to present to the running service

## @exta serviceAccount Options to specify service account for the deployment.
## @param serviceAccount.create Specifies whether a service account should be created.
## @param serviceAccount.annotations [object] Sets annotations to be added to the service account.
## @param serviceAccount.name Specifies the name of the service account to use. If it is not set and create is `true`, a name is generated using a `fullname` template.
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is `true`, a name is generated using the `fullname` template
  name: ""

## @param statefulSet.enabled Enables `statefulset` deployment. Enabling `statefulSet` allows PVC templates for scaling. If using central PVC with RWX `accessMode`, this isn't needed.
statefulSet:
  enabled: true

## @extra tolerations Specify tolerations for pod assignment. Allows the scheduler to schedule pods with matching taints.
## @skip tolerations[0].key
## @skip tolerations[0].operator
## @skip tolerations[0].effect
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"


## @section Autoscaling parameters
## @descriptionStart
## Values used for creating a `Horizontal Pod Autoscaler`. If autoscaling is not enabled, the rest are ignored.
## NVIDIA recommends usage of the custom metrics API, commonly implemented with the prometheus-adapter.
## Standard metrics of CPU and memory are of limited use in scaling NIM.
## @descriptionEnd
## @param autoscaling.enabled Enables horizontal pod autoscaler.
## @param autoscaling.minReplicas Specify minimum replicas for autoscaling.
## @param autoscaling.maxReplicas Specify maximum replicas for autoscaling.
## @param autoscaling.metrics Array of metrics for autoscaling.
autoscaling:
  enabled: false 
  minReplicas: 1
  maxReplicas: 10
  metrics: []


## @section Ingress parameters
## @param ingress.enabled Enables ingress.
## @param ingress.className Specify class name for Ingress.
## @param ingress.annotations Specify additional annotations for ingress.
## @extra ingress.hosts Specify list of hosts each containing lists of paths.
## @param ingress.hosts[0].host Specify name of host.
## @param ingress.hosts[0].paths[0].path Specify ingress path. 
## @param ingress.hosts[0].paths[0].pathType Specify path type.
## @param ingress.hosts[0].paths[0].serviceType Specify service type. It can be can be `nemo` or `openai` -- make sure your model serves the appropriate port(s).
## @param ingress.tls Specify list of pairs of TLS `secretName` and hosts.
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
          serviceType: openai  # can be nemo or openai -- make sure your model serves the appropriate port(s)

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local


## @section Probe parameters
## @param livenessProbe.enabled Enables `livenessProbe``.
## @param livenessProbe.method `LivenessProbe` `http` or `script`, but no script is currently provided.
## @param livenessProbe.command `LivenessProbe`` script command to use (unsupported at this time).
## @param livenessProbe.path `LivenessProbe`` endpoint path.
## @param livenessProbe.initialDelaySeconds Initial delay seconds for `livenessProbe`.
## @param livenessProbe.timeoutSeconds Timeout seconds for `livenessProbe`.
## @param livenessProbe.periodSeconds Period seconds for `livenessProbe`.
## @param livenessProbe.successThreshold Success threshold for `livenessProbe`.
## @param livenessProbe.failureThreshold Failure threshold for `livenessProbe`.
livenessProbe:
  enabled: true
  method: http
  command:
    - myscript.sh
  path: /v1/health/live  # correct for LLM container
  initialDelaySeconds: 15
  timeoutSeconds: 1
  periodSeconds: 10
  successThreshold: 1
  failureThreshold: 3

## @param readinessProbe.enabled Enables `readinessProbe`.
## @param readinessProbe.path Readiness Endpoint Path.
## @param readinessProbe.initialDelaySeconds Initial delay seconds for `readinessProbe`.
## @param readinessProbe.timeoutSeconds Timeout seconds for `readinessProbe`.
## @param readinessProbe.periodSeconds Period seconds for `readinessProbe`.
## @param readinessProbe.successThreshold Success threshold for `readinessProbe`.
## @param readinessProbe.failureThreshold Failure threshold for `readinessProbe`.
readinessProbe:
  enabled: true
  path: /v1/health/ready  # correct for LLM container
  initialDelaySeconds: 15
  timeoutSeconds: 1
  periodSeconds: 10
  successThreshold: 1
  failureThreshold: 3

## @param startupProbe.enabled Enables `startupProbe`.
## @param startupProbe.path `StartupProbe` Endpoint Path.
## @param startupProbe.initialDelaySeconds Initial delay seconds for `startupProbe`.
## @param startupProbe.timeoutSeconds Timeout seconds for `startupProbe`.
## @param startupProbe.periodSeconds Period seconds for `startupProbe`.
## @param startupProbe.successThreshold Success threshold for `startupProbe`.
## @param startupProbe.failureThreshold Failure threshold for `startupProbe`.
startupProbe:
  enabled: true
  path: /v1/health/ready  # correct for LLM container
  initialDelaySeconds: 40
  timeoutSeconds: 1
  periodSeconds: 10
  successThreshold: 1
  failureThreshold: 180


## @section Metrics parameters
## @extra metrics Opens the metrics port for the triton inference server on port 8002.
## @param metrics.enabled Enables metrics endpoint -- for `legacyCompat` only since current NIMs serve metrics on the OpenAI API port.
## @extra serviceMonitor Options for `serviceMonitor` to use the Prometheus Operator and the primary service object.
## @param metrics.serviceMonitor.enabled Enables `serviceMonitor` creation.
## @param metrics.serviceMonitor.additionalLabels [object] Specify additional labels for ServiceMonitor.
metrics:
  enabled: true
  serviceMonitor:  # for use with the Prometheus Operator and the primary service object
    enabled: false
    additionalLabels: {}


## @section Models parameters
## @param model.nimCache [string] Path to mount writeable storage or pre-filled model cache for the NIM.
## @param model.name Specifies the name of the model in the API (usually, the name of the NIM). This is mostly used for helm tests and is usually otherwise optional. This must match the name from _/v1/models_ to allow `helm test <release-name>` to work. In `legacyCompat`, this is required and sets the name of the model in /v1/models.
## @param model.ngcAPISecret [string] Name of pre-existing secret with a key named `NGC_API_KEY` that contains an API key for NGC model downloads.
## @param model.ngcAPIKey [string] NGC API key literal to use as the API secret and image pull secret when set.
## @param model.openaiPort Specifies the Open AI API Port.
## @param model.labels [object] Specifies extra labels to be added on deployed pods.
## @param model.jsonLogging Turn JSON lines logging on or off. Defaults to true.
## @param model.logLevel Log level of NIM service. Possible values of the variable are TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL.
## @section Deprecated and Legacy Model parameters
## @param model.legacyCompat Set `true` to enable compatibility with pre-release NIM versions prior to 1.0.0.
## @param model.numGpus (deprecated) Specify GPU requirements for the model.
## @param model.subPath (deprecated) Specify path within the model volume to mount if not the root -- default works with `ngcInit` and persistent volume. (`legacyCompat` only)
## @param model.modelStorePath (deprecated) Specify location of unpacked model.
model:
  nimCache: /model-store
  name: meta/llama3-8b-instruct # optional name of the model in the OpenAI API -- used in `helm test`
  ngcAPISecret: "ngc-api"
  ngcAPIKey: ""
  openaiPort: 8000
  labels: {}  # any extra labels desired on deployed pods
  jsonLogging: true
  logLevel: INFO
  # All options below are deprecated
  legacyCompat: false # enable compatiblity with pre-release NIM versions prior to 1.0.0
  numGpus: 1 # GPU requirements for the model
  subPath: model-store # `legacyCompat` only -- path within the model volume to mount if not the root -- default works with ngcInit and persistent volume
  # openai_port: 8005 # deprecated for of openaiPort for backward legacy compatibility
  # nemo_port: 8006  # deprecated for backward legacy compatibility
  modelStorePath: ""  # location of unpacked model
  # numWorkers: 1
  # trtModelName: "trt_llm"  # is not required in most cases -- name of trtllm model
  # tritonModelName: "ensemble"  # not required for NVIDIA trtllm models
  # dataStoreURL: "gateway-api:9009"  # URL of data store service, if used
  # customizationSource: "NDS"  # Type of customization source, can be NDS, SERVICE or DIRECTORY -- optional


## @section Storage parameters
## @extra persistence Specify settings to modify the path `/model-store` if `model.legacyCompat` is enabled else `/.cache` volume where the model is served from.
## @param persistence.enabled Enables the use of persistent volumes.
## @param persistence.existingClaim Specifies an existing persistent volume claim. If using `existingClaim`, run only one replica or use a `ReadWriteMany` storage setup.
## @param persistence.storageClass [nullable] Specifies the persistent volume storage class. If set to `"-"`, this disables dynamic provisioning. If left undefined or set to null, the cluster default storage provisioner is used.
## @param persistence.accessMode Specify `accessMode`. If using an NFS or similar setup, you can use `ReadWriteMany`.
## @param persistence.stsPersistentVolumeClaimRetentionPolicy.whenDeleted Specifies persistent volume claim retention policy when deleted. Only used with Stateful Set volume templates.
## @param persistence.stsPersistentVolumeClaimRetentionPolicy.whenScaled Specifies persistent volume claim retention policy when scaled. Only used with Stateful Set volume templates.
## @param persistence.size Specifies the size of the persistent volume claim (for example 40Gi).
## @param persistence.annotations [object] Adds annotations to the persistent volume claim.
persistence:
  enabled: false
  existingClaim: ""  # if using existingClaim, run only one replica or use a `ReadWriteMany` storage setup
  # Persistent Volume Storage Class
  # If defined, storageClassName: <storageClass>
  # If set to "-", storageClassName: "", which disables dynamic provisioning.
  # If undefined (the default) or set to null, no storageClassName spec is
  #   set, choosing the default provisioner.
  storageClass: ""
  accessMode: ReadWriteOnce  # If using an NFS or similar setup, you can use `ReadWriteMany`
  stsPersistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  size: 50Gi  # size of claim in bytes (for example 8Gi)
  annotations: {}

## @extra hostPath Configures model cache on local disk on the nodes using `hostPath` -- for special cases. You should understand the security implications before using this option.
## @param hostPath.enabled Enable `hostPath`.
## @param hostPath.path Specifies path on the node used as a `hostPath` volume. 
hostPath:
  enabled: false
  path: /model-store

## @extra nfs Configures the model cache to sit on shared direct-mounted NFS. NOTE: you cannot set mount options using direct NFS mount to pods without a node-intalled nfsmount.conf. An NFS-based `PersistentVolumeClaim` is likely better in most cases.
## @param nfs.enabled Enables direct pod NFS mount.
## @param nfs.path Specify path on NFS server to mount.
## @param nfs.server Specify NFS server address.
## @param nfs.readOnly Set to true to mount as read-only.
nfs:
  enabled: false
  server: nfs-server.example.com
  path: /exports
  readOnly: false

## @section Service parameters
## @param service.type Specifies the service type for the deployment.
## @param service.name Overrides the default service name
## @param service.openaiPort Specifies Open AI Port for the service.
## @param service.annotations [object] Specify additional annotations to be added to service.
## @param service.labels Specifies additional labels to be added to service.
service:
  type: ClusterIP
  openaiPort: 8000
  annotations: {}
  labels: {}
  name: ""  # override the default service name 
  # below options are deprecated
  # http_port: 8000  # exposes http interface used in healthchecks to the service
  # grpc_port: 8001  # exposes the triton grpc interface
  # metrics_port: 8002  # expose metrics through the main service
  # openai_port: 8005
  # nemo_port: 8006

## @section Multi-node parameters
## @descriptionStart
## Large models that must span multiple nodes do not work on plain Kubernetes with the GPU Operator alone at this time.
## Optimized TensorRT profiles, when selected automatically or by environment variable, require either
## [LeaderWorkerSets](https://github.com/kubernetes-sigs/lws) or the [MPI Operator]](https://github.com/kubeflow/mpi-operator)'s `MPIJobs` to be installed.
## Since `MPIJob` is a batch-type resource that is not designed with service stability and reliability in mind, you should use LeaderWorkerSets if your cluster version allows it.
## Only optimized profiles are supported for multi-node deployment at this time.
## @descriptionEnd
## @param multiNode.enabled Enables multi-node deployments.
## @param multiNode.clusterStartTimeout Sets the number of seconds to wait for worker nodes to come up before failing.
## @param multiNode.gpusPerNode Number of GPUs that will be presented to each pod. In most cases, this should match `resources.limits.nvidia.com/gpu`.
## @param multiNode.workers Specifies how many worker pods per multi-node replica to launch.
## @param multiNode.leaderWorkerSet.enabled NVIDIA recommends you use `LeaderWorkerSets` to deploy. If disabled, defaults to using `MPIJob` from mpi-operator.
## @param multiNode.existingSSHSecret [string,nullable] Sets the SSH private key for MPI to an existing secret. Otherwise, the Helm chart generates a key randomly during installation.
## @param multiNode.mpiJob.workerAnnotations Annotations only applied to workers for `MPIJob`, if used. This may be necessary to ensure the workers connect to `CNI`s offered by `multus` and the network operator, if used.
## @param multiNode.mpiJob.launcherResources Resources section to apply only to the launcher pods in `MPIJob`, if used. Launchers do not get the chart resources restrictions. Only workers do, since they require GPUs.
## @param multiNode.optimized.enabled Enables optimized multi-node deployments (currently the only option supported).
multiNode:
  clusterStartTimeout: 300
  enabled: false
  workers: 1
  leaderWorkerSet:
    enabled: true
  existingSSHSecret: ""
  mpiJob:
    workerAnnotations: {}
    launcherResources: {}
  gpusPerNode: 1
  optimized:
    enabled: true

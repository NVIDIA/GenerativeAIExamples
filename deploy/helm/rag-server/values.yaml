replicaCount: 1

# Secrets
imagePullSecret:
  name: "ngc-secret"
  registry: "nvcr.io"
  username: "$oauthtoken"
  password: ""
  create: true

ngcApiSecret:
  name: "ngc-api"
  password: ""
  create: true

# RAG server config
image:
  repository: nvcr.io/nvidia/blueprint/rag-server
  tag: "2.0.0"
  pullPolicy: IfNotPresent

server:
  workers: 8

envVars:
  EXAMPLE_PATH: "src/"

  ##===MINIO specific configurations which is used to store the multimodal base64 content===
  MINIO_ENDPOINT: "rag-minio:9000"
  MINIO_ACCESSKEY: "minioadmin"
  MINIO_SECRETKEY: "minioadmin"

  ##===Vector DB specific configurations===
  # URL on which vectorstore is hosted
  APP_VECTORSTORE_URL: "http://milvus:19530"
  # Type of vectordb used to store embedding supported type milvus
  APP_VECTORSTORE_NAME: "milvus"
  # Type of vectordb search to be used
  APP_VECTORSTORE_SEARCHTYPE: "dense"
  # vectorstore collection name to store embeddings
  COLLECTION_NAME: "multimodal_data"
  APP_RETRIEVER_SCORETHRESHOLD: "0.25"
  # Top K from vector DB, which goes as input to reranker model - not applicable if ENABLE_RERANKER is set to False
  VECTOR_DB_TOPK: "100"
  # Number of document chunks to insert in LLM prompt
  APP_RETRIEVER_TOPK: "10"

  ##===LLM Model specific configurations===
  APP_LLM_MODELNAME: "meta/llama-3.1-70b-instruct"
  # URL on which LLM model is hosted. If "", Nvidia hosted API is used
  APP_LLM_SERVERURL: "nim-llm:8000"

  ##===Query Rewriter Model specific configurations===
  APP_QUERYREWRITER_MODELNAME: "meta/llama-3.1-70b-instruct"
  # URL on which query rewriter model is hosted. If "", Nvidia hosted API is used
  APP_QUERYREWRITER_SERVERURL: "nim-llm:8000"

  ##===Embedding Model specific configurations===
  # URL on which embedding model is hosted. If "", Nvidia hosted API is used
  APP_EMBEDDINGS_SERVERURL: "nemo-retriever-embedding-ms:8000"
  APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"

  ##===Reranking Model specific configurations===
  # URL on which ranking model is hosted. If "", Nvidia hosted API is used
  APP_RANKING_SERVERURL: "nemo-retriever-reranking-ms:8000"
  APP_RANKING_MODELNAME: "nvidia/llama-3.2-nv-rerankqa-1b-v2"
  ENABLE_RERANKER: "True"

  # === Text Splitter ===
  APP_TEXTSPLITTER_CHUNKSIZE: "2000"
  APP_TEXTSPLITTER_CHUNKOVERLAP: "200"

  # === General ===
  # Choose whether to enable citations in the response
  ENABLE_CITATIONS: "True"
  # Choose whether to enable/disable guardrails
  ENABLE_GUARDRAILS: "False"
  # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
  LOGLEVEL: "INFO"
  # enable multi-turn conversation in the rag chain - this controls conversation history usage
  # while doing query rewriting and in LLM prompt
  ENABLE_MULTITURN: "True"
  # enable query rewriting for multiturn conversation in the rag chain.
  # This will improve accuracy of the retrieiver pipeline but increase latency due to an additional LLM call
  ENABLE_QUERYREWRITER: "False"
  # number of last n chat messages to consider from the provided conversation history
  CONVERSATION_HISTORY: "5"

  # === Tracing ===
  APP_TRACING_ENABLED: "False"
  # HTTP endpoint
  APP_TRACING_OTLPHTTPENDPOINT: "http://rag-opentelemetry-collector:4318/v1/traces"
  # GRPC endpoint
  APP_TRACING_OTLPGRPCENDPOINT: "grpc://rag-opentelemetry-collector:4317"

  # === Reflection ===
  # enable reflection (context relevance and response groundedness checking) in the rag chain
  ENABLE_REFLECTION: "False"
  # Maximum number of context relevance loop iterations
  MAX_REFLECTION_LOOP: "3"
  # Minimum relevance score threshold (0-2)
  CONTEXT_RELEVANCE_THRESHOLD: "1"
  # Minimum groundedness score threshold (0-2)
  RESPONSE_GROUNDEDNESS_THRESHOLD: "1"
  # reflection llm
  REFLECTION_LLM: "mistralai/mixtral-8x22b-instruct-v0.1"
  # reflection llm server url. If "", Nvidia hosted API is used
  REFLECTION_LLM_SERVERURL: ""

  # Choose whether to enable source metadata in document content during generation
  ENABLE_SOURCE_METADATA: "true"

  # Whether to filter content within <think></think> tags in model responses
  FILTER_THINK_TOKENS: "true"

# Ingestor Server
ingestor-server:
  enabled: true
  imagePullSecret:
    name: "ngc-secret"
    create: false
  image:
    repository: nvcr.io/nvidia/blueprint/ingestor-server
    tag: "2.0.0"
    pullPolicy: IfNotPresent
  server:
    workers: 1
  envVars:
    # === Vector Store Configurations ===
    APP_VECTORSTORE_URL: "http://milvus:19530"
    APP_VECTORSTORE_NAME: "milvus"
    APP_VECTORSTORE_SEARCHTYPE: "dense"
    APP_VECTORSTORE_ENABLEGPUINDEX: "True"
    APP_VECTORSTORE_ENABLEGPUSEARCH: "True"
    COLLECTION_NAME: "multimodal_data"

    # === MinIO Configurations ===
    MINIO_ENDPOINT: "rag-minio:9000"
    MINIO_ACCESSKEY: "minioadmin"
    MINIO_SECRETKEY: "minioadmin"

    # === Embeddings Configurations ===
    APP_EMBEDDINGS_SERVERURL: "nemo-retriever-embedding-ms:8000"
    APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"
    APP_EMBEDDINGS_DIMENSIONS: "2048"

    # === NV-Ingest Configurations ===
    APP_NVINGEST_MESSAGECLIENTHOSTNAME: "rag-nv-ingest"
    APP_NVINGEST_MESSAGECLIENTPORT: "7670"

    # === NV-Ingest extraction configurations ===
    APP_NVINGEST_EXTRACTMETHOD: "pdfium"  # Method used for text extraction
    APP_NVINGEST_EXTRACTTEXT: "True"  # Enable text extraction
    APP_NVINGEST_EXTRACTTABLES: "True"  # Enable table extraction
    APP_NVINGEST_EXTRACTCHARTS: "True"  # Enable chart extraction
    APP_NVINGEST_EXTRACTIMAGES: "False"  # Enable image extraction
    APP_NVINGEST_TEXTDEPTH: "page"  # Extract text by "page" or "document"

    # === NV-Ingest caption configurations ===
    APP_NVINGEST_CAPTIONMODELNAME: "meta/llama-3.2-11b-vision-instruct"  # Model name for captioning
    APP_NVINGEST_CAPTIONENDPOINTURL: ""  # Endpoint URL for captioning model

    # === General ===
    ENABLE_CITATIONS: "True"
    LOGLEVEL: "INFO"

    # === NV-Ingest splitting configurations ===
    APP_NVINGEST_CHUNKSIZE: "1024"  # Size of chunks for splitting
    APP_NVINGEST_CHUNKOVERLAP: "150"  # Overlap size for chunks

  # NV-Ingest
  nv-ingest:
    imagePullSecrets:
      - name: "ngc-secret"
    ngcApiSecret:
      create: false
    ngcImagePullSecret:
      create: false
    image:
      repository: "nvcr.io/nvidia/nemo-microservices/nv-ingest"
      tag: "25.3.0"
    resources:
      limits:
        nvidia.com/gpu: 0
    envVars:
      MAX_INGEST_PROCESS_WORKERS: 16
      INGEST_LOG_LEVEL: DEFAULT
      INGEST_EDGE_BUFFER_SIZE: 64
      MRC_IGNORE_NUMA_CHECK: 1
      READY_CHECK_ALL_COMPONENTS: False
      REDIS_MORPHEUS_TASK_QUEUE: morpheus_task_queue
      AUDIO_GRPC_ENDPOINT: "audio:50051"
      AUDIO_INFER_PROTOCOL: "grpc"
      CUDA_VISIBLE_DEVICES: "0"
      MAX_INGEST_PROCESS_WORKERS: 16
      EMBEDDING_NIM_ENDPOINT: "http://nemo-retriever-embedding-ms:8000/v1"
      MESSAGE_CLIENT_HOST: "rag-redis-master"
      MESSAGE_CLIENT_PORT: 6379
      MESSAGE_CLIENT_TYPE: "redis"
      MINIO_INTERNAL_ADDRESS: "rag-minio:9000"
      MILVUS_ENDPOINT: "http://milvus:19530"
      MINIO_BUCKET: "nv-ingest"
      NEMORETRIEVER_PARSE_HTTP_ENDPOINT: "http://nemoretriever-parse:8000/v1/chat/completions"
      NEMORETRIEVER_PARSE_INFER_PROTOCOL: "http"
      OTEL_EXPORTER_OTLP_ENDPOINT: "otel-collector:4317"
      PADDLE_GRPC_ENDPOINT: "nv-ingest-paddle:8001"
      PADDLE_HTTP_ENDPOINT: "http://nv-ingest-paddle:8000/v1/infer"
      PADDLE_INFER_PROTOCOL: "grpc"
      YOLOX_GRPC_ENDPOINT: "nemoretriever-page-elements-v2:8001"
      YOLOX_HTTP_ENDPOINT: "http://nemoretriever-page-elements-v2:8000/v1/infer"
      YOLOX_INFER_PROTOCOL: "grpc"
      YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT: "nemoretriever-graphic-elements-v1:8001"
      YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT: "http://nemoretriever-graphic-elements-v1:8000/v1/infer"
      YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT: "nemoretriever-table-structure-v1:8001"
      YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT: "http://nemoretriever-table-structure-v1:8000/v1/infer"
      YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL: "grpc"
      VLM_CAPTION_ENDPOINT: "http://vlm-ms:8000/v1/chat/completions"
      VLM_CAPTION_MODEL_NAME: "meta/llama-3.2-11b-vision-instruct"
      MODEL_PREDOWNLOAD_PATH: "/workspace/models/"

    paddleocr-nim:
      image:
        repository: nvcr.io/nim/baidu/paddleocr
        tag: "1.2.0"
      imagePullSecrets:
      - name: ngc-secret

    nemoretriever-graphic-elements-v1:
      image:
        repository: nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1
        tag: "1.2.0"

    nemoretriever-page-elements-v2:
      image:
        repository: nvcr.io/nim/nvidia/nemoretriever-page-elements-v2
        tag: "1.2.0"

    nemoretriever-table-structure-v1:
      image:
        repository: nvcr.io/nim/nvidia/nemoretriever-table-structure-v1
        tag: "1.2.0"

    nim-vlm-text-extraction:
      image:
        repository: "nvcr.io/nim/nvidia/nemoretriever-parse"
        tag: "1.2"
      deployed: false

    nim-vlm-image-captioning:
      deployed: false

    nvidia-nim-llama-32-nv-embedqa-1b-v2:
      image:
        repository: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2
        tag: "1.5.0"
      deployed: false

    milvus:
      image:
        all:
          repository: milvusdb/milvus
          tag: v2.5.3-gpu
          pullPolicy: IfNotPresent
      standalone:
        resources:
          limits:
            nvidia.com/gpu: 1
      fullnameOverride: "milvus"

    otelDeployed: false
    zipkinDeployed: false
    # Uncomment to enable OpenTelemetry Collector using NV-Ingest deployed Otel and Zipkin
    # otelDeployed: false
    # zipkinDeployed: false
    # opentelemetry-collector:
    #   ports:
    #     metrics:
    #       enabled: true
    #       containerPort: 8889
    #       servicePort: 8889
    #       protocol: TCP
    #   serviceMonitor:
    #     enabled: true
    #   config:
    #     exporters:
    #       prometheus:
    #         endpoint: ${env:MY_POD_IP}:8889
    #     service:
    #       pipelines:
    #         metrics:
    #           exporters:
    #             - debug
    #             - prometheus
    #           processors:
    #             - memory_limiter
    #             - batch
    #           receivers:
    #             - otlp
    #             - prometheus

# NIMs
nim-llm:
  enabled: true
  service:
    name: "nim-llm"
  image:
      repository: nvcr.io/nim/meta/llama-3.1-70b-instruct-pb24h2
      pullPolicy: IfNotPresent
      tag: "1.3.4"
  resources:
    limits:
      nvidia.com/gpu: 2
    requests:
      nvidia.com/gpu: 2
  model:
    ngcAPIKey: ""
    name: "meta/llama-3.1-70b-instruct"


nvidia-nim-llama-32-nv-embedqa-1b-v2:
  enabled: true
  service:
    name: "nemo-retriever-embedding-ms"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2
    tag: "1.5.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
        nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

text-reranking-nim:
  enabled: true
  service:
    name: "nemo-retriever-reranking-ms"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2
    tag: "1.3"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

## Observability Support
serviceMonitor:
  enabled: false
opentelemetry-collector:
  enabled: false
  mode: deployment
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: '${env:MY_POD_IP}:4317'
          http:
            cors:
              allowed_origins:
                - "*"
    exporters:
      # NOTE: Prior to v0.86.0 use `logging` instead of `debug`.
      zipkin:
        endpoint: "http://rag-zipkin:9411/api/v2/spans"
      debug:
        verbosity: detailed
      prometheus:
        endpoint: ${env:MY_POD_IP}:8889
    extensions:
      health_check: {}
      zpages:
        endpoint: 0.0.0.0:55679
    processors:
      batch: {}
      tail_sampling:
        # filter out health checks
        # https://github.com/open-telemetry/opentelemetry-collector/issues/2310#issuecomment-1268157484
        policies:
          - name: drop_noisy_traces_url
            type: string_attribute
            string_attribute:
              key: http.target
              values:
                - \/health
              enabled_regex_matching: true
              invert_match: true
      transform:
        trace_statements:
          - context: span
            statements:
              - set(status.code, 1) where attributes["http.path"] == "/health"

              # after the http target has been anonymized, replace other aspects of the span
              - replace_match(attributes["http.route"], "/v1", attributes["http.target"]) where attributes["http.target"] != nil

              # replace the title of the span with the route to be more descriptive
              - replace_pattern(name, "/v1", attributes["http.route"]) where attributes["http.route"] != nil

              # set the route to equal the URL if it's nondescriptive (for the embedding case)
              - set(name, Concat([name, attributes["http.url"]], " ")) where name == "POST"
    service:
      extensions: [zpages, health_check]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [debug, zipkin]
          processors: [tail_sampling, transform]
        metrics:
          exporters:
            - debug
            - prometheus
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
            - prometheus
        logs:
          receivers: [otlp]
          exporters: [debug]
          processors: [batch]
  ports:
    metrics:
      enabled: true
      containerPort: 8889
      servicePort: 8889
      protocol: TCP
zipkin:
  enabled: false
kube-prometheus-stack:
  enabled: false
  prometheus:
    serviceMonitor:
      interval: "1s"
    prometheusSpec:
      scrapeInterval: "1s"
      evaluationInterval: "1s"
  grafana:
    adminUser: admin
    adminPassword: "admin"

# Frontend
frontend:
  enabled: true
  image:
    repository: nvcr.io/nvidia/blueprint/rag-playground
    pullPolicy: IfNotPresent
    tag: "2.0.0"
  imagePullSecret:
    name: "ngc-secret"
    registry: "nvcr.io"
    username: "$oauthtoken"
    password: ""
  service:
    type: NodePort
    port: 3000
  envVars:
    - name: NEXT_PUBLIC_MODEL_NAME
      value: "meta/llama-3.1-70b-instruct"
    - name: NEXT_PUBLIC_EMBEDDING_MODEL
      value: "nvidia/llama-3.2-nv-embedqa-1b-v2"
    - name: NEXT_PUBLIC_RERANKER_MODEL
      value: "nvidia/llama-3.2-nv-rerankqa-1b-v2"
    - name: NEXT_PUBLIC_CHAT_BASE_URL
      value: "http://rag-server:8081/v1"
    - name: NEXT_PUBLIC_VDB_BASE_URL
      value: "http://ingestor-server:8082/v1"
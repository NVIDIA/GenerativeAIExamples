services:

  # Main orchestrator server which stiches together all calls to different services to fulfill the user request
  rag-server:
    container_name: rag-server
    image: nvcr.io/nvidia/blueprint/rag-server:${TAG:-2.0.0}
    build:
      # Set context to repo's root directory
      context: ../../
      dockerfile: src/Dockerfile
    # start the server on port 8081 with 8 workers for improved latency on concurrent requests.
    command: --port 8081 --host 0.0.0.0 --workers 8

    # Common customizations to the pipeline can be controlled using env variables
    environment:
      # Path to example directory relative to root
      EXAMPLE_PATH: 'src/'

      ##===MINIO specific configurations which is used to store the multimodal base64 content===
      MINIO_ENDPOINT: "minio:9010"
      MINIO_ACCESSKEY: "minioadmin"
      MINIO_SECRETKEY: "minioadmin"

      ##===Vector DB specific configurations===
      # URL on which vectorstore is hosted
      APP_VECTORSTORE_URL: "http://milvus:19530"
      # Type of vectordb used to store embedding supported type milvus
      APP_VECTORSTORE_NAME: "milvus"
      # Type of vectordb search to be used
      APP_VECTORSTORE_SEARCHTYPE: ${APP_VECTORSTORE_SEARCHTYPE:-"dense"} # Can be dense or hybrid
      # vectorstore collection name to store embeddings
      COLLECTION_NAME: ${COLLECTION_NAME:-multimodal_data}
      APP_RETRIEVER_SCORETHRESHOLD: 0.25
      # Top K from vector DB, which goes as input to reranker model - not applicable if ENABLE_RERANKER is set to False
      VECTOR_DB_TOPK: 100

      ##===LLM Model specific configurations===
      APP_LLM_MODELNAME: ${APP_LLM_MODELNAME:-"meta/llama-3.1-70b-instruct"}
      # url on which llm model is hosted. If "", Nvidia hosted API is used
      APP_LLM_SERVERURL: ${APP_LLM_SERVERURL-"nim-llm:8000"}

      ##===Query Rewriter Model specific configurations===
      APP_QUERYREWRITER_MODELNAME: ${APP_QUERYREWRITER_MODELNAME:-"meta/llama-3.1-8b-instruct"}
      # url on which query rewriter model is hosted. If "", Nvidia hosted API is used
      APP_QUERYREWRITER_SERVERURL: ${APP_QUERYREWRITER_SERVERURL-"nim-llm-llama-8b-ms:8000"}

      ##===Embedding Model specific configurations===
      # url on which embedding model is hosted. If "", Nvidia hosted API is used
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL-"nemoretriever-embedding-ms:8000"}
      APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}

      ##===Reranking Model specific configurations===
      # url on which ranking model is hosted. If "", Nvidia hosted API is used
      APP_RANKING_SERVERURL: ${APP_RANKING_SERVERURL-"nemoretriever-ranking-ms:8000"}
      APP_RANKING_MODELNAME: ${APP_RANKING_MODELNAME:-"nvidia/llama-3.2-nv-rerankqa-1b-v2"}
      ENABLE_RERANKER: ${ENABLE_RERANKER:-True}

      NVIDIA_API_KEY: ${NVIDIA_API_KEY:?"NVIDIA_API_KEY is required"}

      # Number of document chunks to insert in LLM prompt
      APP_RETRIEVER_TOPK: 10

      # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
      LOGLEVEL: ${LOGLEVEL:-INFO}

      # enable multi-turn conversation in the rag chain - this controls conversation history usage
      # while doing query rewriting and in LLM prompt
      ENABLE_MULTITURN: ${ENABLE_MULTITURN:-True}

      # enable query rewriting for multiturn conversation in the rag chain.
      # This will improve accuracy of the retrieiver pipeline but increase latency due to an additional LLM call
      ENABLE_QUERYREWRITER: ${ENABLE_QUERYREWRITER:-False}

      # Choose whether to enable citations in the response
      ENABLE_CITATIONS: ${ENABLE_CITATIONS:-True}

      # Choose whether to enable/disable guardrails
      ENABLE_GUARDRAILS: ${ENABLE_GUARDRAILS:-False}

      # NeMo Guardrails URL when ENABLE_GUARDRAILS is true
      NEMO_GUARDRAILS_URL: ${NEMO_GUARDRAILS_URL:-nemo-guardrails-microservice:7331}

      # number of last n chat messages to consider from the provided conversation history
      CONVERSATION_HISTORY: 5

      # Tracing
      APP_TRACING_ENABLED: "False"
      # HTTP endpoint
      APP_TRACING_OTLPHTTPENDPOINT: http://otel-collector:4318/v1/traces
      # GRPC endpoint
      APP_TRACING_OTLPGRPCENDPOINT: grpc://otel-collector:4317

      # Choose whether to enable source metadata in document content during generation
      ENABLE_SOURCE_METADATA: ${ENABLE_SOURCE_METADATA:-true}

      # Whether to filter content within <think></think> tags in model responses
      FILTER_THINK_TOKENS: ${FILTER_THINK_TOKENS:-true}

      # enable reflection (context relevance and response groundedness checking) in the rag chain
      ENABLE_REFLECTION: ${ENABLE_REFLECTION:-false}
      # Maximum number of context relevance loop iterations
      MAX_REFLECTION_LOOP: ${MAX_REFLECTION_LOOP:-3}
      # Minimum relevance score threshold (0-2)
      CONTEXT_RELEVANCE_THRESHOLD: ${CONTEXT_RELEVANCE_THRESHOLD:-1}
      # Minimum groundedness score threshold (0-2)
      RESPONSE_GROUNDEDNESS_THRESHOLD: ${RESPONSE_GROUNDEDNESS_THRESHOLD:-1}
      # reflection llm
      REFLECTION_LLM: ${REFLECTION_LLM:-"mistralai/mixtral-8x22b-instruct-v0.1"}
      # reflection llm server url. If "", Nvidia hosted API is used
      REFLECTION_LLM_SERVERURL: ${REFLECTION_LLM_SERVERURL-"nim-llm-mixtral-8x22b:8000"}

    ports:
      - "8081:8081"
    expose:
      - "8081"
    shm_size: 5gb

  # Sample UI container which interacts with APIs exposed by rag-server container
  rag-playground:
    container_name: rag-playground
    image: nvcr.io/nvidia/blueprint/rag-playground:${TAG:-2.0.0}
    build:
      # Set context to repo's root directory
      context: ../../frontend
      dockerfile: ./Dockerfile
      args:
        # Model name for LLM
        NEXT_PUBLIC_MODEL_NAME: ${APP_LLM_MODELNAME:-meta/llama-3.1-70b-instruct}
        # Model name for embeddings
        NEXT_PUBLIC_EMBEDDING_MODEL: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}
        # Model name for reranking
        NEXT_PUBLIC_RERANKER_MODEL: ${APP_RANKING_MODELNAME:-nvidia/llama-3.2-nv-rerankqa-1b-v2}
        # URL for rag server container
        NEXT_PUBLIC_CHAT_BASE_URL: "http://rag-server:8081/v1"
        # URL for ingestor container
        NEXT_PUBLIC_VDB_BASE_URL: "http://ingestor-server:8082/v1"
    ports:
      - "8090:3000"
    expose:
      - "3000"
    depends_on:
      - rag-server

networks:
  default:
    name: nvidia-rag

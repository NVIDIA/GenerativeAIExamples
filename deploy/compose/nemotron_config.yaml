milvus:
  # The configuration of the Milvus connection.

  url: "http://milvus:19530"
  # The location of the Milvus Server.
  # Type: str
  # ENV Variable: APP_MILVUS_URL

llm:
  # The configuration for the server hosting the Large Language models.

  server_url: "llm:9999"
  # The location of the server hosting the large language model.
  # Type: str
  # ENV Variable: APP_LLM_SERVERURL

  model_name: "ensemble"
  # The name of the hosted model.
  # Type: str
  # ENV Variable: APP_LLM_MODELNAME

  model_engine: "triton-trt-llm"
  # The backend name hosting the model. Right now only triton-trt-llm and nemo-infer is supported.
  # Type: str
  # ENV Variable: APP_LLM_MODELENGINE

text_splitter:
  # The configuration for the Text Splitter.

  chunk_size: 510
  # Chunk size for text splitting.
  # When using a token-based text splitter, this is the number of 'tokens per chunk'
  # Type: int

  chunk_overlap: 200
  # Overlapping text length for splitting.
  # Type: int

embeddings:
  # The configuration embedding models.

  model_name: intfloat/e5-large-v2
  # The name embedding search model from huggingface.
  # Type: str

  dimensions: 1024
  # The dimensions of the embedding search model from huggingface.
  # Type: int

  model_engine: huggingface
  # The backend name hosting the model. Right now only huggingface is supported.
  # Type: str

prompts:
  # The configuration for the prompts used for response generation.

  chat_template:
    "<extra_id_0>System You are a helpful, respectful and honest assistant.Always answer as helpfully as possible, while being safe.Please ensure that your responses are positive in nature.\n<extra_id_0>System {context_str} \n {query_str} Given context followed by query, you try to answer the query truthfully \n<extra_id_1>Assistant"
  # The chat prompt template guides the model to generate responses for queries.
  # Type: str

  rag_template:
    "<extra_id_0>System Use the following context to answer the user's question. If you don't know the answer,just say that you don't know, don't try to make up an answer.\n<extra_id_0>System Context {context_str} Question {query_str} \n<extra_id_0>Only return the helpful answer below and nothing else. Helpful answer\n<extra_id_1>Assistant"
  # The RAG prompt template instructs the model to generate responses for queries while utilizing knowledge base.
  # Type: str

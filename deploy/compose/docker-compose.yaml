include:
  - path:
      - ./vectordb.yaml # Different vector db services are defined here. Milvus is used as default.
      - ./nims.yaml # All NIM services are defined here.

services:

  # Main orchestrator server which stiches together all calls to different services to fulfill the user request
  rag-server:
    container_name: rag-server
    image: nvcr.io/nvidia/blueprint/rag-server:${TAG:-1.0.0}
    build:
      # Set context to repo's root directory
      context: ../../
      dockerfile: src/Dockerfile
    # start the server on port 8081 with 8 workers for improved latency on concurrent requests.
    command: --port 8081 --host 0.0.0.0 --workers 8

    # Common customizations to the pipeline can be controlled using env variables
    environment:
      # Path to example directory relative to root
      EXAMPLE_PATH: 'src/'

      ##===Vector DB specific configurations===
      # URL on which vectorstore is hosted
      APP_VECTORSTORE_URL: "http://milvus:19530"
      # Type of vectordb used to store embedding supported type milvus
      APP_VECTORSTORE_NAME: "milvus"
      # Index type for milvus for vectorstore
      APP_VECTORSTORE_INDEXTYPE: GPU_CAGRA
      # vectorstore collection name to store embeddings
      COLLECTION_NAME: ${COLLECTION_NAME:-nvidia_blogs}
      APP_RETRIEVER_SCORETHRESHOLD: 0.25

      ##===LLM Model specific configurations===
      APP_LLM_MODELNAME: ${APP_LLM_MODELNAME:-"meta/llama-3.1-70b-instruct"}
      # url on which llm model is hosted. If "", Nvidia hosted API is used
      APP_LLM_SERVERURL: ${APP_LLM_SERVERURL:-""}

      ##===Embedding Model specific configurations===
      # url on which embedding model is hosted. If "", Nvidia hosted API is used
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL:-""}
      APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}

      ##===Reranking Model specific configurations===
      # url on which ranking model is hosted. If "", Nvidia hosted API is used
      APP_RANKING_SERVERURL: ${APP_RANKING_SERVERURL:-""}
      APP_RANKING_MODELNAME: ${APP_RANKING_MODELNAME:-"nvidia/llama-3.2-nv-rerankqa-1b-v2"} # Leave it blank to avoid using ranking
      # Top K from vector DB, which goes as input to reranker model - not applicable if APP_RANKING_MODELNAME is set to ""
      VECTOR_DB_TOPK: 20

      ##===Text splitter Model specific configurations===
      APP_TEXTSPLITTER_CHUNKSIZE: 2000
      APP_TEXTSPLITTER_CHUNKOVERLAP: 200

      NVIDIA_API_KEY: ${NVIDIA_API_KEY:?"NVIDIA_API_KEY is required"}

      # Number of document chunks to insert in LLM prompt
      APP_RETRIEVER_TOPK: 4

      # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
      LOGLEVEL: ${LOGLEVEL:-INFO}

      # enable multi-turn conversation in the rag chain - this controls conversation history usage
      # while doing query rewriting and in LLM prompt
      ENABLE_MULTITURN: ${ENABLE_MULTITURN:-true}

      # enable query rewriting for multiturn conversation in the rag chain.
      # This will improve accuracy of the retrieiver pipeline but increase latency due to an additional LLM call
      ENABLE_QUERYREWRITER: ${ENABLE_QUERYREWRITER:-true}

      # number of last n chat messages to consider from the provided conversation history
      CONVERSATION_HISTORY: 5

    ports:
      - "8081:8081"
    expose:
      - "8081"
    shm_size: 5gb
    depends_on:
      embedding-ms:
        condition: service_healthy
        required: false
      nemollm-inference:
        condition: service_healthy
        required: false

  # Sample UI container which interacts with APIs exposed by rag-server container
  rag-playground:
    container_name: rag-playground
    image: nvcr.io/nvidia/blueprint/rag-playground:${TAG:-1.0.0}
    build:
      # Set context to repo's root directory
      context: ../../
      dockerfile: ./frontend/Dockerfile
    command: --port 8090
    environment:
      # URL or rag server container
      APP_SERVERURL: http://rag-server
      APP_SERVERPORT: 8081
      # model name displayed on UI
      APP_MODELNAME: ${APP_LLM_MODELNAME:-"meta/llama-3.1-70b-instruct"}
    ports:
      - "8090:8090"
    expose:
      - "8090"
    depends_on:
      - rag-server

networks:
  default:
    name: nvidia-rag

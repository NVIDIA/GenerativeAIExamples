<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Evaluation Tool &mdash; NVIDIA Generative AI Examples 24.6.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/version.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-catalog.html">API Catalog Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector-database.html">Alternative Vector Database</a></li>
<li class="toctree-l1"><a class="reference internal" href="nim-llms.html">NIM for LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="simple-examples.html">Developing Simple Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tools/evaluation/index.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/01_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA API Catalog with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA API Catalog, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/04_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Catelog and AI Catalog with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/05_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/06_LangGraph_HandlingAgent_IntermediateSteps.html">LangGraph Handling LangChain Agent Intermediate_Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Chat_with_nvidia_financial_reports.html">Notebook: Chatting with NVIDIA Financial Reports</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_RAG_Langchain_with_Local_NIM.html">Build a RAG using a locally hosted NIM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Evaluation Tool</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
  SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  SPDX-License-Identifier: Apache-2.0

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<section class="tex2jax_ignore mathjax_ignore" id="evaluation-tool">
<h1>Evaluation Tool<a class="headerlink" href="#evaluation-tool" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id1">Introduction</a></p>
<ul>
<li><p><a class="reference internal" href="#synthetic-data-generation" id="id2">Synthetic Data Generation</a></p></li>
<li><p><a class="reference internal" href="#automated-metrics" id="id3">Automated Metrics</a></p></li>
<li><p><a class="reference internal" href="#llm-as-a-judge" id="id4">LLM-as-a-Judge</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#prerequisites" id="id5">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#get-an-api-key-for-the-llama-2-70b-api-endpoint" id="id6">Get an API Key for the Llama 2 70B API Endpoint</a></p></li>
<li><p><a class="reference internal" href="#build-and-start-the-containers" id="id7">Build and Start the Containers</a></p></li>
<li><p><a class="reference internal" href="#next-steps" id="id8">Next Steps</a></p></li>
</ul>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>Evaluation is crucial for retrieval augmented generation (RAG) pipelines because it ensures the accuracy and relevance of the information that is retrieved as well as the generated content.</p>
<p>There are three components needed for evaluating the performance of a RAG pipeline:</p>
<ul class="simple">
<li><p>Data for testing.</p></li>
<li><p>Automated metrics to measure performance of both the context retrieval and response generation.</p></li>
<li><p>Human-like evaluation of the generated response from the end-to-end pipeline.</p></li>
</ul>
<p>This tool provides a set of notebooks that demonstrate how to address these requirements in an automated fashion for the default developer RAG example.</p>
<p>The following figure shows the sample topology:</p>
<ul class="simple">
<li><p>The Jupyter notebooks for evaluation are served by a notebook server.</p></li>
<li><p>The notebook server communicates with the chain server to ingest documents and build a knowledge base.</p></li>
<li><p>The notebook server communicates NVIDIA AI Foundation Models and Endpoints for inference.</p></li>
</ul>
<p><img alt="Evaluation example toplogy" src="_images/evaluation-topology.png" /></p>
<section id="synthetic-data-generation">
<h3>Synthetic Data Generation<a class="headerlink" href="#synthetic-data-generation" title="Permalink to this headline"></a></h3>
<p>Using an existing knowledge base, we can generate synthetic question|answer|context triplets using an LLM.
This tool uses the Llama 2 70B model from the NVIDIA AI Foundation Models and Endpoints for data generation.</p>
</section>
<section id="automated-metrics">
<h3>Automated Metrics<a class="headerlink" href="#automated-metrics" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://github.com/explodinggradients/ragas">RAGAS</a> is an automated metrics tool for measuring performance of both the retriever and generator.
This tool uses a LangChain wrapper to connect to NVIDIA AI Foundation Models and Endpoints to run RAGAS evaluation on our example RAG pipeline.</p>
</section>
<section id="llm-as-a-judge">
<h3>LLM-as-a-Judge<a class="headerlink" href="#llm-as-a-judge" title="Permalink to this headline"></a></h3>
<p>This tool uses LLMs to provide human-like feedback and Likert evaluation scores for full end-to-end RAG pipelines.
The Llama 2 70B model is used as a judge LLM.</p>
</section>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h2>
<ul>
<li><p>Clone the Generative AI examples Git repository using Git LFS:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>apt<span class="w"> </span>-y<span class="w"> </span>install<span class="w"> </span>git-lfs
<span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>git@github.com:NVIDIA/GenerativeAIExamples.git
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>GenerativeAIExamples/
<span class="gp">$ </span>git<span class="w"> </span>lfs<span class="w"> </span>pull
</pre></div>
</div>
</li>
<li><p>A host with an NVIDIA A100, H100, or L40S GPU.</p></li>
<li><p>Verify NVIDIA GPU driver version 535 or later is installed and that the GPU is in compute mode:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvidia-smi<span class="w"> </span>-q<span class="w"> </span>-d<span class="w"> </span>compute
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">==============NVSMI LOG==============</span>

<span class="go">Timestamp                                 : Sun Nov 26 21:17:25 2023</span>
<span class="hll"><span class="go">Driver Version                            : 535.129.03</span>
</span><span class="go">CUDA Version                              : 12.2</span>

<span class="go">Attached GPUs                             : 1</span>
<span class="go">GPU 00000000:CA:00.0</span>
<span class="hll"><span class="go">    Compute Mode                          : Default</span>
</span></pre></div>
</div>
<p>If the driver is not installed or below version 535, refer to the <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html"><em>NVIDIA Driver Installation Quickstart Guide</em></a>.</p>
</li>
<li><p>Install Docker Engine and Docker Compose.
Refer to the instructions for <a class="reference external" href="https://docs.docker.com/engine/install/ubuntu/">Ubuntu</a>.</p></li>
<li><p>Install the NVIDIA Container Toolkit.</p>
<ol class="arabic">
<li><p>Refer to the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">installation documentation</a>.</p></li>
<li><p>When you configure the runtime, set the NVIDIA runtime as the default:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>nvidia-ctk<span class="w"> </span>runtime<span class="w"> </span>configure<span class="w"> </span>--runtime<span class="o">=</span>docker<span class="w"> </span>--set-as-default
</pre></div>
</div>
<p>If you did not set the runtime as the default, you can reconfigure the runtime by running the preceding command.</p>
</li>
<li><p>Verify the NVIDIA container toolkit is installed and configured as the default container runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>/etc/docker/daemon.json
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;default-runtime&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvidia&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;runtimes&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;nvidia&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">            </span><span class="nt">&quot;path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvidia-container-runtime&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> command in a container to verify the configuration:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>--runtime<span class="o">=</span>nvidia<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>ubuntu<span class="w"> </span>nvidia-smi<span class="w"> </span>-L
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-d8ce95c1-12f7-3174-6395-e573163a2ace)</span>
</pre></div>
</div>
</li>
</ol>
</li>
</ul>
</section>
<section id="get-an-api-key-for-the-llama-2-70b-api-endpoint">
<h2>Get an API Key for the Llama 2 70B API Endpoint<a class="headerlink" href="#get-an-api-key-for-the-llama-2-70b-api-endpoint" title="Permalink to this headline"></a></h2>
<p>Perform the following steps if you do not already have an API key.
You can use different model API endpoints with the same API key.</p>
<ol class="arabic">
<li><p>Navigate to <a class="reference external" href="https://catalog.ngc.nvidia.com/ai-foundation-models">https://catalog.ngc.nvidia.com/ai-foundation-models</a>.</p></li>
<li><p>Find the <strong>Llama 2 70B</strong> card and click <strong>Learn More</strong>.</p>
<p><img alt="Llama 2 70B model card" src="_images/llama-2-70b-card.png" /></p>
</li>
<li><p>Click the <strong>API</strong> button and then click <strong>Generate Key</strong>.</p>
<p><img alt="API section of the playground tab." src="_images/llama-2-70b-api.png" /></p>
</li>
<li><p>Save the generated API key.</p></li>
</ol>
</section>
<section id="build-and-start-the-containers">
<h2>Build and Start the Containers<a class="headerlink" href="#build-and-start-the-containers" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>In the Generative AI Examples repository, edit the <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code> file.</p>
<p>Specify the absolute path to the model location, model architecture, and model name.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># full path to the local copy of the model weights
# NOTE: This should be an absolute path and not relative path
export MODEL_DIRECTORY=&quot;/path/to/llama/llama-2-13b_chat/&quot;

# the architecture of the model. eg: llama
export MODEL_ARCHITECTURE=&quot;llama&quot;

# the name of the model being used - only for displaying on frontend
export MODEL_NAME=&quot;Llama-2-13b-chat&quot;
...
</pre></div>
</div>
</li>
<li><p>Export the <code class="docutils literal notranslate"><span class="pre">NVIDIA_API_KEY</span></code> variable in terminal.</p>
<p>Add the API for the model endpoint:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export NVIDIA_API_KEY=&quot;nvapi-&lt;...&gt;&quot;
</pre></div>
</div>
</li>
<li><p>From the root of the repository, build the containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>build
</pre></div>
</div>
</li>
<li><p>Start the Milvus container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-vectordb.yaml<span class="w"> </span>up<span class="w"> </span>-d<span class="w"> </span>milvus
</pre></div>
</div>
</li>
<li><p>Start the containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>up<span class="w"> </span>-d
</pre></div>
</div>
<p>The inference server can require 5 minutes to start. The <code class="docutils literal notranslate"><span class="pre">-d</span></code> flag starts the services in the background.</p>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">✔ Network nvidia-rag              Created</span>
<span class="go">✔ Container llm-inference-server  Started</span>
<span class="go">✔ Container notebook-server       Started</span>
<span class="go">✔ Container chain-server          Started</span>
<span class="go">✔ Container rag-playground        Started</span>
</pre></div>
</div>
</li>
<li><p>Build and deploy the evaluation service:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-evaluation.yaml<span class="w"> </span>build
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-evaluation.yaml<span class="w"> </span>up<span class="w"> </span>-d
</pre></div>
</div>
</li>
</ol>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Access the evaluation notebook server at <code class="docutils literal notranslate"><span class="pre">http://host-ip:8889</span></code> from your web browser and run the notebooks sequentially starting from <code class="docutils literal notranslate"><span class="pre">01_synthetic_data_generation.ipynb</span></code>.</p></li>
<li><p>Stop the containers by running the following commands:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">-f</span> <span class="pre">deploy/compose/rag-app-text-chatbot.yaml</span> <span class="pre">down</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">-f</span> <span class="pre">deploy/compose/docker-compose-vectordb.yaml</span> <span class="pre">down</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">-f</span> <span class="pre">deploy/compose/docker-compose-evaluation.yaml</span> <span class="pre">down</span></code></p></li>
</ul>
</li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>
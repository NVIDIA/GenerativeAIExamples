<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LangChain with Local Llama 2 Model &mdash; NVIDIA Generative AI Examples 24.4.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/version.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="NVIDIA API Catalog, LlamaIndex, and LangChain" href="08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html" />
    <link rel="prev" title="NVIDIA API Catalog with LangChain" href="07_Option%281%29_NVIDIA_AI_endpoint_simple.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="../index.html">
  <img src="../_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-catalog.html">API Catalog Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../local-gpu.html">Local GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-gpu.html">Multi-GPU for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantized-llm-model.html">Quantized Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vector-database.html">Alternative Vector Database</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nim-llms.html">NIM for LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../simple-examples.html">Developing Simple Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tools/evaluation/index.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="00-llm-non-streaming-nemotron.html">Basics: Prompt, Client, and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-llm-streaming-client.html">LLM Streaming Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_langchain_simple.html">Q&amp;A with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_llama_index_simple.html">Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_llamaindex_hier_node_parser.html">Advanced Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA API Catalog with LangChain</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA API Catalog, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Catelog and AI Catalog with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_LangGraph_HandlingAgent_IntermediateSteps.html">LangGraph Handling LangChain Agent Intermediate_Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_Chat_wtih_nvidia_financial_reports.html">Notebook: Chating with NVIDIA Financial Reports</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm-inference-server.html">NeMo Framework Inference Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>LangChain with Local Llama 2 Model</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="langchain-with-local-llama-2-model">
<h1>LangChain with Local Llama 2 Model<a class="headerlink" href="#langchain-with-local-llama-2-model" title="Permalink to this headline"></a></h1>
<p>This notebook uses the checkpoint from the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">HuggingFace Llama-2-13b-chat-hf</a> model.</p>
<div class="alert alert-block alert-info">
<p>⚠️ The notebook before this one, <code class="docutils literal notranslate"><span class="pre">07_Option(1)_NVIDIA_AI_endpoint_simple.ipynb</span></code>, contains the same exercise as this notebook but uses NVIDIA AI Catalog’ models via API calls instead of loading the models’ checkpoints pulled from huggingface model hub, and then load from host to devices (i.e GPUs).</p>
<p>Noted that, since we will load the checkpoints, it will be significantly slower to go through this entire notebook.</p>
<p>If you do decide to go through this notebook, please kindly check the <strong>Prerequisite</strong> section below.</p>
</div>
<section id="prerequisite">
<h2>Prerequisite<a class="headerlink" href="#prerequisite" title="Permalink to this headline"></a></h2>
<p>To run this notebook, you need the following:</p>
<ol class="arabic">
<li><p>Prior approval to use the checkpoints by applying for access to the <a class="reference external" href="https://huggingface.co/meta-llama">meta-llama</a> model.</p></li>
<li><p>At least 2 NVIDIA GPUs, each with at least 32G mem, preferably using Ampere architecture.</p></li>
<li><p>Installed Docker and <a class="reference external" href="https://github.com/NVIDIA/nvidia-container-toolkit">nvidia-container-toolkit</a>.</p></li>
<li><p>Registered with <a class="reference external" href="https://www.nvidia.com/en-us/gpu-cloud/">NVIDIA NGC</a> and can pull and run NGC PyTorch containers.</p></li>
<li><p>Installed Python dependencies for this notebook, by overwriting langchain-core version:</p>
<p>If you are using the <a class="reference external" href="https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/notebooks/Dockerfile.gpu_notebook">Dockerfile.gpu_notebook</a>, the dependency is already installed for you.</p>
</li>
</ol>
<p>The notebook will walk you through how to build an end-to-end RAG pipeline using <a class="reference external" href="https://python.langchain.com/docs/get_started/introduction">LangChain</a>, <a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/faiss">faiss</a> as the vectorstore and a custom llm of your choice from huggingface ( more specifically, we will be using <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">HuggingFace Llama-2-13b-chat-hf</a> in this notebook, but the process is similar for other llms from huggingface.</p>
<p>Generically speaking, the RAG pipeline will involve 2 phases -</p>
<p>The first one is the preprocessing phase illustrated below -</p>
<p><img alt="preprocessing" src="../_images/preprocessing.png" /></p>
<p>The second phase is the inference runtime -</p>
<p><img alt="inference_runtime" src="../_images/inference_runtime.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have only CPU, please use faiss-cpu instead</span>
<span class="c1">#!pip install faiss-gpu accelerate</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p>Let’s now go through this notebook step-by-step
For the first phase, reminder of the flow
<img alt="preprocessing" src="../_images/preprocessing.png" /></p>
<section id="step-1-load-huggingface-embedding">
<h3>Step 1 - Load huggingface embedding<a class="headerlink" href="#step-1-load-huggingface-embedding" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### load custom embedding and use it in Faiss</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">PyPDFLoader</span>
<span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">DirectoryLoader</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">embedding_model_name</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span> <span class="c1"># sentence-transformer is the most commonly used embedding</span>
<span class="n">emd_model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cuda&quot;</span><span class="p">}</span>
<span class="n">hf_embedding</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">embedding_model_name</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">emd_model_kwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-prepare-the-toy-text-dataset">
<h3>Step 2 - Prepare the toy text dataset<a class="headerlink" href="#step-2-prepare-the-toy-text-dataset" title="Permalink to this headline"></a></h3>
<p>We will prepare the XXX.txt files ( there should be Sweden.txt and and using the above embedding to parse chuck of text and store them into faiss-gpu vectorstore</p>
<p>Let’s have a look at text datasets</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
head -1 ./toy_data/Sweden.txt
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
head -3 ./toy_data/Titanic_film.txt
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-3-process-the-document-into-faiss-vectorstore-and-save-to-disk">
<h3>Step 3 -  Process the document into faiss vectorstore and save to disk<a class="headerlink" href="#step-3-process-the-document-into-faiss-vectorstore-and-save-to-disk" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>
<span class="kn">import</span> <span class="nn">faiss</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span><span class="p">,</span><span class="n">utils</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="c1"># Here we read in the text data and prepare them into vectorstore</span>
<span class="n">ps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;./toy_data/&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;**/*.txt&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">sources</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ps</span><span class="p">:</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
    <span class="n">sources</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="c1"># We do this due to the context limits of the LLMs.</span>
<span class="c1"># Here we split the documents, as needed, into smaller chunks.</span>
<span class="c1"># We do this due to the context limits of the LLMs.</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">metadatas</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">docs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">splits</span><span class="p">)</span>
    <span class="n">metadatas</span><span class="o">.</span><span class="n">extend</span><span class="p">([{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="n">sources</span><span class="p">[</span><span class="n">i</span><span class="p">]}]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">splits</span><span class="p">))</span>

<span class="c1"># Here we create a vector store from the documents and save it to disk.</span>
<span class="n">store</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_texts</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">hf_embedding</span><span class="p">,</span> <span class="n">metadatas</span><span class="o">=</span><span class="n">metadatas</span><span class="p">)</span>

<span class="n">store</span><span class="o">.</span><span class="n">save_local</span><span class="p">(</span><span class="s1">&#39;./toy_data/nv_embedding&#39;</span><span class="p">)</span>
<span class="c1"># you will only need to do this once, later on we will restore the already saved vectorstore</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-4-reload-the-already-saved-vectorstore-and-prepare-for-retrival">
<h3>Step 4 - Reload the already saved vectorstore and prepare for retrival<a class="headerlink" href="#step-4-reload-the-already-saved-vectorstore-and-prepare-for-retrival" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the LangChain.</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>
<span class="kn">import</span> <span class="nn">faiss</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">FAISS</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">store</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">load_local</span><span class="p">(</span><span class="s2">&quot;./toy_data/nv_embedding&quot;</span><span class="p">,</span><span class="n">hf_embedding</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-5-prepare-the-loaded-vectorstore-into-a-retriver">
<h3>Step 5 - Prepare the loaded vectorstore into a retriver<a class="headerlink" href="#step-5-prepare-the-loaded-vectorstore-into-a-retriver" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">retriever</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="s1">&#39;similarity&#39;</span><span class="p">,</span> <span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">})</span> <span class="c1"># k is a hyperparameter, usally by default set to 3</span>
</pre></div>
</div>
</div>
</div>
<p>Now we are finally done with the preprocessing step, next we will proceed to phase 2</p>
<hr class="docutils" />
<p>Recall phase 2 involve a runtime which we could query the already loaded faiss vectorstore.</p>
<p><img alt="inference" src="../_images/inference_runtime.png" /></p>
</section>
<section id="step-6-load-the-huggingface-llama-2-13b-chat-hf-to-your-gpus">
<h3>Step 6 - Load the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">HuggingFace Llama-2-13b-chat-hf</a> to your GPUs<a class="headerlink" href="#step-6-load-the-huggingface-llama-2-13b-chat-hf-to-your-gpus" title="Permalink to this headline"></a></h3>
<p>Note: Scroll down and make sure you supply the <strong>hf_token in code block below [FILL_IN] your huggingface token</strong>
, for how to generate the token from huggingface, please following instruction from <a class="reference external" href="https://huggingface.co/docs/transformers.js/guides/private">this link</a></p>
<p>Note: The execution of cell below will take up sometime, please be patient until the checkpoint is fully loaded. Alternatively, turn to previous notebook 07_Option(1)_NVIDIA_AI_endpoint_simply.ipynb if you wish to use already deployed models as API calls instead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">langchain</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoConfig</span><span class="p">,</span>
    <span class="n">AutoModel</span><span class="p">,</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">GenerationConfig</span><span class="p">,</span>
    <span class="n">LlamaForCausalLM</span><span class="p">,</span>
    <span class="n">LlamaTokenizer</span><span class="p">,</span>
    <span class="n">pipeline</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_gpus</span><span class="p">,</span> <span class="n">hf_auth_token</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load an HF locally saved checkpoint.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">elif</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;torch_dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">num_gpus</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device_map&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_gpus</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">num_gpus</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;device_map&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;max_memory&quot;</span><span class="p">:</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="s2">&quot;20GiB&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">)},</span>
                    <span class="p">}</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;torch_dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">}</span>
        <span class="c1"># Avoid bugs in mps backend by not using in-place operations.</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mps not supported&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">hf_auth_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="n">hf_auth_token</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span>
            <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">use_auth_token</span><span class="o">=</span><span class="n">hf_auth_token</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="n">num_gpus</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">debug</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span>

<span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-2-13b-chat-hf&quot;</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1">## minimal requirement is that you have 2x NVIDIA GPUs</span>

<span class="c1">## Remember to supply your own huggingface access token</span>
<span class="n">hf_token</span><span class="o">=</span> <span class="s2">&quot;[FILL_IN]&quot;</span>
<span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_gpus</span><span class="p">,</span><span class="n">hf_auth_token</span><span class="o">=</span><span class="n">hf_token</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">hf_llm</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">(</span><span class="n">pipeline</span><span class="o">=</span><span class="n">pipe</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-7-supply-the-hf-llm-as-well-as-the-retriver-we-prepared-above-into-langchain-s-retrievalqa-chain">
<h3>Step 7 - Supply the hf_llm as well as the retriver we prepared above into langchain’s RetrievalQA chain<a class="headerlink" href="#step-7-supply-the-hf-llm-as-well-as-the-retriver-we-prepared-above-into-langchain-s-retrievalqa-chain" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the using RetrievalQA</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>

<span class="n">qa_chain</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">hf_llm</span><span class="p">,</span> <span class="c1"># supply meta llama2 model</span>
                                  <span class="n">chain_type</span><span class="o">=</span><span class="s2">&quot;stuff&quot;</span><span class="p">,</span>
                                  <span class="n">retriever</span><span class="o">=</span><span class="n">retriever</span><span class="p">,</span> <span class="c1"># using our own retriever</span>
                                  <span class="n">return_source_documents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-8-we-are-now-ready-to-ask-questions">
<h3>Step 8 - We are now ready to ask questions<a class="headerlink" href="#step-8-we-are-now-ready-to-ask-questions" title="Permalink to this headline"></a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;When is the film Titanic being made ?&quot;</span>
<span class="c1">#query =&quot;Who is the director for the film?&quot;</span>
<span class="n">llm_response</span> <span class="o">=</span> <span class="n">qa_chain</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;llm response after retrieve from KB, the answer is :</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">llm_response</span><span class="p">[</span><span class="s1">&#39;result&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---&quot;</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;source paragraph &gt;&gt; &quot;</span><span class="p">)</span>
<span class="n">llm_response</span><span class="p">[</span><span class="s1">&#39;source_documents&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="07_Option%281%29_NVIDIA_AI_endpoint_simple.html" class="btn btn-neutral float-left" title="NVIDIA API Catalog with LangChain" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html" class="btn btn-neutral float-right" title="NVIDIA API Catalog, LlamaIndex, and LangChain" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>
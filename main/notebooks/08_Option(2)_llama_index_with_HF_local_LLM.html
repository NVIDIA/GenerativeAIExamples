<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>HF Checkpoints with LlamaIndex and LangChain &mdash; NVIDIA Generative AI Examples 24.4.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/version.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Multimodal Models from NVIDIA AI Catelog and AI Catalog with LangChain Agent" href="09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html" />
    <link rel="prev" title="NVIDIA API Catalog, LlamaIndex, and LangChain" href="08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="../index.html">
  <img src="../_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-catalog.html">API Catalog Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../local-gpu.html">Local GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-gpu.html">Multi-GPU for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantized-llm-model.html">Quantized Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vector-database.html">Alternative Vector Database</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nim-llms.html">NIM for LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../simple-examples.html">Developing Simple Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tools/evaluation/index.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="00-llm-non-streaming-nemotron.html">Basics: Prompt, Client, and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-llm-streaming-client.html">LLM Streaming Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_langchain_simple.html">Q&amp;A with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_llama_index_simple.html">Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_llamaindex_hier_node_parser.html">Advanced Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA API Catalog with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA API Catalog, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Catelog and AI Catalog with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_LangGraph_HandlingAgent_IntermediateSteps.html">LangGraph Handling LangChain Agent Intermediate_Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_Chat_wtih_nvidia_financial_reports.html">Notebook: Chating with NVIDIA Financial Reports</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm-inference-server.html">NeMo Framework Inference Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>HF Checkpoints with LlamaIndex and LangChain</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="hf-checkpoints-with-llamaindex-and-langchain">
<h1>HF Checkpoints with LlamaIndex and LangChain<a class="headerlink" href="#hf-checkpoints-with-llamaindex-and-langchain" title="Permalink to this headline"></a></h1>
<p>This notebook demonstrates how to plug in a local llm from <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">HuggingFace Hub Llama-2-13b-chat-hf</a> and <a class="reference external" href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">all-MiniLM-L6-v2 embedding from Huggingface</a>, bind these to into <a class="reference external" href="https://gpt-index.readthedocs.io/en/stable/">LlamaIndex</a> with these customizations.</p>
<p>The custom plug-ins shown in this notebook can be replaced, for example, you can swap out the <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">HuggingFace Llama-2-13b-chat-hf</a> with <a class="reference external" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1">HuggingFace checkpoint from Mistral</a>.</p>
<div class="alert alert-block alert-info">
<p>⚠️ The notebook before this one, <code class="docutils literal notranslate"><span class="pre">08_Option(1)_llama_index_with_NVIDIA_AI_endpoint.ipynb</span></code>, contains the same exercise as this notebook but uses NVIDIA AI Catelog’s models via API calls instead of loading the models’ checkpoints pulled from huggingface model hub, and then load from host to devices (i.e GPUs).</p>
<p>Noted that, since we will load the checkpoints, it will be significantly slower to go through this entire notebook.</p>
<p>If you do decide to go through this notebook, please kindly check the <strong>Prerequisite</strong> section below.</p>
<p>There are continous development and retrieval techniques supported in LlamaIndex and this notebook just shows how to quickly replace components such as llm and embedding per user’s choice, read more <a class="reference external" href="https://docs.llamaindex.ai/en/stable/">documentation on llama-index</a> for the latest nformation.</p>
</div>
<section id="prerequisite">
<h2>Prerequisite<a class="headerlink" href="#prerequisite" title="Permalink to this headline"></a></h2>
<p>In order to successfully run this notebook, you will need the following -</p>
<ol class="arabic simple">
<li><p>Already being approved of using the checkpoints via applying for <a class="reference external" href="https://huggingface.co/meta-llama">meta-llama</a></p></li>
<li><p>At least 2 NVIDIA GPUs, each with at least 32G mem, preferably using Ampere architecture</p></li>
<li><p>docker and <a class="reference external" href="https://github.com/NVIDIA/nvidia-container-toolkit">nvidia-docker</a> installed</p></li>
<li><p>Registered <a class="reference external" href="https://www.nvidia.com/en-us/gpu-cloud/">NVIDIA NGC</a> and can pull and run NGC pytorch containers</p></li>
<li><p>install necesary python dependencies :
Note: if you are using the <a class="reference external" href="https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/notebooks/Dockerfile.gpu_notebook">Dockerfile.gpu_notebook</a>, it should already prepare the environment for you. Otherwise please refer to the Dockerfile for environment building.</p></li>
</ol>
<p>In this notebook, we will cover the following custom plug-in components -</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>- LLM locally load from [HuggingFace Hub Llama-2-13b-chat-hf](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) and warp this into llama-index 

- A [HuggingFace embedding all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) 
</pre></div>
</div>
</section>
<section id="step-1-load-huggingface-hub-llama-2-13b-chat-hf">
<h2>Step 1 - Load <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">HuggingFace Hub Llama-2-13b-chat-hf</a><a class="headerlink" href="#step-1-load-huggingface-hub-llama-2-13b-chat-hf" title="Permalink to this headline"></a></h2>
<p>Note: Scroll down and make sure you supply the <strong>hf_token in code block below, replace [FILL_IN] with your huggingface token</strong>
, for how to generate the token from huggingface, please following instruction from <a class="reference external" href="https://huggingface.co/docs/transformers.js/guides/private">this link</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## uncomment the below if you have not yet install the python dependencies</span>
<span class="c1">#!pip install accelerate transformers==4.33.1 --upgrade</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">addHandler</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">StreamHandler</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">TextStreamer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">load_hf_model</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_gpus</span><span class="p">,</span><span class="n">hf_auth_token</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load an HF locally saved checkpoint.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">elif</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;torch_dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">num_gpus</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device_map&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_gpus</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">num_gpus</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;device_map&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;max_memory&quot;</span><span class="p">:</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="s2">&quot;13GiB&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">)},</span>
                    <span class="p">}</span>
                <span class="p">)</span>
    <span class="k">elif</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;torch_dtype&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">}</span>
        <span class="c1"># Avoid bugs in mps backend by not using in-place operations.</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mps not supported&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">hf_auth_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="n">hf_auth_token</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">use_auth_token</span><span class="o">=</span><span class="n">hf_auth_token</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="n">num_gpus</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">debug</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span>



<span class="c1"># Define variable to hold llama2 weights naming</span>
<span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-2-13b-chat-hf&quot;</span>
<span class="c1"># Set auth token variable from hugging face</span>
<span class="c1"># Create tokenizer</span>
<span class="n">hf_token</span><span class="o">=</span> <span class="s2">&quot;[FILL_IN]&quot;</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">load_hf_model</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_gpus</span><span class="p">,</span><span class="n">hf_auth_token</span><span class="o">=</span><span class="n">hf_token</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Setup a prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;### User:What is the fastest car in  </span><span class="se">\</span>
<span class="s2">          the world and how much does it cost? </span><span class="se">\</span>
<span class="s2">          ### Assistant:&quot;</span>
<span class="c1"># Pass the prompt to the tokenizer</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># Setup the text streamer</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">skip_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>run a test and see the model generating output response</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="c1"># Covert the output tokens back to text</span>
<span class="n">output_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">output_text</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-construct-prompt-template">
<h2>Step 2 - Construct prompt template<a class="headerlink" href="#step-2-construct-prompt-template" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the prompt wrapper...but for llama index</span>
<span class="kn">from</span> <span class="nn">llama_index.prompts.prompts</span> <span class="kn">import</span> <span class="n">SimpleInputPrompt</span>
<span class="c1"># Create a system prompt</span>
<span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;&lt;SYS&gt;&gt;</span>
<span class="s2">You are a helpful, respectful and honest assistant. Always answer as</span>
<span class="s2">helpfully as possible, while being safe. Your answers should not include</span>
<span class="s2">any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.</span>
<span class="s2">Please ensure that your responses are socially unbiased and positive in nature.</span>

<span class="s2">If a question does not make any sense, or is not factually coherent, explain</span>
<span class="s2">why instead of answering something not correct. If you don&#39;t know the answer</span>
<span class="s2">to a question, please don&#39;t share false information.</span>

<span class="s2">Your goal is to provide answers relating to the financial performance of</span>
<span class="s2">the company.&lt;&lt;/SYS&gt;&gt;[INST]</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="c1"># Throw together the query wrapper</span>
<span class="n">query_wrapper_prompt</span> <span class="o">=</span> <span class="n">SimpleInputPrompt</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{query_str}</span><span class="s2"> [/INST]&quot;</span><span class="p">)</span>
<span class="c1">## do a test query</span>
<span class="n">query_str</span><span class="o">=</span><span class="s1">&#39;What can you help me with?&#39;</span>
<span class="n">query_wrapper_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">query_str</span><span class="o">=</span><span class="n">query_str</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-3-load-the-chosen-huggingface-embedding">
<h2>Step 3 - Load the chosen huggingface Embedding<a class="headerlink" href="#step-3-load-the-chosen-huggingface-embedding" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create and dl embeddings instance wrapping huggingface embedding into langchain embedding</span>
<span class="c1"># Bring in embeddings wrapper</span>
<span class="kn">from</span> <span class="nn">llama_index.embeddings</span> <span class="kn">import</span> <span class="n">LangchainEmbedding</span>
<span class="c1"># Bring in HF embeddings - need these to represent document chunks</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings.huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
<span class="n">embeddings</span><span class="o">=</span><span class="n">LangchainEmbedding</span><span class="p">(</span>
    <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-4-prepare-the-locally-loaded-huggingface-llm-into-into-llamaindex">
<h2>Step 4 - Prepare the locally loaded huggingface llm into into llamaindex<a class="headerlink" href="#step-4-prepare-the-locally-loaded-huggingface-llm-into-into-llamaindex" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the llama index HF Wrapper</span>
<span class="kn">from</span> <span class="nn">llama_index.llms</span> <span class="kn">import</span> <span class="n">HuggingFaceLLM</span>
<span class="c1"># Create a HF LLM using the llama index wrapper</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">HuggingFaceLLM</span><span class="p">(</span><span class="n">context_window</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
                    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                    <span class="n">system_prompt</span><span class="o">=</span><span class="n">system_prompt</span><span class="p">,</span>
                    <span class="n">query_wrapper_prompt</span><span class="o">=</span><span class="n">query_wrapper_prompt</span><span class="p">,</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-5-wrap-the-custom-embedding-and-the-locally-loaded-huggingface-llm-into-llama-index-s-servicecontext">
<h2>Step 5 - Wrap the custom embedding and the locally loaded huggingface llm into llama-index’s ServiceContext<a class="headerlink" href="#step-5-wrap-the-custom-embedding-and-the-locally-loaded-huggingface-llm-into-llama-index-s-servicecontext" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bring in stuff to change service context</span>
<span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">set_global_service_context</span>
<span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">ServiceContext</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create new service context instance</span>
<span class="n">service_context</span> <span class="o">=</span> <span class="n">ServiceContext</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">embed_model</span><span class="o">=</span><span class="n">embeddings</span>
<span class="p">)</span>
<span class="c1"># And set the service context</span>
<span class="n">set_global_service_context</span><span class="p">(</span><span class="n">service_context</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-6a-load-the-text-data-using-llama-index-s-simpledirectoryreader-and-we-will-be-using-the-built-in-vectorstoreindex">
<h2>Step 6a - Load the text data using llama-index’s SimpleDirectoryReader and we will be using the built-in <a class="reference external" href="https://docs.llamaindex.ai/en/latest/community/integrations/vector_stores.html">VectorStoreIndex</a><a class="headerlink" href="#step-6a-load-the-text-data-using-llama-index-s-simpledirectoryreader-and-we-will-be-using-the-built-in-vectorstoreindex" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#create query engine with cross encoder reranker</span>
<span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">VectorStoreIndex</span><span class="p">,</span> <span class="n">SimpleDirectoryReader</span><span class="p">,</span> <span class="n">ServiceContext</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">documents</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span><span class="s2">&quot;./toy_data&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">VectorStoreIndex</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">service_context</span><span class="o">=</span><span class="n">service_context</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-6b-this-will-serve-as-the-query-engine-for-us-to-ask-questions">
<h2>Step 6b - This will serve as the query engine for us to ask questions<a class="headerlink" href="#step-6b-this-will-serve-as-the-query-engine-for-us-to-ask-questions" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup index query engine using LLM</span>
<span class="n">query_engine</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">as_query_engine</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test out a query in natural</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">query_engine</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;Tell me about Sweden&#39;s population?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">metadata</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">response</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html" class="btn btn-neutral float-left" title="NVIDIA API Catalog, LlamaIndex, and LangChain" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html" class="btn btn-neutral float-right" title="Multimodal Models from NVIDIA AI Catelog and AI Catalog with LangChain Agent" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>
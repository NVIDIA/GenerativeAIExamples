{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55881f49-4f2d-4831-ac9f-9765f8048fa2",
   "metadata": {},
   "source": [
    "# Synthetic Evaluation Data Generation Using NeMo Retriever SDG\n",
    "\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "### Install required libraries\n",
    "\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Please also see [README.md](../README.md) for environment setup including necessary library installation.\n",
    "\n",
    "\n",
    "### Prepare input data\n",
    "\n",
    "The synthetic data generation framework supports two input formats `rawdoc` or `squad`. \n",
    "\n",
    "- `input_format=rawdoc`\n",
    "\n",
    "The file should be stored in a JSONL format. Each line contains a document in the format of `{\"text\": <document>, \"title\": <title>}`.\n",
    "\n",
    "```\n",
    "{\"text\": \"The quick brown fox jumps over the lazy dog.\", \"title\": \"Classic Pangram\" }\n",
    "{\"text\": \"The Eiffel Tower is an iron lattice tower on the Champ de Mars in Paris.\", \"title\": \"Iconic Landmark\" }\n",
    "...\n",
    "```\n",
    "\n",
    "This repository contains a sample JSONL file `data/sample_data.jsonl`.\n",
    "\n",
    "\n",
    "- `input_format=squad`\n",
    "\n",
    "If you have manually created questions and would like to conduct further analysis (correlation between synthetic questions and original questions), the input data should follow the SQuAD format.\n",
    "\n",
    "```\n",
    "       {\n",
    "            \"data\": [\n",
    "                {\n",
    "                    \"paragraphs\": [\n",
    "                        {\n",
    "                            \"context\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "                            \"qas\": [\n",
    "                                {\n",
    "                                    \"question\": \"What does the fox jump over?\",\n",
    "                                    \"id\": \"q1\",\n",
    "                                    \"synthetic\": true,\n",
    "                                    \"answers\": [\n",
    "                                        {\n",
    "                                            \"text\": \"The fox jump over the lazy dog\",\n",
    "                                            \"answer_start\": -1,  # For generative answers\n",
    "                                            \"synthetic\": true,\n",
    "                                        }\n",
    "                                    ]\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ],\n",
    "                    \"title\": \"Example\"\n",
    "                }\n",
    "            ],\n",
    "            \"version\": \"2.0\"\n",
    "        }        \n",
    "```\n",
    "\n",
    "\n",
    "### Run pipeline\n",
    "\n",
    "- Visit [this page](https://build.nvidia.com/mistralai/mixtral-8x7b-instruct) and click \"Get API Key\" to generate an API key\n",
    "\n",
    "![NVIDIA API Catalog](../figures/api_key.png) |\n",
    "-\n",
    "\n",
    "- Run the following command. It will roughly take 5-10 minutes. \n",
    "    - Add `PYTHONPATH=.` if you get an error message `ModuleNotFoundError: No module named 'nemo_retriever_sdg'`\n",
    "\n",
    "```\n",
    "HYDRA_FULL_ERROR=1 PYTHONPATH=. python scripts/run_pipeline.py \\\n",
    "  api_key=\"<API KEY>\" \\\n",
    "  input_file=$(pwd)/data/sample_data_rawdoc.jsonl \\\n",
    "  input_format=\"rawdoc\"\n",
    "  output_dir=$(pwd)/outputs/sample_synthetic_data\n",
    "```\n",
    "\n",
    "\n",
    "## Output\n",
    "\n",
    "This creates synthetic eval datasets in the SQuAD and BEIR formats. \n",
    "If you use `input_format=squad` and `evaluate=True`, you would see `eval` and `beir/original` dictionaries.\n",
    "\n",
    "```\n",
    "outputs/sample_synthetic_data\n",
    "├── beir\n",
    "│   ├── all\n",
    "│   │   └── synthetic\n",
    "│   │       ├── corpus.jsonl\n",
    "│   │       ├── qrels\n",
    "│   │       │   └── test.tsv\n",
    "│   │       └── queries.jsonl\n",
    "│   └── filtered\n",
    "│       └── synthetic\n",
    "│           ├── corpus.jsonl\n",
    "│           ├── qrels\n",
    "│           │   └── test.tsv\n",
    "│           └── queries.jsonl\n",
    "├── eval\n",
    "│   ├── all\n",
    "│   │   ├── beir_evaluator__recall5.csv\n",
    "│   │   ├── beir_evaluator__synthetic_topk_rel_doc_flags.csv\n",
    "│   │   ├── beir_evaluator__synthetic_topk_rel_doc_flags_counts.csv\n",
    "│   │   └── beir_evaluator__type_model_eval_dict.json\n",
    "│   └── filtered\n",
    "│       ├── beir_evaluator__recall5.csv\n",
    "│       ├── beir_evaluator__synthetic_topk_rel_doc_flags.csv\n",
    "│       ├── beir_evaluator__synthetic_topk_rel_doc_flags_counts.csv\n",
    "│       └── beir_evaluator__type_model_eval_dict.json\n",
    "├── report__all.json\n",
    "├── report__filtered.json\n",
    "└── squad\n",
    "    ├── synthetic_data__all.json\n",
    "    └── synthetic_data__filtered.json\n",
    "\n",
    "```\n",
    "\n",
    "### SQuAD format\n",
    "\n",
    "The command will generate a `.json` file in a modified version of the SQuAD v2 format. The difference from the origial SQuAD v2 format is \n",
    "- Set `answer_start: -1` for generative answers.\n",
    "    - Note: only generative answers (not extractive) can be created by the current version of the SDG pipeline. Thus, the value is set to be a dummy value `-1` for synehtic answers\n",
    "- Use of `synthetic: true` for synthetic quetsions and answers\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"paragraphs\": [\n",
    "                {\n",
    "                    \"context\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "                    \"document_id\": \"Example\",\n",
    "                    \"qas\": [\n",
    "                        {\n",
    "                            \"question\": \"What does the fox jump over?\",\n",
    "                            \"id\": \"q1\",\n",
    "                            \"synthetic\": true,\n",
    "                            \"answers\": [\n",
    "                                {\n",
    "                                    \"text\": \"The fox jump over the lazy dog\",\n",
    "                                    \"answer_start\": -1,  # For generative answers\n",
    "                                    \"synthetic\": true,\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"2.0\"\n",
    "}\n",
    "```\n",
    "\n",
    "### BEIR format\n",
    "\n",
    "The directory structure follows the BEIR format. \n",
    "\n",
    "```\n",
    "synthetic\n",
    "├── corpus.jsonl\n",
    "├── qrels\n",
    "│   └── test.tsv\n",
    "└── queries.jsonl\n",
    "```\n",
    "\n",
    "You can use the directory as it to load by the BEIR framework for evauation. For example,\n",
    "\n",
    "```\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=\"synthetic\").load(split=\"test\")\n",
    "```\n",
    "\n",
    "\n",
    "### Report (`report.json`)\n",
    "\n",
    "```\n",
    "{\n",
    "  \"synthetic_question_length\": {\n",
    "    \"count\": 1500,\n",
    "    \"mean\": 83.68066666666667,\n",
    "    \"std\": 22.751082774243716,\n",
    "    \"min\": 31,\n",
    "    \"25%\": 67,\n",
    "    \"50%\": 82,\n",
    "    \"75%\": 97,\n",
    "    \"max\": 267\n",
    "  },\n",
    "  \"original_question_length\": {\n",
    "    \"count\": 137,\n",
    "    \"mean\": 53.613138686131386,\n",
    "    \"std\": 21.75709761649885,\n",
    "    \"min\": 12,\n",
    "    \"25%\": 37,\n",
    "    \"50%\": 50,\n",
    "    \"75%\": 70,\n",
    "    \"max\": 107\n",
    "  },\n",
    "  \"synthetic_lexical_divergence\": {\n",
    "    \"count\": 1500,\n",
    "    \"mean\": 0.057704510678110436,\n",
    "    \"std\": 0.05364795698080602,\n",
    "    \"min\": 0,\n",
    "    \"25%\": 0,\n",
    "    \"50%\": 0.04347826086956519,\n",
    "    \"75%\": 0.08890374331550804,\n",
    "    \"max\": 0.2777777777777778\n",
    "  },\n",
    "  \"original_lexical_divergence\": {\n",
    "    \"count\": 137,\n",
    "    \"mean\": 0.028657367331165418,\n",
    "    \"std\": 0.04839056633930506,\n",
    "    \"min\": 0,\n",
    "    \"25%\": 0,\n",
    "    \"50%\": 0,\n",
    "    \"75%\": 0.045454545454545414,\n",
    "    \"max\": 0.2272727272727273\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd118a-e036-4830-a3a9-013d9d06712f",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation (SDG) Pipeline\n",
    "\n",
    "![Overall architecture of the SDG Pipeline](../figures/sdg_pipeline.png)\n",
    "<p style=\"text-align: center;\">Figure 1. Overall architecture of the SDG Pipeline.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2ea4a-6b6a-4aff-8554-fa3afa2b4443",
   "metadata": {},
   "source": [
    "First step to running the SDG pipeline is to setup the configurations for: \n",
    "\n",
    "- 1. LLM generator model,\n",
    "- 2. Easiness filter (embedding-model-as-a-judge).\n",
    "- 3. Answerability filter (LLM-as-a-Judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fb357-c11c-46a3-9a80-c3a8a2eceeb3",
   "metadata": {},
   "source": [
    "*Answerability filer* uses LLM-as-judge in order to determine quality of questions in terms of them being answerable from content in the passage. The filter weeds out questions that are invalid and not relevant to the document chunk that was used to generate them.\n",
    "\n",
    "*Easiness filter* is used to filter out questions that are deemed easy for the retriever models to retrieve positive passages for the given generated question. It uses embedding model as judge. The user needs to provide threshold (number between 0 and 1) for this filter. Lower the value of the filter, harder the questions in the dataset. If the threshold value is higher, then we have many easy questions in the dataset. \n",
    "\n",
    "The filters can be applied in any order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec92b06-2e18-440d-8f08-6327c2f7c1e0",
   "metadata": {},
   "source": [
    "Additionally, we need to specify configuration for \n",
    "1. Evaluators if 'evalulate' flag is set to true.\n",
    "2. Analyzers (query length & lexical divergence between context & query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc1813-395c-4a8c-ba27-34dfb9021eeb",
   "metadata": {},
   "source": [
    "Let's see how the config file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffd8fa-5d78-4898-8779-76f7013fae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../scripts/conf/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bda584-1b09-4017-9f25-6b31ca88269c",
   "metadata": {},
   "source": [
    "As you can see above the prompts for generator model, the llm-as-judge model needs to be specified in the config.yaml file.\n",
    "\n",
    "Also, for shorter test runs, the user can specify max_examples parameter. This sets number of input document chunks (from the input file) to be used for synthetic data generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58035707-338c-4b3a-b344-63a82933ce66",
   "metadata": {},
   "source": [
    "The data containing the passages needs to be placed in the data directory. It has to be in jsonl format as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db1e4d-61a5-4546-b8c6-710ec31cee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e5f5a-507e-4d51-9ff9-24d41d4a7664",
   "metadata": {},
   "source": [
    "### Running the SDG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cbf08b-2d15-4cdf-bd69-bde63f9f416c",
   "metadata": {},
   "source": [
    "The pipeline can be run using the run_pipeline.py. It needs api_key which can be obtained using the pointer above. We show here a run using input document in 'rawdoc' format. You can see the progress of the data generation pipeline as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d1d82-bfb0-4d6b-b370-59a48694ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!HYDRA_FULL_ERROR=1 PYTHONPATH=.. python ../scripts/run_pipeline.py \\\n",
    "  api_key=<API Key> \\\n",
    "  input_file=../data/sample_data_rawdoc.jsonl \\\n",
    "  input_format=rawdoc \\\n",
    "  output_dir=../outputs/sample_data_synthetic_w_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3e7a3-ca19-48e3-a80a-6bdf886e9bdb",
   "metadata": {},
   "source": [
    "### Output/Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1c6cc-2341-43e1-ac94-8c11010200d8",
   "metadata": {},
   "source": [
    "After the run, an 'outputs' directory is created, which has two sub-directories, 'beir' and 'squad' containing results in beir and squad formats respecitively. The output directory structure should look like below\n",
    "\n",
    "- beir\n",
    "    - all\n",
    "    - filtered\n",
    "- squad\n",
    "    - all\n",
    "    - filtered\n",
    "- eval\n",
    "    - all\n",
    "        - synthetic\n",
    "        - original (if `use_original=true`)\n",
    "    - filtered\n",
    "        - synthetic\n",
    "        - original (if `use_original=true`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e25c39-2b8e-4413-b13a-a8258ef29eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../outputs/sample_data_synthetic_w_evals/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2878f3a-566b-4f09-b27b-54a014dbe135",
   "metadata": {},
   "source": [
    "Here is the snapshot of sample output in squad format\n",
    "\n",
    "![Sample output in the SQuAD format](../figures/sample_output.png)\n",
    "<p style=\"text-align: center;\">Figure 2. Sample output in the SQuAD format.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77ca22-04fd-4643-ab3e-7d98af8c63e1",
   "metadata": {},
   "source": [
    "As seen from Figure 2, we can observe that in addition to question and answer being generated for the given passage, we also have other meta data such as filter-by-easiness and filter-by-answerability as well as llm-as-judge-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e19fe4-36c4-4044-85b6-99dfacad1648",
   "metadata": {},
   "source": [
    "Filter-by-easiness takes values Y/N which denotes where easiness filter (embedding-model-as-judge) would filter the questions based on the threshold we have set in the config file. We had set a threshold value of 0.8 for cosine-similarity metric, so we observe that filter-by-easiness is 'Y'. The question is deemed too easy for retrieval and would not be passed by easiness filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a9b6a5-3930-4492-93ff-adf78a76bf9c",
   "metadata": {},
   "source": [
    "Filter-by-answerability also takes values Y/N. We see that all the criteria are satisfied for llm-as-judge so question is good quality and would pass the answerability filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449c012-0024-4170-bc42-d1ad8c0941f3",
   "metadata": {},
   "source": [
    "Now lets take a look at the generated queries, we show queries in beir format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4ec84-4762-42a4-92ec-05d84089cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at generated questions \n",
    "!cat ../outputs/sample_data_synthetic_w_evals/beir/all/synthetic/queries.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce6d6f-ff75-417e-bac1-1ab4168406fe",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855f53e-d73c-4d5c-a0d9-8206736bf9b5",
   "metadata": {},
   "source": [
    "We showcase beir evaluation of synthetically generated data. All the results can be found in outputs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e63e4-eeba-4d74-a7d5-6cb0990e505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../outputs/sample_data_synthetic_w_evals/eval/filtered/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585f673-1687-4345-a4b9-17bf1ea29cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f357db9-613f-4e98-8339-3e3bec889ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/sample_data_synthetic_w_evals/eval/all/beir_evaluator__recall5.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540cdcc5-54ec-4c7b-9126-3266283dd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/sample_data_synthetic_w_evals/eval/filtered/beir_evaluator__recall5.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d50b33-0a15-4083-9158-2ed6eb80d8b9",
   "metadata": {},
   "source": [
    "Recall@5 values for three different embedding models. We see a value of 1 for all since we have very small sample data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e8d5d-0f3c-4fa6-acc1-f11ab81854ac",
   "metadata": {},
   "source": [
    "### Analysis Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98582e9f-8e50-405a-8001-0924d016387f",
   "metadata": {},
   "source": [
    "We also showcase other statistics such as query length, lexical divergence (uni-gram) between query and passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bcd3e3-f87a-4714-bd1e-13a49cc0e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(json.load(open(\"../outputs/sample_data_synthetic_w_evals/report__all.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fe903-9223-4d40-8bd9-8626f401457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(json.load(open(\"../outputs/sample_data_synthetic_w_evals/report__filtered.json\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

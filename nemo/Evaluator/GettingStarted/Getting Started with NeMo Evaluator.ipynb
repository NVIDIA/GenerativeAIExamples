{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Nemo Evaluator\n",
    "\n",
    "In the following notebook we will examine a routine experimentation flow where we first select a baseline model and evaluate it on our task, then we customize our model using a dataset created with Synthetic Data generation and evaluate it.\n",
    "\n",
    "We will be working with Llama 3.1 8B Instruct as our baseline model, and customizing it for a title-generation (summarization) task by using the Low-Rank Adaptation (LoRA) Parameter Efficient Fine-tuning (PEFT) method on a document-title pair dataset that was created using Synthetic Data Generation.\n",
    "\n",
    "This notebook will follow from [this](https://github.com/NVIDIA/NeMo/tree/main/tutorials/llm/llama-3/sdg-law-title-generation) customizer tutorial.\n",
    "\n",
    "We will explore how to leverage Nemo Evaluator for the following tasks:\n",
    "\n",
    "1. Baseline Evaluation of Llama 3.1 8B Instruct using BigBench (Intent Recognition)\n",
    "2. Custom Dataset Evaluation of a Customized Model Using ROUGE Through Similarity Metrics\n",
    "\n",
    "Before you begin, you will need to make sure you're in an environment where you have API access to Nemo Evaluator API, baseline model NIM, the customized model NIM, and a judge LLM NIM.\n",
    "\n",
    "For instructions on the above, please check out the detailed [Nemo Evaluator deployment guide](https://docs.nvidia.com/nemo/microservices/latest/set-up/deploy-as-microservices/evaluator/parent-chart.html), and the [NIM deployment guide](https://developer.nvidia.com/docs/nemo-microservices/inference/getting_started/deploy-helm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Nemo Evaluator is Healthy\n",
    "\n",
    "Before digging into the Evaluator Service, we will first need to verify that the service is active and running. The can be achieved through the health endpoint. \n",
    "\n",
    "The first step in this process will be to provide the Nemo Evaluator endpoint URL. Assuming you've followed the deployment guide, you will use the same URL used during the [Verify Installation](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/deploy-helm.html#verify-installation) step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "EVAL_URL = \"<< YOUR EVALUATOR URL >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can send a request to the `/health` endpoint to verify that the endpoint is active and healthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{EVAL_URL}/health\"\n",
    "response = requests.get(endpoint).json()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Evaluation of Llama 3.1 8B Instruct with LM Evaluation Harness\n",
    "\n",
    "The Nemo Evaluator microservice allows users to run a number of academic benchmarks, all of which are accessible through the Nemo Evaluator API.\n",
    "\n",
    "> NOTE: For more details on what evaluations are available, please head to the [Evaluation documentation](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/evaluations.html)\n",
    "\n",
    "For this notebook, we will be running the LM Evaluation Harness evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll point to our NIM baseline model for our \"target\" in our Evaluation payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_config = {\n",
    "  \"type\": \"model\",\n",
    "  \"model\": {\n",
    "    \"api_endpoint\": {\n",
    "      \"url\": \"<< YOUR NIM INFERENCE URL >>\",\n",
    "      \"model_id\": \"<< YOUR MODEL ID >>\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_endpoint = f\"{EVAL_URL}/v1/evaluation/targets\"\n",
    "response = requests.post(\n",
    "    target_endpoint,\n",
    "    json=target_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_namespace = response[\"namespace\"]\n",
    "target_name = response[\"name\"]\n",
    "print(f\"Target Namespace: {target_namespace}, Target Name: {target_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize our evaluation config, which is how we communicate which benchmark tasks, subtasks, etc. to use during evaluation. \n",
    "\n",
    "For this evaluation, we'll focus on the [GSM8K](https://arxiv.org/abs/2110.14168) evaluation which uses Eleuther AI's [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/v0.4.3) as a backend. \n",
    "\n",
    "The LM Evaluation Harness supports more than 60 standard academic benchmarks for LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config = {\n",
    " \"type\": \"lm_eval_harness\",\n",
    " \"tasks\": [\n",
    "   {\n",
    "     \"type\": \"gsm8k\",\n",
    "     \"params\": {\n",
    "       \"num_fewshot\": 0,\n",
    "       \"batch_size\": 4,\n",
    "       \"bootstrap_iters\": 10,\n",
    "       \"limit\": 5\n",
    "     }\n",
    "   }\n",
    " ],\n",
    " \"params\": {\n",
    "   \"use_greedy\": True,\n",
    "   \"top_p\": 0.0,\n",
    "   \"top_k\": 1,\n",
    "   \"temperature\": 1.0,\n",
    "   \"stop\": [\n",
    "     \"<|endoftext|>\",\n",
    "     \"<extra_id_1>\"\n",
    "   ],\n",
    "   \"tokens_to_generate\": 512\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our payload - we can send it to our Nemo Evaluator endpoint.\n",
    "\n",
    "We'll set up our Evaluator endpoint URL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_endpoint = f\"{EVAL_URL}/v1/evaluation/configs\"\n",
    "response = requests.post(\n",
    "    eval_config_endpoint,\n",
    "    json=evaluation_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again capture our evaluation config for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval-config-WkskqHD4VeawBTTgQnP2BP\n"
     ]
    }
   ],
   "source": [
    "config_namespace = response[\"namespace\"]\n",
    "config_name = response[\"name\"]\n",
    "print(f\"Config Namespace: {config_namespace}, Config Name: {config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Evaluation Job\n",
    "\n",
    "Now that we have our `target_id` and `config_id` -  we have everything we need to run an evaluation.\n",
    "\n",
    "Let's see the process to create and run a job! \n",
    "\n",
    "First things first, we need to create a job payload to send to our endpoint - this will point to our target, and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = {\n",
    "    \"target\": target_namespace + \"/\" + target_name,\n",
    "    \"config\": config_namespace + \"/\" + config_name,\n",
    "    \"tags\": [\n",
    "        \"lm-eval-harness-gsm8k\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set the evaluation jobs endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    job_endpoint,\n",
    "    json=job_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = response[\"id\"]\n",
    "print(f\"Job ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    monitoring_endpoint,\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our job status and wait for it to be done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"status\"][\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done - let's look at the full results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload a Custom Dataset for Evaluation\n",
    "\n",
    "The first thing we will need to do is to upload our custom dataset to the Data Store. The dataset is provided in the `custom_dataset` directory. \n",
    "\n",
    "First, we will examine the structure of the dataset:\n",
    "\n",
    "- `inputs.jsonl` is a collection of the raw question prompts that can be useful for custom evaluation.\n",
    "\n",
    "### Preparing to Upload to Data Store\n",
    "\n",
    "In order to upload this custom dataset, we'll take advantage of the Hugging Face Hub library from Hugging Face to interact with our Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU huggingface_hub==0.26.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll point to our Data Store API and use the provided `mock` token to gain access to the Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_url = \"<< YOUR_DATASTORE_URL >>\"\n",
    "token = \"mock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also name our Data Store repository with something descriptive so we can reference it later.\n",
    "\n",
    "We will also provide the path to our local data that needs to be added to our Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_name = \"nvidia/legal-title-dataset\"\n",
    "repository_type = \"dataset\"\n",
    "local_data_path = \"./custom_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an empty dataset repository in our Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hh\n",
    "\n",
    "hf_api = hh.HfApi(endpoint=datastore_url, token=token)\n",
    "\n",
    "hf_api.create_repo(\n",
    "    repo_id=repository_name,\n",
    "    repo_type=repository_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a repository available on our Data Store - we can upload our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in_repo = \".\"\n",
    "result = hf_api.upload_folder(repo_id=repository_name, folder_path=local_data_path, path_in_repo=path_in_repo, repo_type=repository_type)\n",
    "print(f\"Dataset Folder Uploaded To: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Similarity Metrics to Evaluate the Customized Model on ROUGE \n",
    "\n",
    "Now that we've seen how our baseline performs on our task - we can evaluate our customized model on the same metric to see how it performs.\n",
    "\n",
    "ROUGE is available through the `similarity_metrics` - which contains metrics where we compare the target model's response to a ground truth. Other similarity metrics are available as well, like `f1`, `bleu`, and more!\n",
    "\n",
    "> NOTE: As a reminder, we used PEFT LoRA to customize our model on synthetically created document-title data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the model config above with minor modifications - which needs to reference the customized model's NIM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_config = {\n",
    "  \"type\": \"model\",\n",
    "  \"model\": {\n",
    "    \"api_endpoint\": {\n",
    "      \"url\": \"<< YOUR CUSTOMIZED NIM INFERENCE URL >>\",\n",
    "      \"model_id\": \"<< YOUR MODEL ID >>\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_endpoint = f\"{EVAL_URL}/v1/evaluation/targets\"\n",
    "response = requests.post(\n",
    "    target_endpoint,\n",
    "    json=target_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_namespace = response[\"namespace\"]\n",
    "target_name = response[\"name\"]\n",
    "print(f\"Target Namespace: {target_namespace}, Target Name: {target_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a customized evaluation config for our ROUGE evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config = {\n",
    " \"type\": \"similarity_metrics\",\n",
    " \"tasks\": [\n",
    "   {\n",
    "     \"type\": \"default\",\n",
    "     \"metrics\": [\n",
    "       {\n",
    "         \"name\": \"rouge\"\n",
    "       },\n",
    "     ],\n",
    "     \"dataset\": {\n",
    "       \"files_url\": f\"nds:{repository_name}/inputs.jsonl\",\n",
    "     },\n",
    "     \"params\": {\n",
    "       \"tokens_to_generate\": 200,\n",
    "       \"temperature\": 0.7,\n",
    "       \"top_k\": 20,\n",
    "       \"n_samples\": -1\n",
    "     }\n",
    "   }\n",
    " ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our evaluation config name and namespacefrom the Evaluator API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_endpoint = f\"{EVAL_URL}/v1/evaluation/configs\"\n",
    "response = requests.post(\n",
    "    eval_config_endpoint,\n",
    "    json=evaluation_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_namespace = response[\"namespace\"]\n",
    "config_name = response[\"name\"]\n",
    "print(f\"Config Namespace: {config_namespace}, Config Name: {config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can send the evaluation job off to the Evaluator API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = {\n",
    "    \"target\": target_namespace + \"/\" + target_name,\n",
    "    \"config\": config_namespace + \"/\" + config_name,\n",
    "    \"tags\": [\n",
    "        \"rouge-similarity\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set the evaluation jobs endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    job_endpoint,\n",
    "    json=job_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    monitoring_endpoint,\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our job status and wait for it to be done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"status\"][\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done - let's look at the full results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia-sdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

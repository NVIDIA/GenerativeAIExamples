{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LLM-as-a-Judge Implementation\n",
    "\n",
    "In the following notebook, we'll be walking through an example of how you can leverage Custom LLM-as-a-Judge through NeMo Evaluator Microservice. \n",
    "\n",
    "Full documentation is available [here](https://docs.nvidia.com/nemo/microservices/latest/evaluate/evaluation-custom.html#evaluation-with-llm-as-a-judge)!\n",
    "\n",
    "In our example - we'll be looking at the following scenario:\n",
    "\n",
    "*We have a JSONL file with medical consultation information (synthetically generated). We will use a [`build.nvidia.com`](https://build.nvidia.com/) endpoint model to generate summaries of those consultations - and then use OpenAI to judge the summaries on metrics we define ahead of time - in this case: Correctness and Completeness.*\n",
    "\n",
    "We'll note different places you could change this example to adjust to your desired workflow along the way, as Custom LLM-as-a-Judge is a flexible evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Configurations\n",
    "\n",
    "You'll need to have set up the NeMo Microservices including: \n",
    "\n",
    "- NeMo Evaluator\n",
    "- NeMo Data Store and Entity Store\n",
    "\n",
    "If you wish to evaluate a NIM for LLMs, or use a NIM for LLMs as a judge, you will also need to provide the respective NIM for LLMs URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Required) NeMo Microservices URLs\n",
    "NDS_URL = \"\"\n",
    "NEMO_URL = \"\"\n",
    "# (Optional based on use case) NeMo Microservices URLs\n",
    "NIM_URL = \"\"\n",
    "\n",
    "# If you have set a token for NeMo Data Store, provide it here\n",
    "NDS_TOKEN = \"token\"\n",
    "\n",
    "# Configure to your liking!\n",
    "NMS_NAMESPACE = \"custom-llm-as-a-judge-eval\"\n",
    "DATASET_NAME = \"custom-llm-as-a-judge-eval-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store endpoint: https://nmp.int.aire.nvidia.com\n",
      "Entity Store, Customizer, Evaluator endpoint: https://datastore.int.aire.nvidia.com\n",
      "NIM endpoint: https://nim.int.aire.nvidia.com\n",
      "Namespace: custom-llm-as-a-judge-eval\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Store endpoint: {NDS_URL}\")\n",
    "print(f\"Entity Store, Customizer, Evaluator endpoint: {NEMO_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up NeMo Data Store and Entity Store\n",
    "\n",
    "We'll first need to ensure that our namespace is created and is available both in our NeMo Entity Store and Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [201]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def create_namespaces(entity_host, ds_host, namespace):\n",
    "    # Create namespace in Entity Store\n",
    "    entity_store_url = f\"{entity_host}/v1/namespaces\"\n",
    "    resp = requests.post(entity_store_url, json={\"id\": namespace})\n",
    "    assert resp.status_code in (200, 201, 409, 422), \\\n",
    "        f\"Unexpected response from Entity Store during namespace creation: {resp.status_code}\"\n",
    "    print(resp)\n",
    "\n",
    "    # Create namespace in Data Store\n",
    "    nds_url = f\"{ds_host}/v1/datastore/namespaces\"\n",
    "    resp = requests.post(nds_url, data={\"namespace\": namespace})\n",
    "    assert resp.status_code in (200, 201, 409, 422), \\\n",
    "        f\"Unexpected response from Data Store during namespace creation: {resp.status_code}\"\n",
    "    print(resp)\n",
    "\n",
    "create_namespaces(entity_host=NEMO_URL, ds_host=NDS_URL, namespace=NMS_NAMESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a simple verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 201\n",
      "Response JSON: {'namespace': 'custom-llm-as-a-judge-eval-v1', 'created_at': '2025-05-08T17:19:19Z', 'updated_at': '2025-05-08T17:19:19Z'}\n",
      "Status Code: 200\n",
      "Response JSON: {'id': 'custom-llm-as-a-judge-eval-v1', 'created_at': '2025-05-08T17:19:19.316626', 'updated_at': '2025-05-08T17:19:19.316630', 'description': None, 'project': None, 'custom_fields': {}, 'ownership': None}\n"
     ]
    }
   ],
   "source": [
    "# Verify Namespace in Data Store\n",
    "response = requests.get(f\"{NDS_URL}/v1/datastore/namespaces/{NMS_NAMESPACE}\")\n",
    "print(f\"Status Code: {response.status_code}\\nResponse JSON: {response.json()}\")\n",
    "\n",
    "# Verify Namespace in Entity Store\n",
    "response = requests.get(f\"{NEMO_URL}/v1/namespaces/{NMS_NAMESPACE}\")\n",
    "print(f\"Status Code: {response.status_code}\\nResponse JSON: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Repository for our Data\n",
    "\n",
    "Next, we'll want to create a repository on our NeMo Data Store!\n",
    "\n",
    "We'll start by defining our repository ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = f\"{NMS_NAMESPACE}/{DATASET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the Hugging Face Hub API to create the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupiter-core/Code/NVIDIA/GenerativeAIExamples/nemo/Evaluator/Custom LLM-as-a-Judge/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RepoUrl('datasets/custom-llm-as-a-judge-eval-v1/custom-llm-as-a-judge-eval-data-v1', endpoint='https://datastore.int.aire.nvidia.com/v1/hf', repo_type='dataset', repo_id='custom-llm-as-a-judge-eval-v1/custom-llm-as-a-judge-eval-data-v1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "hf_api = HfApi(endpoint=f\"{NDS_URL}/v1/hf\", token=\"\")\n",
    "\n",
    "# Create repo\n",
    "hf_api.create_repo(\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to upload our data to the NeMo Data Store, but before we do - let's take a look at it!\n",
    "\n",
    "Here's an example of a row of data:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"ID\": \"C012\", \n",
    "    \"content\": \"Date: 2025-04-12\\nChief Complaint (CC): ...\", \n",
    "    \"summary\": \"New Clinical Problem: ...\"\n",
    "}\n",
    "```\n",
    "\n",
    "As you can see, we have a `content` field with a synthetically generated medical consultation, as well as a `summary` field with an AI generated summary. \n",
    "\n",
    "> NOTE: In this example we won't be directly leveraging the `summary` field - but we'll cover how you would be able to leverage extra fields if they were necessary!\n",
    "\n",
    "Next, let's upload our file directly to our newly created repository using the following code cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='', commit_message='Upload doctor_consults_with_summaries.jsonl with huggingface_hub', commit_description='', oid='6f73f13bc78005f7edc7306437aa61a758e0c560', pr_url=None, repo_url=RepoUrl('', endpoint='https://huggingface.co', repo_type='model', repo_id=''), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_api.upload_file(\n",
    "    path_or_fileobj=\"./doctor_consults_with_summaries.jsonl\",\n",
    "    path_in_repo=\"doctor_consults_with_summaries.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll registed the dataset with NeMo Entity Store!\n",
    "\n",
    "This will allow us to leverage this dataset for evaluation jobs - through the `/v1/datasets/` endpoint, which will allow us to refer to the dataset by it's namespace and name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': '2025-05-08T17:19:35.030601',\n",
       " 'updated_at': '2025-05-08T17:19:35.030605',\n",
       " 'name': 'custom-llm-as-a-judge-eval-data-v1',\n",
       " 'namespace': 'custom-llm-as-a-judge-eval-v1',\n",
       " 'description': 'LLM As a Judge Test',\n",
       " 'format': None,\n",
       " 'files_url': 'hf://datasets/custom-llm-as-a-judge-eval-v1/custom-llm-as-a-judge-eval-data-v1',\n",
       " 'hf_endpoint': None,\n",
       " 'split': None,\n",
       " 'limit': None,\n",
       " 'id': 'dataset-CU15CaykeHJJPTKrZDJw68',\n",
       " 'project': 'custom-llm-as-a-judge-test',\n",
       " 'custom_fields': {}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.post(\n",
    "    url=f\"{NEMO_URL}/v1/datasets\",\n",
    "    json={\n",
    "        \"name\": DATASET_NAME,\n",
    "        \"namespace\": NMS_NAMESPACE,\n",
    "        \"description\": \"LLM As a Judge Test\",\n",
    "        \"files_url\": f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}\",\n",
    "        \"project\": \"custom-llm-as-a-judge-test\",\n",
    "    },\n",
    ")\n",
    "assert resp.status_code in (200, 201), f\"Status Code {resp.status_code} Failed to create dataset {resp.text}\"\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's verify it landed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files URL: hf://datasets/custom-llm-as-a-judge-eval-v1/custom-llm-as-a-judge-eval-data-v1\n"
     ]
    }
   ],
   "source": [
    "res = requests.get(url=f\"{NEMO_URL}/v1/datasets/{NMS_NAMESPACE}/{DATASET_NAME}\")\n",
    "assert res.status_code in (200, 201), f\"Status Code {res.status_code} Failed to fetch dataset {res.text}\"\n",
    "dataset_obj = res.json()\n",
    "\n",
    "print(\"Files URL:\", dataset_obj[\"files_url\"])\n",
    "assert dataset_obj[\"files_url\"] == f\"hf://datasets/{repo_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMo Evaluator Set-Up\n",
    "\n",
    "In the following steps, we'll make a few assumptions:\n",
    "\n",
    "1. You will be using an OpenAI model as the Judge LLM\n",
    "2. You will be using a [`build.nvidia.com`](https://build.nvidia.com/) model to generate responses. \n",
    "\n",
    "Each of these models can be changed to accomodate NIM for LLMs, or any OpenAI API compatible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Configuration Set Up\n",
    "\n",
    "In order to use both the OpenAI model, and the [`build.nvidia.com`](https://build.nvidia.com/) model, we'll need to provide our API keys for both!\n",
    "\n",
    "> NOTE: You can find the API key on [`build.nvidia.com`](https://build.nvidia.com/) by clicking the green \"Get API Key\" button!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NVDEV_API_KEY = getpass.getpass(\"NVDEV API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Judge LLM Configuration\n",
    "\n",
    "In the following cell - we'll going to set our Judge LLM configuration - while the example provided is for an OpenAI model - you could change this to point at any Judge LLM you'd like that is compatible with NeMo Evaluator. \n",
    "\n",
    "This includes, but is not limited to:\n",
    "\n",
    "Completion Endpoints\n",
    "```python \n",
    "\"api_endpoint\": {\n",
    "    \"url\": \"<my-nim-deployment-base-url>/chat/completions\",\n",
    "    \"model_id\": \"<my-model>\"\n",
    "}\n",
    "```\n",
    "\n",
    "External Endpoint\n",
    "```python \n",
    "\"api_endpoint\": {\n",
    "    \"url\": \"<external-openai-compatible-base-url>/chat/completions\",\n",
    "    \"model_id\": \"<external-model>\",\n",
    "    \"api_key\": \"<my-api-key>\",\n",
    "    \"format\": \"openai\"        \n",
    "}\n",
    "```\n",
    "\n",
    "You can check out more examples on this page of the [documentation](https://docs.nvidia.com/nemo/microservices/latest/evaluate/evaluation-targets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model_config = {\n",
    "    \"api_endpoint\": {\n",
    "        \"url\": \"https://api.openai.com/v1/chat/completions\",\n",
    "        \"model_id\": \"gpt-4.1\",\n",
    "        \"api_key\": OPENAI_API_KEY,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a few prompt templates we can use for our Judge LLM to judge the produced summary on a few different metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_system_prompt = \"\"\"\n",
    "You are a judge. Rate how complete the summary is \n",
    "on a scale from 1 to 5:\n",
    "1 = missing critical information … 5 = fully complete\n",
    "Please respond with RATING: <number>\n",
    "\"\"\"\n",
    "\n",
    "correctness_system_prompt = \"\"\"\n",
    "You are a judge. Rate the summary's correctness \n",
    "(no false info) on a scale 1-5:\n",
    "1 = many inaccuracies … 5 = completely accurate\n",
    "Please respond with RATING: <number>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set up our user prompt, which we'll use across both metrics. \n",
    "\n",
    "Notice that we can reference items in our dataset through the `{{ item.content }}` template. If we wanted to address our summaries, we could instead use `{{ item.summary }}`!\n",
    "\n",
    "Also notice that we can address the generation from our target LLM with the ``{{ sample.output_text }}``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "Full Consult: {{ item.content }}\n",
    "Summary: {{ sample.output_text }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to pass a `regex` parser so we can collect the numeric scores from our prompt - for this reason, it's important to specify in the system prompt some easily identifiable score extraction sequence. \n",
    "\n",
    "In the example system prompt above, you'll notice we used:\n",
    "\n",
    "```python\n",
    "\"Please respond with RATING: <number>\"\n",
    "```\n",
    "\n",
    "This allows us to use the following parser to collect our scores.\n",
    "\n",
    "```python\n",
    "\"scores\": { \n",
    "    \"completeness\": { \n",
    "        \"type\": \"int\",\n",
    "        \"parser\": {\n",
    "            \"type\": \"regex\",\n",
    "            \"pattern\": r\"RATING:\\s*(\\d+)\"\n",
    "        }\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the atomic parts of our Custom LLM-as-a-Judge evaluation configuration in place - let's build the whole thing!\n",
    "\n",
    "> NOTE: We're using two metrics here `correctness` and `completeness` - but you can define more (or a single metric) as you see fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_as_a_judge_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"name\": \"doctor_consult_summary_eval\",\n",
    "    \"tasks\": {\n",
    "        \"consult_summary_eval\": {\n",
    "            \"type\": \"chat-completion\",\n",
    "            \"params\": {\n",
    "                \"template\": {\n",
    "                    # This is where we define the prompt template that will be sent to the target LLM we are evaluating. \n",
    "                    # Notice that we can reference items in our dataset through the `{{ item.content }}` template in this prompt as well.\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": (\n",
    "                                \"Given a full medical consultation, please provide a 50 word summary of the consultation.\"\n",
    "                            )\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": (\n",
    "                                \"Full Consult: {{ item.content }}\"\n",
    "                            )\n",
    "                        }\n",
    "                    ],\n",
    "                    \"max_tokens\": 200\n",
    "                }\n",
    "            },\n",
    "            \"dataset\": {\n",
    "                \"files_url\": (\n",
    "                    f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}/\"\n",
    "                ),\n",
    "                # This is where we can limit the number of samples we want to use for evaluation.\n",
    "                \"limit\" : 25\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"completeness\": {\n",
    "                    \"type\": \"llm-judge\",\n",
    "                    \"params\": {\n",
    "                        \"model\": judge_model_config,\n",
    "                        \"template\": {\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": completeness_system_prompt\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": user_prompt\n",
    "                                }\n",
    "                            ]\n",
    "                        },\n",
    "                        \"scores\": { \n",
    "                            \"completeness\": { \n",
    "                                \"type\": \"int\",\n",
    "                                \"parser\": {\n",
    "                                    \"type\": \"regex\",\n",
    "                                    \"pattern\": r\"RATING:\\s*(\\d+)\"\n",
    "                                }\n",
    "                            },\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"correctness\": {\n",
    "                    \"type\": \"llm-judge\",\n",
    "                    \"params\": {\n",
    "                        \"model\": judge_model_config,\n",
    "                        \"template\": {\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": correctness_system_prompt\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": user_prompt\n",
    "                                }\n",
    "                            ]\n",
    "                        },\n",
    "                        \"scores\": { \n",
    "                            \"correctness\": { \n",
    "                                \"type\": \"int\",\n",
    "                                \"parser\": {\n",
    "                                    \"type\": \"regex\",\n",
    "                                    \"pattern\": r\"RATING:\\s*(\\d+)\"\n",
    "                                }\n",
    "                            },\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Configuration\n",
    "\n",
    "Just as with the Judge LLM - you can identify any targets, please see the [Target documentation](https://docs.nvidia.com/nemo/microservices/latest/evaluate/evaluation-targets.html) for more examples!\n",
    "\n",
    "We're going to be using Llama 3.1 70B as our model to be tested in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_3_1_70b_target = {\n",
    "    \"type\" : \"model\",\n",
    "    \"model\" : {\n",
    "        \"api_endpoint\": {\n",
    "            \"url\": \"https://integrate.api.nvidia.com/v1/chat/completions\",\n",
    "            \"model_id\": \"meta/llama-3.1-70b-instruct\",\n",
    "            \"api_key\": NVDEV_API_KEY\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Job and Status\n",
    "\n",
    "At this point - we're ready to kick-off our Evaluation Job as we've prepared both our Evaluation Configuration and our Target configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': '2025-05-08T17:21:24.210032',\n",
       " 'updated_at': '2025-05-08T17:21:24.210033',\n",
       " 'id': 'eval-CjUYLQdriBtAA5X9KPDLAU',\n",
       " 'namespace': 'default',\n",
       " 'description': None,\n",
       " 'target': {'schema_version': '1.0',\n",
       "  'id': 'eval-target-V5VgZ54rJBeoD8w1WmzSHn',\n",
       "  'description': None,\n",
       "  'type_prefix': 'eval-target',\n",
       "  'namespace': 'default',\n",
       "  'project': None,\n",
       "  'created_at': '2025-05-08T17:21:24.209441',\n",
       "  'updated_at': '2025-05-08T17:21:24.209442',\n",
       "  'custom_fields': {},\n",
       "  'ownership': None,\n",
       "  'name': 'eval-target-V5VgZ54rJBeoD8w1WmzSHn',\n",
       "  'type': 'model',\n",
       "  'cached_outputs': None,\n",
       "  'model': {'schema_version': '1.0',\n",
       "   'id': 'model-R1gn9w1tPwdfukCDoCBF2F',\n",
       "   'description': None,\n",
       "   'type_prefix': 'model',\n",
       "   'namespace': 'default',\n",
       "   'project': None,\n",
       "   'created_at': '2025-05-08T17:21:24.209465',\n",
       "   'updated_at': '2025-05-08T17:21:24.209465',\n",
       "   'custom_fields': {},\n",
       "   'ownership': None,\n",
       "   'name': 'model-R1gn9w1tPwdfukCDoCBF2F',\n",
       "   'version_id': 'main',\n",
       "   'version_tags': [],\n",
       "   'spec': None,\n",
       "   'artifact': None,\n",
       "   'base_model': None,\n",
       "   'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/chat/completions',\n",
       "    'model_id': 'meta/llama-3.1-70b-instruct',\n",
       "    'api_key': '******',\n",
       "    'format': 'nim'},\n",
       "   'peft': None,\n",
       "   'prompt': None,\n",
       "   'guardrails': None},\n",
       "  'retriever': None,\n",
       "  'rag': None,\n",
       "  'rows': None,\n",
       "  'dataset': None},\n",
       " 'config': {'schema_version': '1.0',\n",
       "  'id': 'eval-config-MGiCNpVtA1vKP3e7Npqm3P',\n",
       "  'description': None,\n",
       "  'type_prefix': 'eval-config',\n",
       "  'namespace': 'default',\n",
       "  'project': None,\n",
       "  'created_at': '2025-05-08T17:21:24.209320',\n",
       "  'updated_at': '2025-05-08T17:21:24.209322',\n",
       "  'custom_fields': {},\n",
       "  'ownership': None,\n",
       "  'name': 'doctor_consult_summary_eval',\n",
       "  'type': 'custom',\n",
       "  'params': None,\n",
       "  'tasks': {'consult_summary_eval': {'type': 'chat-completion',\n",
       "    'params': {'template': {'messages': [{'role': 'system',\n",
       "        'content': 'Given a full medical consultation, please provide a 50 word summary of the consultation.'},\n",
       "       {'role': 'user', 'content': 'Full Consult: {{ item.content }}'}],\n",
       "      'max_tokens': 200}},\n",
       "    'metrics': {'completeness': {'type': 'llm-judge',\n",
       "      'params': {'model': {'api_endpoint': {'url': 'https://api.openai.com/v1/chat/completions',\n",
       "         'model_id': 'gpt-4.1',\n",
       "         'api_key': '******'}},\n",
       "       'template': {'messages': [{'role': 'system',\n",
       "          'content': '\\nYou are a judge. Rate how complete the summary is \\non a scale from 1 to 5:\\n1 = missing critical information … 5 = fully complete\\nPlease respond with RATING: <number>\\n'},\n",
       "         {'role': 'user',\n",
       "          'content': '\\nFull Consult: {{ item.content }}\\nSummary: {{ sample.output_text }}\\n'}]},\n",
       "       'scores': {'completeness': {'type': 'int',\n",
       "         'parser': {'type': 'regex', 'pattern': 'RATING:\\\\s*(\\\\d+)'}}}}},\n",
       "     'correctness': {'type': 'llm-judge',\n",
       "      'params': {'model': {'api_endpoint': {'url': 'https://api.openai.com/v1/chat/completions',\n",
       "         'model_id': 'gpt-4.1',\n",
       "         'api_key': '******'}},\n",
       "       'template': {'messages': [{'role': 'system',\n",
       "          'content': \"\\nYou are a judge. Rate the summary's correctness \\n(no false info) on a scale 1-5:\\n1 = many inaccuracies … 5 = completely accurate\\nPlease respond with RATING: <number>\\n\"},\n",
       "         {'role': 'user',\n",
       "          'content': '\\nFull Consult: {{ item.content }}\\nSummary: {{ sample.output_text }}\\n'}]},\n",
       "       'scores': {'correctness': {'type': 'int',\n",
       "         'parser': {'type': 'regex', 'pattern': 'RATING:\\\\s*(\\\\d+)'}}}}}},\n",
       "    'dataset': {'schema_version': '1.0',\n",
       "     'id': 'dataset-9dhgzV4RtNi1vceLtG9b37',\n",
       "     'description': None,\n",
       "     'type_prefix': None,\n",
       "     'namespace': 'default',\n",
       "     'project': None,\n",
       "     'created_at': '2025-05-08T17:21:24.209381',\n",
       "     'updated_at': '2025-05-08T17:21:24.209381',\n",
       "     'custom_fields': {},\n",
       "     'ownership': None,\n",
       "     'name': 'dataset-9dhgzV4RtNi1vceLtG9b37',\n",
       "     'version_id': 'main',\n",
       "     'version_tags': [],\n",
       "     'format': None,\n",
       "     'files_url': 'hf://datasets/custom-llm-as-a-judge-eval-v1/custom-llm-as-a-judge-eval-data-v1/',\n",
       "     'hf_endpoint': None,\n",
       "     'split': None,\n",
       "     'limit': 25}}},\n",
       "  'groups': None},\n",
       " 'result': None,\n",
       " 'output_files_url': None,\n",
       " 'status_details': {'message': None, 'task_status': {}, 'progress': None},\n",
       " 'status': 'created',\n",
       " 'project': None,\n",
       " 'custom_fields': {},\n",
       " 'ownership': None}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.post(\n",
    "    f\"{NEMO_URL}/v1/evaluation/jobs\",\n",
    "    json={\n",
    "        \"config\": llm_as_a_judge_config,\n",
    "        \"target\": llama_3_1_70b_target\n",
    "    }\n",
    ")\n",
    "\n",
    "base_eval_job_id = res.json()[\"id\"]\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following helper function to wait for our job to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep, time\n",
    "\n",
    "def wait_eval_job(job_url: str, polling_interval: int = 10, timeout: int = 6000):\n",
    "    \"\"\"Helper for waiting an eval job.\"\"\"\n",
    "    start_time = time()\n",
    "    res = requests.get(job_url)\n",
    "    status = res.json()[\"status\"]\n",
    "\n",
    "    while (status in [\"pending\", \"created\", \"running\"]):\n",
    "        # Check for timeout\n",
    "        if time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Took more than {timeout} seconds.\")\n",
    "\n",
    "        # Sleep before polling again\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        # Fetch updated status and progress\n",
    "        res = requests.get(job_url)\n",
    "        status = res.json()[\"status\"]\n",
    "\n",
    "        # Progress details (only fetch if status is \"running\")\n",
    "        if status == \"running\":\n",
    "            progress = res.json().get(\"status_details\", {}).get(\"progress\", 0)\n",
    "        elif status == \"completed\":\n",
    "            progress = 100\n",
    "\n",
    "        print(f\"Job status: {status} after {time() - start_time:.2f} seconds. Progress: {progress}%\")\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job itself may take ~250-300s to complete, depending on hardware, models used, and other factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 6.03 seconds. Progress: 8.0%\n",
      "Job status: running after 14.26 seconds. Progress: 16.0%\n",
      "Job status: running after 21.37 seconds. Progress: 20.0%\n",
      "Job status: running after 26.88 seconds. Progress: 28.0%\n",
      "Job status: running after 32.41 seconds. Progress: 32.0%\n",
      "Job status: running after 37.93 seconds. Progress: 40.0%\n",
      "Job status: running after 43.44 seconds. Progress: 44.0%\n",
      "Job status: running after 48.95 seconds. Progress: 48.0%\n",
      "Job status: running after 56.06 seconds. Progress: 56.0%\n",
      "Job status: running after 61.57 seconds. Progress: 64.0%\n",
      "Job status: running after 67.08 seconds. Progress: 72.0%\n",
      "Job status: running after 72.59 seconds. Progress: 72.0%\n",
      "Job status: running after 78.12 seconds. Progress: 80.0%\n",
      "Job status: running after 83.64 seconds. Progress: 84.0%\n",
      "Job status: running after 90.75 seconds. Progress: 88.0%\n",
      "Job status: running after 96.26 seconds. Progress: 96.0%\n",
      "Job status: completed after 101.78 seconds. Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "res = wait_eval_job(f\"{NEMO_URL}/v1/evaluation/jobs/{base_eval_job_id}\", polling_interval=5, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify our job is complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2025-05-08T17:21:24.210032', 'updated_at': '2025-05-08T17:23:09.228092', 'id': 'eval-CjUYLQdriBtAA5X9KPDLAU', 'namespace': 'default', 'description': None, 'target': {'schema_version': '1.0', 'id': 'eval-target-V5VgZ54rJBeoD8w1WmzSHn', 'description': None, 'type_prefix': 'eval-target', 'namespace': 'default', 'project': None, 'created_at': '2025-05-08T17:21:24.209441', 'updated_at': '2025-05-08T17:21:24.209442', 'custom_fields': {}, 'ownership': None, 'name': 'eval-target-V5VgZ54rJBeoD8w1WmzSHn', 'type': 'model', 'cached_outputs': None, 'model': {'schema_version': '1.0', 'id': 'model-R1gn9w1tPwdfukCDoCBF2F', 'description': None, 'type_prefix': 'model', 'namespace': 'default', 'project': None, 'created_at': '2025-05-08T17:21:24.209465', 'updated_at': '2025-05-08T17:21:24.209465', 'custom_fields': {}, 'ownership': None, 'name': 'model-R1gn9w1tPwdfukCDoCBF2F', 'version_id': 'main', 'version_tags': [], 'spec': None, 'artifact': None, 'base_model': None, 'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/chat/completions', 'model_id': 'meta/llama-3.1-70b-instruct', 'api_key': '******', 'format': 'nim'}, 'peft': None, 'prompt': None, 'guardrails': None}, 'retriever': None, 'rag': None, 'rows': None, 'dataset': None}, 'config': {'schema_version': '1.0', 'id': 'eval-config-MGiCNpVtA1vKP3e7Npqm3P', 'description': None, 'type_prefix': 'eval-config', 'namespace': 'default', 'project': None, 'created_at': '2025-05-08T17:21:24.209320', 'updated_at': '2025-05-08T17:21:24.209322', 'custom_fields': {}, 'ownership': None, 'name': 'doctor_consult_summary_eval', 'type': 'custom', 'params': None, 'tasks': {'consult_summary_eval': {'type': 'chat-completion', 'params': {'template': {'messages': [{'role': 'system', 'content': 'Given a full medical consultation, please provide a 50 word summary of the consultation.'}, {'role': 'user', 'content': 'Full Consult: {{ item.content }}'}], 'max_tokens': 200}}, 'metrics': {'completeness': {'type': 'llm-judge', 'params': {'model': {'api_endpoint': {'url': 'https://api.openai.com/v1/chat/completions', 'model_id': 'gpt-4.1', 'api_key': '******'}}, 'template': {'messages': [{'role': 'system', 'content': '\\nYou are a judge. Rate how complete the summary is \\non a scale from 1 to 5:\\n1 = missing critical information … 5 = fully complete\\nPlease respond with RATING: <number>\\n'}, {'role': 'user', 'content': '\\nFull Consult: {{ item.content }}\\nSummary: {{ sample.output_text }}\\n'}]}, 'scores': {'completeness': {'type': 'int', 'parser': {'type': 'regex', 'pattern': 'RATING:\\\\s*(\\\\d+)'}}}}}, 'correctness': {'type': 'llm-judge', 'params': {'model': {'api_endpoint': {'url': 'https://api.openai.com/v1/chat/completions', 'model_id': 'gpt-4.1', 'api_key': '******'}}, 'template': {'messages': [{'role': 'system', 'content': \"\\nYou are a judge. Rate the summary's correctness \\n(no false info) on a scale 1-5:\\n1 = many inaccuracies … 5 = completely accurate\\nPlease respond with RATING: <number>\\n\"}, {'role': 'user', 'content': '\\nFull Consult: {{ item.content }}\\nSummary: {{ sample.output_text }}\\n'}]}, 'scores': {'correctness': {'type': 'int', 'parser': {'type': 'regex', 'pattern': 'RATING:\\\\s*(\\\\d+)'}}}}}}, 'dataset': {'schema_version': '1.0', 'id': 'dataset-9dhgzV4RtNi1vceLtG9b37', 'description': None, 'type_prefix': None, 'namespace': 'default', 'project': None, 'created_at': '2025-05-08T17:21:24.209381', 'updated_at': '2025-05-08T17:21:24.209381', 'custom_fields': {}, 'ownership': None, 'name': 'dataset-9dhgzV4RtNi1vceLtG9b37', 'version_id': 'main', 'version_tags': [], 'format': None, 'files_url': 'hf://datasets/custom-llm-as-a-judge-eval-v1/custom-llm-as-a-judge-eval-data-v1/', 'hf_endpoint': None, 'split': None, 'limit': 25}}}, 'groups': None}, 'result': 'evaluation_result-QWzZVGtKY1Z7hdty63hLfP', 'output_files_url': 'hf://datasets/evaluation-results/eval-CjUYLQdriBtAA5X9KPDLAU', 'status_details': {'message': 'Job completed successfully.', 'task_status': {'consult_summary_eval': 'completed'}, 'progress': 100.0}, 'status': 'completed', 'project': None, 'custom_fields': {}, 'ownership': None}\n"
     ]
    }
   ],
   "source": [
    "print(res.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's complete - we can look at the scores the Custom LLM-as-a-Judge evaluation produced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': '2025-05-08T17:21:24.281058',\n",
       " 'updated_at': '2025-05-08T17:21:24.281059',\n",
       " 'id': 'evaluation_result-QWzZVGtKY1Z7hdty63hLfP',\n",
       " 'job': 'eval-CjUYLQdriBtAA5X9KPDLAU',\n",
       " 'tasks': {'consult_summary_eval': {'metrics': {'completeness': {'scores': {'completeness': {'value': 4.92,\n",
       "       'stats': {'count': 25, 'sum': 123.0, 'mean': 4.92}}}},\n",
       "    'correctness': {'scores': {'correctness': {'value': 4.92,\n",
       "       'stats': {'count': 25, 'sum': 123.0, 'mean': 4.92}}}}}}},\n",
       " 'groups': {},\n",
       " 'namespace': 'default',\n",
       " 'custom_fields': {}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get(f\"{NEMO_URL}/v1/evaluation/jobs/{base_eval_job_id}/results\")\n",
    "res.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

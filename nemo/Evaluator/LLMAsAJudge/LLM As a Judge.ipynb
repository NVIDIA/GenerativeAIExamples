{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeMo Evaluator Microservice: Custom LLM-as-a-Judge Eval (Summarization Task)\n",
    "\n",
    "In the following notebook we'll be covering how to create a custom LLM-As-A-Judge evaluation with NVIDIA NeMo Evaluator Microservice (NeMo Evaluator).\n",
    "\n",
    "We'll walk through the required steps of: \n",
    "\n",
    "1. Creating an evaluation target\n",
    "2. Creating an evaluation configuration\n",
    "3. Submitting the Evaluation job\n",
    "4. Collecting results!\n",
    "\n",
    "Let's dive right in!\n",
    "\n",
    "> NOTE: You will need to be in an environment where you have access to a deployed instance of NeMo Evaluator, as well as NVIDIA NeMo Data Store Microservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU requests huggingface_hub==0.26.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator Endpoint\n",
    "\n",
    "Here we just need to capture out Evaluator Endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "EVAL_URL = \"<< YOUR EVALUATOR MS URL >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a health check to confirm we're connected to a working instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'healthy'}\n"
     ]
    }
   ],
   "source": [
    "endpoint = f\"{EVAL_URL}/health\"\n",
    "response = requests.get(endpoint).json()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset in the correct format\n",
    "\n",
    "In this example, we're evaluating the models using a summarization task. So our evaluation dataset needs to reflect this as well as the prompt for our judge model.\n",
    "\n",
    "Look at files in `summarization_bench`:\n",
    "* `question.jsonl` - prompts for the user model (can be multi-turn) and question categories\n",
    "* `reference_answer/reference.jsonl` - corresponding reference answers for prompts listed in `question.jsonl`\n",
    "* `judge_prompts.jsonl` - judge prompt for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to NeMo Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hh\n",
    "import requests\n",
    "\n",
    "DATASTORE_URL = \"<< YOUR DATA STORE URL >>\"\n",
    "\n",
    "token = \"mock\"\n",
    "repo_name = \"nvidia/LLMAsAJudge-Simple\"\n",
    "repo_type = \"dataset\"\n",
    "dir_path = \"./llm_as_a_judge\"\n",
    "\n",
    "hf_api = hh.HfApi(endpoint=DATASTORE_URL, token=token)\n",
    "\n",
    "# create repo\n",
    "hf_api.create_repo(\n",
    "    repo_id=repo_name,\n",
    "    repo_type=repo_type,\n",
    ")\n",
    "\n",
    "# upload dir\n",
    "path_in_repo = \".\"\n",
    "result = hf_api.upload_folder(repo_id=repo_name, folder_path=dir_path, path_in_repo=path_in_repo, repo_type=repo_type)\n",
    "\n",
    "print(f\"Dataset folder uploaded to: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Evaluation Configs - Custom dataset for Summarization Task\n",
    "\n",
    "In order to run a job in NeMo Evaluator - we need two specific things:\n",
    "\n",
    "1. A Target Model - that is, the model that is going to be evaluated.\n",
    "2. A Evaluation Configuration - that is, a configuration that describes our evaluation.\n",
    "\n",
    "With those two objects created - we can run jobs! This will reduce the amount of repetition as we might potentially run a large number of evaluations on a single target model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Model \n",
    "\n",
    "Let's start by creating a new target model - we can do this as easily as pointing to the desired inference URL where the NIM is hosted and providing the appropriate model ID!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_config = {\n",
    "  \"type\": \"model\",\n",
    "  \"model\": {\n",
    "    \"api_endpoint\": {\n",
    "      \"url\": \"<< YOUR NIM INFERENCE URL >>\",\n",
    "      \"model_id\": \"<< YOUR MODEL ID >>\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to point our request at the `v1/evaluation/targets` endpoint to create the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_endpoint = f\"{EVAL_URL}/v1/evaluation/targets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are clear to fire off the request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    target_endpoint,\n",
    "    json=target_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll capture our target ID for the coming steps - but with this step we have created our target and are ready to create an evaluation configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm-judge-job\n"
     ]
    }
   ],
   "source": [
    "target_namespace = response[\"namespace\"]\n",
    "target_name = response[\"name\"]\n",
    "print(f\"Target Namespace: {target_namespace}, Target Name: {target_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Configuration\n",
    "\n",
    "In the following step we'll create an Evaluation Configuration which will describe exactly how, and what, we wish to evaluate our target against.\n",
    "\n",
    "In this example - we'll be using a custom LLM-As-A-Judge evaluation. Let's create that configuration now!\n",
    "\n",
    "Notice how we need to provide a judge model as part of our tasks - this is the model that will be doing the judging of our model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config = {\n",
    " \"type\": \"llm_as_a_judge\",\n",
    " \"tasks\": [\n",
    "   {\n",
    "     \"type\": \"custom\",\n",
    "     \"params\": {\n",
    "       \"judge_model\": {\n",
    "         \"api_endpoint\": {\n",
    "           \"url\": \"<< YOUR JUDGE NIM URL >>\",\n",
    "           \"model_id\": \"<< YOUR JUDGE MODEL NAME >>\"\n",
    "         }\n",
    "       },\n",
    "       \"judge_inference_params\": {\n",
    "         \"top_p\": 1.0e-05,\n",
    "         \"top_k\": 1,\n",
    "         \"temperature\": 1.0e-05,\n",
    "         \"stop\": [],\n",
    "         \"tokens_to_generate\": 512\n",
    "       },\n",
    "       \"top_p\": 0.9,\n",
    "       \"top_k\": 40,\n",
    "       \"temperature\": 0.75,\n",
    "       \"stop\": [],\n",
    "       \"tokens_to_generate\": 512\n",
    "     },\n",
    "     \"dataset\": {\n",
    "       \"files_url\": \"nds:LLMAsAJudge\"\n",
    "     }\n",
    "   }\n",
    " ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can point to our evaluation config endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_endpoint = f\"{EVAL_URL}/v1/evaluation/configs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fire off the request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    eval_config_endpoint,\n",
    "    json=evaluation_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again capture our evaluation config for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval-config-WkskqHD4VeawBTTgQnP2BP\n"
     ]
    }
   ],
   "source": [
    "config_namespace = response[\"namespace\"]\n",
    "config_name = response[\"name\"]\n",
    "print(f\"Config Namespace: {config_namespace}, Config Name: {config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Evaluation Job\n",
    "\n",
    "Now that we have our `target_id` and `config_id` -  we have everything we need to run an evaluation.\n",
    "\n",
    "Let's see the process to create and run a job! \n",
    "\n",
    "First things first, we need to create a job payload to send to our endpoint - this will point to our target, and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = {\n",
    "    \"target\": target_namespace + \"/\" + target_name,\n",
    "    \"config\": config_namespace + \"/\" + config_name,\n",
    "    \"tags\": [\n",
    "        \"custom-llm-as-a-judge\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set the evaluation jobs endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    job_endpoint,\n",
    "    json=job_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    monitoring_endpoint,\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our job status and wait for it to be done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeeded\n"
     ]
    }
   ],
   "source": [
    "print(response[\"status\"][\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe the results of our simple benchmark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'metrics': [{'name': 'average',\n",
       "    'value': '  5.62',\n",
       "    'metadata': {'name': 'mtbench', 'metric_ranking': 0}}],\n",
       "  'level_name': 'evaluation',\n",
       "  'isRecommended': True,\n",
       "  'evaluation_results': [{'metrics': [{'name': 'average',\n",
       "      'value': ' 5.95',\n",
       "      'metadata': {'name': 'helpfulness ', 'metric_ranking': 0}},\n",
       "     {'name': 'average',\n",
       "      'value': ' 5.3',\n",
       "      'metadata': {'name': 'toxicity ', 'metric_ranking': 0}},\n",
       "     {'name': 'average',\n",
       "      'value': '  nan',\n",
       "      'metadata': {'name': 'turn 1', 'metric_ranking': 0}},\n",
       "     {'name': 'average',\n",
       "      'value': '  5.62',\n",
       "      'metadata': {'name': 'turn 2', 'metric_ranking': 0}},\n",
       "     {'name': 'average',\n",
       "      'value': ' 8',\n",
       "      'metadata': {'name': 'number of missing judgements',\n",
       "       'metric_ranking': 0}}],\n",
       "    'level_name': 'task',\n",
       "    'isRecommended': True,\n",
       "    'evaluation_results': None,\n",
       "    'extra_grouping_fields': None}],\n",
       "  'extra_grouping_fields': None}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"results\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia-sdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NeMo Evaluator Microservice: Prompt Optimization with MIPROv2\n",
        "\n",
        "In this notebook, we'll demonstrate how to use NVIDIA NeMo Evaluator Microservice for prompt optimization using MIPROv2 (Multiprompt Instruction PRoposal Optimizer Version 2). This approach uses Bayesian Optimization to improve LLM-as-a-Judge prompts and evaluate their effectiveness.\n",
        "\n",
        "The Judge model we'll be improving today is the NVIDIA Nemotron Nano 9B V2 model, which is a Large Language Model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks.\n",
        "\n",
        "The model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just four Attention layers, making it an effecient and fast model - well suited to this task.\n",
        "\n",
        "We'll walk through the required steps of:\n",
        "\n",
        "1. Setting up the environment and data\n",
        "2. Creating and Submitting the optimization job\n",
        "3. Analyzing results and comparing baseline vs optimized prompts\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "> **NOTE**: You will need access to a deployed instance of NeMo Evaluator and NVIDIA NeMo Data Store Microservice. You can find details [here](https://docs.nvidia.com/nemo/microservices/25.9.0/get-started/setup/minikube/index.html#nemo-ms-get-started-prerequisites) on how to do that!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to use this notebook, you'll want to set-up the virtual environment with `uv`. \n",
        "\n",
        "1. Get `uv` - you can start [here](https://docs.astral.sh/uv/getting-started/installation/)\n",
        "2. Run `uv sync` to create the virtual environment.\n",
        "3. Select the newly created virtual environment to use as the kernel in this Jupyter Notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Endpoints\n",
        "\n",
        "Set up your Evaluator and Data Store endpoints:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Configure your endpoints\n",
        "EVAL_URL = \"<< YOUR NEMO MICROSERVICE ENDPOINTS HERE >>\"\n",
        "DATASTORE_URL = \"<< YOUR NEMO DATA STORE ENDPOINTS HERE >>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Health Check\n",
        "\n",
        "Verify connectivity to the Evaluator service:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examine Dataset Format\n",
        "\n",
        "Let's examine the HelpSteer2 dataset format to understand the structure for prompt optimization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Structure:\n",
            "\n",
            "Example 1:\n",
            "Keys: ['prompt', 'response', 'reference_helpfulness']\n",
            "Prompt: c#...\n",
            "Response: C# is a high-level, object-oriented programming language developed by Microsoft as part of its .NET ...\n",
            "Reference Helpfulness: 3\n"
          ]
        }
      ],
      "source": [
        "# Read and examine the dataset\n",
        "import json\n",
        "\n",
        "dataset_path = \"./data/hs2.jsonl\"\n",
        "\n",
        "# Load first few examples to understand the format\n",
        "examples = []\n",
        "with open(dataset_path, 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 5:  # Just show first 5 examples\n",
        "            break\n",
        "        examples.append(json.loads(line))\n",
        "\n",
        "print(\"Dataset Structure:\")\n",
        "for i, example in enumerate(examples):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Keys: {list(example.keys())}\")\n",
        "    print(f\"Prompt: {example['prompt'][:100]}...\")\n",
        "    print(f\"Response: {example['response'][:100]}...\")\n",
        "    print(f\"Reference Helpfulness: {example['reference_helpfulness']}\")\n",
        "    break  # Show detailed view of first example only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Dataset to NeMo Data Store\n",
        "\n",
        "Upload the dataset to the NeMo Data Store for use in prompt optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15175dcea8f04849bad50ef1c0174443",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "hs2.jsonl:   0%|          | 0.00/199k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset uploaded: https://datastore.aire.nvidia.com/v1/hf/datasets/llm-judge/hs2-short/blob/main/hs2.jsonl\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "HF_ENDPOINT = f\"{DATASTORE_URL}/v1/hf\"\n",
        "NAMESPACE = \"llm-judge\"\n",
        "DATASET_NAME = \"hs2-short\"\n",
        "\n",
        "hf_api = HfApi(endpoint=HF_ENDPOINT, token=\"mock\")\n",
        "repo_id = f\"{NAMESPACE}/{DATASET_NAME}\"\n",
        "\n",
        "# Create the dataset repo if it doesn't exist\n",
        "hf_api.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True)\n",
        "\n",
        "# Upload the file\n",
        "dataset_url = hf_api.upload_file(\n",
        "    path_or_fileobj=\"./data/hs2.jsonl\",\n",
        "    path_in_repo=\"hs2.jsonl\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"dataset\",\n",
        "    revision=\"main\",\n",
        "    commit_message=f\"Eval dataset in {repo_id}\"\n",
        ")\n",
        "\n",
        "print(f\"Dataset uploaded: {dataset_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Prompt Optimization with MIPROv2 through an Inline Job!\n",
        "\n",
        "Now we'll set up the prompt optimization configuration using MIPROv2. This includes:\n",
        "\n",
        "- **Initial instruction**: The baseline prompt to optimize\n",
        "- **Signature**: Defines the input/output structure matching our dataset\n",
        "- **Metrics**: How to evaluate prompt performance\n",
        "- **Optimization parameters**: Control the optimization process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Target Configuration\n",
        "\n",
        "Our target configuration tells NeMo Evaluator Microservice what model is the target for our evaluation. \n",
        "\n",
        "Let's break down the key components:\n",
        "\n",
        "- **API Endpoint**: `model_id`, `url`, and `api_key` in this example point at a remote hosted model (in this case, hosted on [OpenRouter](https://openrouter.ai/nvidia/nemotron-nano-9b-v2)). You can substitute any OpenAI API compatible endpoint here - including, of course, the [NVIDIA Nemotron Nano 9B V2 NIM](https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2/deploy)!\n",
        "\n",
        "> NOTE: You can find your OpenRouter API Key through [this process](https://openrouter.ai/docs/api-reference/authentication)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENROUTER_API_KEY\"]  = getpass.getpass(\"Enter your OpenRouter API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration Explanation\n",
        "\n",
        "Let's break down the key components:\n",
        "\n",
        "- **Signature**: `\"prompt, response, reference_helpfulness -> helpfulness\"` matches our dataset structure\n",
        "- **Initial instruction**: A baseline prompt for evaluating helpfulness\n",
        "- **MIPROv2 parameters**:\n",
        "  - `auto: \"light\"` - Light optimization intensity\n",
        "  - `max_bootstrapped_demos: 2` - Generate up to 2 examples\n",
        "  - `max_labeled_demos: 2` - Use up to 2 examples from training set\n",
        "- **Metric**: Number-check with epsilon=1 allows scores within 1 point to be considered correct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submit Prompt Optimization Job\n",
        "\n",
        "Now we'll create and submit the optimization job in a single config!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create job configuration\n",
        "job_config = {\n",
        "  \"target\": {\n",
        "    \"type\": \"model\",\n",
        "    \"model\": {\n",
        "      \"api_endpoint\": {\n",
        "        \"model_id\": \"nvidia/nemotron-nano-9b-v2\",\n",
        "        \"url\": \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        \"api_key\": os.environ[\"OPENROUTER_API_KEY\"]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"config\": {\n",
        "    \"type\": \"custom\",\n",
        "    \"tasks\": {\n",
        "      \"helpfulness-prompt-optimization\": {\n",
        "        \"type\": \"prompt-optimization\",\n",
        "        \"params\": {\n",
        "          \"optimizer\": {\n",
        "            \"type\": \"miprov2\",\n",
        "            \"instruction\": \"Your task is to evaluate the helpfulness of a response to a given prompt on a scale of 0-4. Output ONLY a single digit (0, 1, 2, 3, or 4) with no additional text.\",\n",
        "            \"signature\": \"prompt, response -> reference_helpfulness: int\",\n",
        "            \"auto\": None,\n",
        "            \"num_trials\": 1,\n",
        "            \"num_candidates\": 1,\n",
        "            \"max_bootstrapped_demos\": 0,\n",
        "            \"max_labeled_demos\": 0,\n",
        "            \"minibatch_size\": 2\n",
        "          }\n",
        "        },\n",
        "        \"metrics\": {\n",
        "          \"number-check\": {\n",
        "            \"type\": \"number-check\",\n",
        "            \"params\": {\n",
        "              \"check\": [\n",
        "                \"absolute difference\",\n",
        "                \"{{item.reference_helpfulness | trim}}\",\n",
        "                \"{{reference_helpfulness | trim}}\",\n",
        "                \"epsilon\",\n",
        "                1\n",
        "              ]\n",
        "            }\n",
        "          }\n",
        "        },\n",
        "        \"dataset\": {\n",
        "          \"files_url\": f\"hf://datasets/{NAMESPACE}/{DATASET_NAME}\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we can submit the job to the `v1/evaluation/jobs` endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'created_at': '2025-09-17T21:27:03.479720', 'updated_at': '2025-09-17T21:27:03.479721', 'id': 'eval-GTtMA2M9SBYGSouHtgv2Cs', 'namespace': 'default', 'description': None, 'target': {'schema_version': '1.0', 'id': 'eval-target-FPZPXByTCnzhneCCTjwNSf', 'description': None, 'type_prefix': 'eval-target', 'namespace': 'default', 'project': None, 'created_at': '2025-09-17T21:27:03.479166', 'updated_at': '2025-09-17T21:27:03.479166', 'custom_fields': {}, 'ownership': None, 'name': 'eval-target-FPZPXByTCnzhneCCTjwNSf', 'type': 'model', 'cached_outputs': None, 'model': {'schema_version': '1.0', 'id': 'model-XLdAWScdr44np3Z7UFygpg', 'description': None, 'type_prefix': 'model', 'namespace': 'default', 'project': None, 'created_at': '2025-09-17T21:27:03.479192', 'updated_at': '2025-09-17T21:27:03.479193', 'custom_fields': {}, 'ownership': None, 'name': 'model-XLdAWScdr44np3Z7UFygpg', 'version_id': 'main', 'version_tags': [], 'spec': None, 'artifact': None, 'base_model': None, 'api_endpoint': {'url': 'https://openrouter.ai/api/v1/chat/completions', 'model_id': 'nvidia/nemotron-nano-9b-v2', 'api_key': '******', 'format': 'nim'}, 'peft': None, 'prompt': None, 'guardrails': None}, 'retriever': None, 'rag': None, 'rows': None, 'dataset': None}, 'config': {'schema_version': '1.0', 'id': 'eval-config-WPVtgfv1aYgutLkUfViUVN', 'description': None, 'type_prefix': 'eval-config', 'namespace': 'default', 'project': None, 'created_at': '2025-09-17T21:27:03.479028', 'updated_at': '2025-09-17T21:27:03.479032', 'custom_fields': {}, 'ownership': None, 'name': 'eval-config-WPVtgfv1aYgutLkUfViUVN', 'type': 'custom', 'params': None, 'tasks': {'helpfulness-prompt-optimization': {'type': 'prompt-optimization', 'params': {'optimizer': {'type': 'miprov2', 'instruction': 'Your task is to evaluate the helpfulness of a response to a given prompt on a scale of 0-4. Output ONLY a single digit (0, 1, 2, 3, or 4) with no additional text.', 'signature': 'prompt, response -> reference_helpfulness: int', 'auto': None, 'num_trials': 1, 'num_candidates': 1, 'max_bootstrapped_demos': 0, 'max_labeled_demos': 0, 'minibatch_size': 2}}, 'metrics': {'number-check': {'type': 'number-check', 'params': {'check': ['absolute difference', '{{item.reference_helpfulness | trim}}', '{{reference_helpfulness | trim}}', 'epsilon', 1]}}}, 'dataset': {'schema_version': '1.0', 'id': 'dataset-Km4n5tApgqZY46wnPeWGfm', 'description': None, 'type_prefix': None, 'namespace': 'default', 'project': None, 'created_at': '2025-09-17T21:27:03.479101', 'updated_at': '2025-09-17T21:27:03.479101', 'custom_fields': {}, 'ownership': None, 'name': 'dataset-Km4n5tApgqZY46wnPeWGfm', 'version_id': 'main', 'version_tags': [], 'format': None, 'files_url': 'hf://datasets/llm-judge/hs2-short', 'hf_endpoint': None, 'split': None, 'limit': None}}}, 'groups': None}, 'result': None, 'output_files_url': None, 'status_details': {'message': None, 'task_status': {}, 'progress': None, 'samples_processed': None}, 'status': 'created', 'project': None, 'custom_fields': {}, 'ownership': None}\n",
            "Prompt Optimization Job Submitted!\n",
            "Job ID: eval-GTtMA2M9SBYGSouHtgv2Cs\n",
            "\n",
            "Job Details:\n",
            "{\n",
            "  \"created_at\": \"2025-09-17T21:27:03.479720\",\n",
            "  \"updated_at\": \"2025-09-17T21:27:03.479721\",\n",
            "  \"id\": \"eval-GTtMA2M9SBYGSouHtgv2Cs\",\n",
            "  \"namespace\": \"default\",\n",
            "  \"description\": null,\n",
            "  \"target\": {\n",
            "    \"schema_version\": \"1.0\",\n",
            "    \"id\": \"eval-target-FPZPXByTCnzhneCCTjwNSf\",\n",
            "    \"description\": null,\n",
            "    \"type_prefix\": \"eval-target\",\n",
            "    \"namespace\": \"default\",\n",
            "    \"project\": null,\n",
            "    \"created_at\": \"2025-09-17T21:27:03.479166\",\n",
            "    \"updated_at\": \"2025-09-17T21:27:03.479166\",\n",
            "    \"custom_fields\": {},\n",
            "    \"ownership\": null,\n",
            "    \"name\": \"eval-target-FPZPXByTCnzhneCCTjwNSf\",\n",
            "    \"type\": \"model\",\n",
            "    \"cached_outputs\": null,\n",
            "    \"model\": {\n",
            "      \"schema_version\": \"1.0\",\n",
            "      \"id\": \"model-XLdAWScdr44np3Z7UFygpg\",\n",
            "      \"description\": null,\n",
            "      \"type_prefix\": \"model\",\n",
            "      \"namespace\": \"default\",\n",
            "      \"project\": null,\n",
            "      \"created_at\": \"2025-09-17T21:27:03.479192\",\n",
            "      \"updated_at\": \"2025-09-17T21:27:03.479193\",\n",
            "      \"custom_fields\": {},\n",
            "      \"ownership\": null,\n",
            "      \"name\": \"model-XLdAWScdr44np3Z7UFygpg\",\n",
            "      \"version_id\": \"main\",\n",
            "      \"version_tags\": [],\n",
            "      \"spec\": null,\n",
            "      \"artifact\": null,\n",
            "      \"base_model\": null,\n",
            "      \"api_endpoint\": {\n",
            "        \"url\": \"https://openrouter.ai/api/v1/chat/completions\",\n",
            "        \"model_id\": \"nvidia/nemotron-nano-9b-v2\",\n",
            "        \"api_key\": \"******\",\n",
            "        \"format\": \"nim\"\n",
            "      },\n",
            "      \"peft\": null,\n",
            "      \"prompt\": null,\n",
            "      \"guardrails\": null\n",
            "    },\n",
            "    \"retriever\": null,\n",
            "    \"rag\": null,\n",
            "    \"rows\": null,\n",
            "    \"dataset\": null\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"schema_version\": \"1.0\",\n",
            "    \"id\": \"eval-config-WPVtgfv1aYgutLkUfViUVN\",\n",
            "    \"description\": null,\n",
            "    \"type_prefix\": \"eval-config\",\n",
            "    \"namespace\": \"default\",\n",
            "    \"project\": null,\n",
            "    \"created_at\": \"2025-09-17T21:27:03.479028\",\n",
            "    \"updated_at\": \"2025-09-17T21:27:03.479032\",\n",
            "    \"custom_fields\": {},\n",
            "    \"ownership\": null,\n",
            "    \"name\": \"eval-config-WPVtgfv1aYgutLkUfViUVN\",\n",
            "    \"type\": \"custom\",\n",
            "    \"params\": null,\n",
            "    \"tasks\": {\n",
            "      \"helpfulness-prompt-optimization\": {\n",
            "        \"type\": \"prompt-optimization\",\n",
            "        \"params\": {\n",
            "          \"optimizer\": {\n",
            "            \"type\": \"miprov2\",\n",
            "            \"instruction\": \"Your task is to evaluate the helpfulness of a response to a given prompt on a scale of 0-4. Output ONLY a single digit (0, 1, 2, 3, or 4) with no additional text.\",\n",
            "            \"signature\": \"prompt, response -> reference_helpfulness: int\",\n",
            "            \"auto\": null,\n",
            "            \"num_trials\": 1,\n",
            "            \"num_candidates\": 1,\n",
            "            \"max_bootstrapped_demos\": 0,\n",
            "            \"max_labeled_demos\": 0,\n",
            "            \"minibatch_size\": 2\n",
            "          }\n",
            "        },\n",
            "        \"metrics\": {\n",
            "          \"number-check\": {\n",
            "            \"type\": \"number-check\",\n",
            "            \"params\": {\n",
            "              \"check\": [\n",
            "                \"absolute difference\",\n",
            "                \"{{item.reference_helpfulness | trim}}\",\n",
            "                \"{{reference_helpfulness | trim}}\",\n",
            "                \"epsilon\",\n",
            "                1\n",
            "              ]\n",
            "            }\n",
            "          }\n",
            "        },\n",
            "        \"dataset\": {\n",
            "          \"schema_version\": \"1.0\",\n",
            "          \"id\": \"dataset-Km4n5tApgqZY46wnPeWGfm\",\n",
            "          \"description\": null,\n",
            "          \"type_prefix\": null,\n",
            "          \"namespace\": \"default\",\n",
            "          \"project\": null,\n",
            "          \"created_at\": \"2025-09-17T21:27:03.479101\",\n",
            "          \"updated_at\": \"2025-09-17T21:27:03.479101\",\n",
            "          \"custom_fields\": {},\n",
            "          \"ownership\": null,\n",
            "          \"name\": \"dataset-Km4n5tApgqZY46wnPeWGfm\",\n",
            "          \"version_id\": \"main\",\n",
            "          \"version_tags\": [],\n",
            "          \"format\": null,\n",
            "          \"files_url\": \"hf://datasets/llm-judge/hs2-short\",\n",
            "          \"hf_endpoint\": null,\n",
            "          \"split\": null,\n",
            "          \"limit\": null\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    \"groups\": null\n",
            "  },\n",
            "  \"result\": null,\n",
            "  \"output_files_url\": null,\n",
            "  \"status_details\": {\n",
            "    \"message\": null,\n",
            "    \"task_status\": {},\n",
            "    \"progress\": null,\n",
            "    \"samples_processed\": null\n",
            "  },\n",
            "  \"status\": \"created\",\n",
            "  \"project\": null,\n",
            "  \"custom_fields\": {},\n",
            "  \"ownership\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Submit the job\n",
        "job_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs\"\n",
        "\n",
        "response = requests.post(\n",
        "    job_endpoint,\n",
        "    json=job_config,\n",
        "    headers={'accept': 'application/json'}\n",
        ").json()\n",
        "\n",
        "print(response)\n",
        "\n",
        "job_id = response[\"id\"]\n",
        "print(f\"Prompt Optimization Job Submitted!\")\n",
        "print(f\"Job ID: {job_id}\")\n",
        "print(f\"\\nJob Details:\")\n",
        "print(json.dumps(response, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor Job Progress\n",
        "\n",
        "Let's monitor the optimization job status. Prompt optimization can take some time as it involves multiple optimization trials:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Status: created\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def check_job_status(job_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"Check the status of an evaluation job.\"\"\"\n",
        "    monitoring_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs/{job_id}\"\n",
        "    response = requests.get(monitoring_endpoint).json()\n",
        "    return response\n",
        "\n",
        "def wait_for_completion(job_ids: list, check_interval: int = 30) -> Dict[str, Any]:\n",
        "    \"\"\"Wait for job completion with periodic status updates.\"\"\"\n",
        "    print(f\"Monitoring jobs {job_ids}...\")\n",
        "    \n",
        "    while True:\n",
        "        status_response = check_job_status(job_id)\n",
        "        status = status_response[\"status\"]\n",
        "    \n",
        "        print(f\"\\nJob Status {job_id}: {status}\")\n",
        "    \n",
        "        if status in [\"completed\", \"failed\", \"cancelled\"]:\n",
        "            print(f\"\\nJob completed with status: {status}\")\n",
        "            return status_response\n",
        "        \n",
        "        print(f\"Waiting {check_interval} seconds before next check...\")\n",
        "        time.sleep(check_interval)\n",
        "\n",
        "# Check current status\n",
        "current_status = check_job_status(job_id)\n",
        "print(f\"Current Status: {current_status['status']}\")\n",
        "if current_status['status'] == 'failed':\n",
        "    print(f\"Job failed with status: {current_status['status']}\")\n",
        "    print(f\"Job details: {current_status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> NOTE: At this time - `progress` is not captured during the running job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Monitoring jobs ['eval-GTtMA2M9SBYGSouHtgv2Cs', 'eval-YbisDgYUhnCHY9BFsHfm5b']...\n",
            "\n",
            "Job Status eval-GTtMA2M9SBYGSouHtgv2Cs: running\n",
            "\n",
            "Job Status eval-YbisDgYUhnCHY9BFsHfm5b: completed\n",
            "\n",
            "Job completed with status: completed\n",
            "{'created_at': '2025-09-17T20:42:07.698905', 'updated_at': '2025-09-17T21:26:53.154123', 'id': 'eval-YbisDgYUhnCHY9BFsHfm5b', 'namespace': 'default', 'description': None, 'target': {'schema_version': '1.0', 'id': 'eval-target-9ipuYnMsyznDb7zUiZ7muC', 'description': None, 'type_prefix': 'eval-target', 'namespace': 'default', 'project': None, 'created_at': '2025-09-17T20:42:07.698478', 'updated_at': '2025-09-17T20:42:07.698478', 'custom_fields': {}, 'ownership': None, 'name': 'eval-target-9ipuYnMsyznDb7zUiZ7muC', 'type': 'model', 'cached_outputs': None, 'model': {'schema_version': '1.0', 'id': 'model-KrjaH39DaptdzyusEdA2cu', 'description': None, 'type_prefix': 'model', 'namespace': 'default', 'project': None, 'created_at': '2025-09-17T20:42:07.698498', 'updated_at': '2025-09-17T20:42:07.698499', 'custom_fields': {}, 'ownership': None, 'name': 'model-KrjaH39DaptdzyusEdA2cu', 'version_id': 'main', 'version_tags': [], 'spec': None, 'artifact': None, 'base_model': None, 'api_endpoint': {'url': 'https://openrouter.ai/api/v1/chat/completions', 'model_id': 'nvidia/nemotron-nano-9b-v2', 'api_key': '******', 'format': 'nim'}, 'peft': None, 'prompt': None, 'guardrails': None}, 'retriever': None, 'rag': None, 'rows': None, 'dataset': None}, 'config': {'schema_version': '1.0', 'id': 'eval-config-41bbjsnm5KxtcDU8GzPrGh', 'description': None, 'type_prefix': 'eval-config', 'namespace': 'default', 'project': None, 'created_at': '2025-09-17T20:42:07.698371', 'updated_at': '2025-09-17T20:42:07.698373', 'custom_fields': {}, 'ownership': None, 'name': 'eval-config-41bbjsnm5KxtcDU8GzPrGh', 'type': 'custom', 'params': None, 'tasks': {'helpfulness-prompt-optimization': {'type': 'prompt-optimization', 'params': {'optimizer': {'type': 'miprov2', 'instruction': 'Your task is to evaluate the helpfulness of a response to a given prompt on a scale of 0-4. Output ONLY a single digit (0, 1, 2, 3, or 4) with no additional text.', 'signature': 'prompt, response -> reference_helpfulness: int', 'auto': 'light', 'max_bootstrapped_demos': 2, 'max_labeled_demos': 2, 'seed': 42}}, 'metrics': {'number-check': {'type': 'number-check', 'params': {'check': ['absolute difference', '{{item.reference_helpfulness | trim}}', '{{reference_helpfulness | trim}}', 'epsilon', 1]}}}, 'dataset': {'schema_version': '1.0', 'id': 'dataset-DgnUzaiYcc98z7h1UphU3L', 'description': None, 'type_prefix': None, 'namespace': 'default', 'project': None, 'created_at': '2025-09-17T20:42:07.698427', 'updated_at': '2025-09-17T20:42:07.698427', 'custom_fields': {}, 'ownership': None, 'name': 'dataset-DgnUzaiYcc98z7h1UphU3L', 'version_id': 'main', 'version_tags': [], 'format': None, 'files_url': 'hf://datasets/llm-judge/hs2-short', 'hf_endpoint': None, 'split': None, 'limit': None}}}, 'groups': None}, 'result': 'evaluation_result-WPdqDPDuyLdsqz1oZPrdB6', 'output_files_url': 'hf://datasets/evaluation-results/eval-YbisDgYUhnCHY9BFsHfm5b', 'status_details': {'message': 'Job completed successfully.', 'task_status': {'helpfulness-prompt-optimization': 'completed'}, 'progress': 100.0, 'samples_processed': 794}, 'status': 'completed', 'project': None, 'custom_fields': {}, 'ownership': None}\n"
          ]
        }
      ],
      "source": [
        "wait_for_completion([job_id])\n",
        "final_status = check_job_status(job_id)\n",
        "print(final_status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Optimization Results\n",
        "\n",
        "Once the job completes, let's examine the results to see how the prompt was optimized:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'created_at': '2025-09-17T20:42:17.192537',\n",
              " 'updated_at': '2025-09-17T21:26:53.110771',\n",
              " 'id': 'evaluation_result-WPdqDPDuyLdsqz1oZPrdB6',\n",
              " 'job': 'eval-YbisDgYUhnCHY9BFsHfm5b',\n",
              " 'files_url': 'hf://datasets/evaluation-results/eval-YbisDgYUhnCHY9BFsHfm5b',\n",
              " 'tasks': {'helpfulness-prompt-optimization': {'metrics': {'number-check': {'scores': {'baseline': {'value': 0.8118000000000001,\n",
              "       'stats': {'count': 85}},\n",
              "      'optimized': {'value': 0.8941, 'stats': {'count': 85}}}}},\n",
              "   'data': {'baseline_prompt': 'Your task is to evaluate the helpfulness of a response to a given prompt on a scale of 0-4. Output ONLY a single digit (0, 1, 2, 3, or 4) with no additional text.',\n",
              "    'optimized_prompt': 'Evaluate the helpfulness of the response to the given prompt by systematically analyzing its relevance, clarity, completeness, and alignment with the prompt\\'s requirements. Assign a score from 0-4 based on these criteria, ensuring the output is strictly a single digit (0, 1, 2, 3, or 4) with no additional text. Prioritize technical precision and user-centricity in your assessment.\\n{\"augmented\": true, \"prompt\": \"c#\", \"response\": \"C# is a high-level, object-oriented programming language developed by Microsoft as part of its .NET initiative. It was created as a modern alternative to Java and supports a variety of programming paradigms, including imperative, functional, and event-driven. C# is primarily used for Windows application development, but it can also be used for web, mobile, and game development. The language is designed to be safe, secure, and efficient, and it provides developers with a rich set of libraries and tools for building robust and scalable applications. C# is also widely used in the game development industry, particularly in the development of games for the Xbox 360 and Xbox One consoles.\", \"reasoning\": \"The response provides a general overview of C#, covering its development, features, and use cases. However, the prompt \\\\\"c#\\\\\" is extremely vague, and the response does not address any specific question or context. While it is informative for a broad introduction, it lacks depth or actionable details that would make it highly helpful for a specific query.\", \"reference_helpfulness\": 3}\\n{\"prompt\": \"I would like you to audit content for me based on a URL I specify below. I would like you to audit content as if you were a Google Quality Rater following the rules set out by Google (which you can see here (https://developers.google.com/search/blog/2022/08/helpful-content-update)in respect of August 2022 helpful content update (experience, expertise, authority and trust) - I would also like you to consider YMYL (your money your life where applicable) and Google medic factors also depending on the content type and nature. I would like you to provide a content quality rating based on a scale of 1 to 10 where 10 is best and 0 is worst. You should take into consideration - how well the content is written, how well it aligns with Google\\'s August 2022 helpful content update guidelines for human quality raters, how well structured the content is, if it makes it clear what is on offer, is it gramatically correct and well written and does it fit the end users intent when comparing the main H1 tag to the body of the content. You should provide clear, actionable recommendations for any areas where the content has an issue as well as guidance to bolster expertise and trust where applicable. You should not self reference and should avoid making any assumptions, the content for you to audit can be found here: \\\\n\\\\n\\\\n\\\\n\\\\nhttps://redblink.com/top-ai-content-detector-tools/\", \"response\": \"Sure, I can help you with that. I\\'ll need access to the URL you specified, so please provide it to me.\", \"reference_helpfulness\": 0}'}}},\n",
              " 'groups': {},\n",
              " 'namespace': 'default',\n",
              " 'custom_fields': {}}"
            ]
          },
          "execution_count": 269,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "endpoint = f\"{EVAL_URL}/v1/evaluation/jobs/{job_id_1}/results\"\n",
        "final_results = requests.get(endpoint).json()\n",
        "final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PROMPT OPTIMIZATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "📊 PERFORMANCE METRICS:\n",
            "----------------------------------------\n",
            "Baseline Accuracy:  0.8118 (n=85)\n",
            "Optimized Accuracy: 0.8941 (n=85)\n",
            "Improvement:        +0.0823 (+10.14%)\n",
            "\n",
            "📝 PROMPT COMPARISON:\n",
            "----------------------------------------\n",
            "\n",
            "🔸 BASELINE PROMPT:\n",
            "\"Your task is to evaluate the helpfulness of a response to a given prompt on a scale of 0-4. Output ONLY a single digit (0, 1, 2, 3, or 4) with no additional text.\"\n",
            "\n",
            "🔹 OPTIMIZED PROMPT:\n",
            "\"Evaluate the helpfulness of the response to the given prompt by systematically analyzing its relevance, clarity, completeness, and alignment with the prompt's requirements. Assign a score from 0-4 based on these criteria, ensuring the output is strictly a single digit (0, 1, 2, 3, or 4) with no additional text. Prioritize technical precision and user-centricity in your assessment.\n",
            "{\"augmented\": true, \"prompt\": \"c#\", \"response\": \"C# is a high-level, object-oriented programming language developed by Microsoft as part of its .NET initiative. It was created as a modern alternative to Java and supports a variety of programming paradigms, including imperative, functional, and event-driven. C# is primarily used for Windows application development, but it can also be used for web, mobile, and game development. The language is designed to be safe, secure, and efficient, and it provides developers with a rich set of libraries and tools for building robust and scalable applications. C# is also widely used in the game development industry, particularly in the development of games for the Xbox 360 and Xbox One consoles.\", \"reasoning\": \"The response provides a general overview of C#, covering its development, features, and use cases. However, the prompt \\\"c#\\\" is extremely vague, and the response does not address any specific question or context. While it is informative for a broad introduction, it lacks depth or actionable details that would make it highly helpful for a specific query.\", \"reference_helpfulness\": 3}\n",
            "{\"prompt\": \"I would like you to audit content for me based on a URL I specify below. I would like you to audit content as if you were a Google Quality Rater following the rules set out by Google (which you can see here (https://developers.google.com/search/blog/2022/08/helpful-content-update)in respect of August 2022 helpful content update (experience, expertise, authority and trust) - I would also like you to consider YMYL (your money your life where applicable) and Google medic factors also depending on the content type and nature. I would like you to provide a content quality rating based on a scale of 1 to 10 where 10 is best and 0 is worst. You should take into consideration - how well the content is written, how well it aligns with Google's August 2022 helpful content update guidelines for human quality raters, how well structured the content is, if it makes it clear what is on offer, is it gramatically correct and well written and does it fit the end users intent when comparing the main H1 tag to the body of the content. You should provide clear, actionable recommendations for any areas where the content has an issue as well as guidance to bolster expertise and trust where applicable. You should not self reference and should avoid making any assumptions, the content for you to audit can be found here: \\n\\n\\n\\n\\nhttps://redblink.com/top-ai-content-detector-tools/\", \"response\": \"Sure, I can help you with that. I'll need access to the URL you specified, so please provide it to me.\", \"reference_helpfulness\": 0}\"\n",
            "\n",
            "📋 JOB METADATA:\n",
            "----------------------------------------\n",
            "Job ID:        eval-YbisDgYUhnCHY9BFsHfm5b\n",
            "Created:       2025-09-17T20:42:17.192537\n",
            "Updated:       2025-09-17T21:26:53.110771\n",
            "Files URL:     hf://datasets/evaluation-results/eval-YbisDgYUhnCHY9BFsHfm5b\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def analyze_optimization_results(job_response: Dict[str, Any]):\n",
        "    \"\"\"Analyze and display prompt optimization results.\"\"\"\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"PROMPT OPTIMIZATION RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Navigate to the helpfulness task results\n",
        "    tasks = job_response.get(\"tasks\", {})\n",
        "    helpfulness_task = tasks.get(\"helpfulness-prompt-optimization\", {})\n",
        "    \n",
        "    # Display metrics comparison\n",
        "    metrics = helpfulness_task.get(\"metrics\", {})\n",
        "    if metrics and \"number-check\" in metrics:\n",
        "        scores = metrics[\"number-check\"].get(\"scores\", {})\n",
        "        \n",
        "        print(\"\\n📊 PERFORMANCE METRICS:\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        baseline_score = None\n",
        "        optimized_score = None\n",
        "        \n",
        "        if \"baseline\" in scores:\n",
        "            baseline_data = scores[\"baseline\"]\n",
        "            baseline_score = baseline_data.get(\"value\", 0)\n",
        "            baseline_count = baseline_data.get(\"stats\", {}).get(\"count\", 0)\n",
        "            print(f\"Baseline Accuracy:  {baseline_score:.4f} (n={baseline_count})\")\n",
        "        \n",
        "        if \"optimized\" in scores:\n",
        "            optimized_data = scores[\"optimized\"]\n",
        "            optimized_score = optimized_data.get(\"value\", 0)\n",
        "            optimized_count = optimized_data.get(\"stats\", {}).get(\"count\", 0)\n",
        "            print(f\"Optimized Accuracy: {optimized_score:.4f} (n={optimized_count})\")\n",
        "        \n",
        "        if baseline_score is not None and optimized_score is not None:\n",
        "            improvement = optimized_score - baseline_score\n",
        "            improvement_pct = (improvement / baseline_score) * 100 if baseline_score > 0 else 0\n",
        "            print(f\"Improvement:        {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
        "    \n",
        "    # Display prompts\n",
        "    data = helpfulness_task.get(\"data\", {})\n",
        "    if data:\n",
        "        print(\"\\n📝 PROMPT COMPARISON:\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        if \"baseline_prompt\" in data:\n",
        "            baseline_prompt = data[\"baseline_prompt\"]\n",
        "            print(\"\\n🔸 BASELINE PROMPT:\")\n",
        "            print(f'\"{baseline_prompt}\"')\n",
        "        \n",
        "        if \"optimized_prompt\" in data:\n",
        "            optimized_prompt = data[\"optimized_prompt\"]\n",
        "            print(\"\\n🔹 OPTIMIZED PROMPT:\")\n",
        "            print(f'\"{optimized_prompt}\"')\n",
        "            \n",
        "            # Check if prompts are identical\n",
        "            if baseline_prompt == optimized_prompt:\n",
        "                print(\"\\n⚠️  Note: The optimized prompt is identical to the baseline prompt.\")\n",
        "                print(\"   This suggests the optimization process found the original prompt\")\n",
        "                print(\"   was already optimal for the given task.\")\n",
        "    \n",
        "    # Display additional metadata\n",
        "    print(\"\\n📋 JOB METADATA:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Job ID:        {job_response.get('job', 'N/A')}\")\n",
        "    print(f\"Created:       {job_response.get('created_at', 'N/A')}\")\n",
        "    print(f\"Updated:       {job_response.get('updated_at', 'N/A')}\")\n",
        "    print(f\"Files URL:     {job_response.get('files_url', 'N/A')}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    \n",
        "    return helpfulness_task\n",
        "\n",
        "# Analyze the results\n",
        "if final_status[\"status\"] == \"completed\":\n",
        "    optimization_results = analyze_optimization_results(final_results)\n",
        "else:\n",
        "    print(f\"Job status: {final_status['status']}\")\n",
        "    print(\"Please wait for job completion before analyzing results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Results\n",
        "\n",
        "The optimization results provide several key insights:\n",
        "\n",
        "### Metrics\n",
        "- **Baseline Accuracy**: Performance of the original prompt\n",
        "- **Optimized Accuracy**: Performance of the MIPROv2-optimized prompt\n",
        "- **Improvement**: Quantified improvement in evaluation accuracy\n",
        "\n",
        "### Prompts\n",
        "- **Baseline Prompt**: Your original instruction\n",
        "- **Optimized Prompt**: The improved prompt generated by MIPROv2, which may include:\n",
        "  - Refined instructions\n",
        "  - Few-shot examples\n",
        "  - Better task framing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we've demonstrated how to:\n",
        "\n",
        "1. **Set up prompt optimization** with MIPROv2 using NeMo Evaluator\n",
        "2. **Configure the optimization task** with proper signature and metrics\n",
        "3. **Submit and monitor** optimization jobs\n",
        "4. **Analyze results** to understand prompt improvements\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- **MIPROv2** uses Bayesian Optimization to systematically improve prompts\n",
        "- **Signature definition** must match your dataset structure exactly\n",
        "- **Metric configuration** determines how optimization success is measured\n",
        "- **Optimization intensity** (`auto`: light/medium/heavy) controls compute vs. quality tradeoff\n",
        "- **Results provide both quantitative metrics and the actual optimized prompts**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

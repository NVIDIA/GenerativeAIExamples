{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeMo Evaluator microservice: Retriever and RAG Evaluation\n",
    "\n",
    "In the following notebook, we'll be exploring how to use [NeMo Evaluator microservice](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/overview.html) to evaluate [Retriever Models](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/models/models_retriever.html) as well as [Retrieval Augmented Generation (RAG) Models](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/models/models_rag.html)!\n",
    "\n",
    "We'll look at the following examples: \n",
    "\n",
    "- Retriever Model Evaluation on FiQA\n",
    "- Retriever + Reranking Evaluation on FiQA\n",
    "- Retrieval Augmented Generation (RAG) Evaluation on FiQA with Ragas Metrics\n",
    "- Retrieval Augmented Generation (RAG) Evaluation on Synthetically Generated Data with Ragas Metrics\n",
    "\n",
    "In order to get started, we'll need to make sure our Evaluation Microservice is running, alongside our Retriever, Re-Rank, and LLM NIMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Set-up and Notebook Dependencies\n",
    "\n",
    "In order to run this notebook, the following will need to be up and running: \n",
    "\n",
    "- Evaluator Microservice, which can be conveniently deployed through the [Deploying with Helm](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/deploy-helm.html) guide\n",
    "- NVIDIA NIM Text Embedding, `nvidia/nv-embedqa-e5-v5`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/getting-started.html) guide\n",
    "- NVIDIA NIM Text Reranking, `nvidia/nv-rerankqa-mistral-4b-v3`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/getting-started.html) guide\n",
    "- NVIDIA NIM for LLM, `meta/llama-3.1-8b-instruct`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html) guide\n",
    "\n",
    "Once all of our services are up and running, we can install the Python `requests` library, which we will use to communicate with the Evaluator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU requests huggingface_hub==0.26.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to provide the Evaluation API URL in the cell below.\n",
    "\n",
    "> NOTE: Your evaluation URL will be provided as part of your deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_URL = \"<< YOUR EVALUATOR URL HERE >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to provide the endpoints for your model addresses and model names, which will be set-up as part of the deployment process for each NIM.\n",
    "\n",
    "Below is an example of the default value for the embedding NIM:\n",
    "\n",
    "- Embedding: \n",
    "  - EMBEDDING_URL: `http://localhost:8000/v1/embeddings`\n",
    "  - EMBEDDING_MODEL_NAME: `nvidia/nv-embedqa-e5-v5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "EMBEDDING_URL = \"<< YOUR EMBEDDING MODEL NIM URL >>\"\n",
    "EMBEDDING_MODEL_NAME = \" << YOUR EMBEDDING MODEL NAME >>\"\n",
    "\n",
    "# reranker\n",
    "RERANKER_URL = \"<< YOUR RERANKER MODEL NIM URL >>\"\n",
    "RERANKER_MODEL_NAME = \"<< YOUR RERANKER MODEL NAME >>\"\n",
    "\n",
    "# llm\n",
    "LLM_URL = \"<< YOUR LLM MODEL NIM URL >>\"\n",
    "LLM_MODEL_NAME = \"<< YOUR LLM MODEL NAME >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify our Evaluation API is up and running with the built-in health check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'healthy'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "endpoint = f\"{EVAL_URL}/health\"\n",
    "response = requests.get(endpoint).json()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Model Evaluation on FiQA\n",
    "\n",
    "For our first evaluation, we're going to evaluate our Retrieval Model (`nvidia/nv-embedqa-e5-v5`) on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "The core pieces we need to provide are: \n",
    "\n",
    "- `top_k`, how many documents to retriever through our retriever model\n",
    "- `query_embedding_url`, the address of your currently running `nvidia/nv-embedqa-e5-v5` NIM if you're following the notebook exactly.\n",
    "- `query_embedding_model`, this will be `nvidia/nv-embedqa-e5-v5` if you're following the notebook exactly.\n",
    "- `index_embedding_url`, which will mirror the `query_embedding_url` assuming that you're using the same NIM deployment for both Query Embedding and Index embedding.\n",
    "- `index_embedding_model`, this will mirror the `query_embedding_model` assuming that you're using the same NIM deployment for both Query Embedding and Index embedding.\n",
    "\n",
    "> NOTE: While it's possible to use different NIM *deployments* for Query/Index Embedding - you will need to ensure the underlying model is the same between both.\n",
    "\n",
    "We'll also want to ensure we've set-up our evaluations correctly by following the available [documentation](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/evaluations/evaluations_retriever.html) for Retriever evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_target_config = {\n",
    " \"type\": \"retriever\",\n",
    " \"retriever\": {\n",
    "   \"pipeline\": {\n",
    "     \"query_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": EMBEDDING_URL,\n",
    "         \"model_id\": EMBEDDING_MODEL_NAME\n",
    "       }\n",
    "     },\n",
    "     \"index_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": EMBEDDING_URL,\n",
    "         \"model_id\": EMBEDDING_MODEL_NAME\n",
    "       }\n",
    "     },\n",
    "     \"top_k\": 10\n",
    "   }\n",
    " }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to point our request at the `v1/evaluation/targets` endpoint to create the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_endpoint = f\"{EVAL_URL}/v1/evaluation/targets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are clear to fire off the request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_response = requests.post(\n",
    "    target_endpoint,\n",
    "    json=retriever_target_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll capture our target ID for the coming steps - but with this step we have created our target and are ready to create an evaluation configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Name: eval-target-J7SB71Jji73LrPEg4LxnV7, Target Namespace: default\n"
     ]
    }
   ],
   "source": [
    "retriever_target_name = retriever_response[\"name\"]\n",
    "retriever_target_namespace = retriever_response[\"namespace\"]\n",
    "print(f\"Target Name: {retriever_target_name}, Target Namespace: {retriever_target_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab our evaluation configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_eval_config = {\n",
    " \"type\": \"retriever\",\n",
    " \"tasks\": [\n",
    "   {\n",
    "     \"type\": \"beir\",\n",
    "     \"dataset\": {\n",
    "       \"format\": \"beir\",\n",
    "       \"files_url\": \"fiqa\"\n",
    "     },\n",
    "     \"metrics\": [\n",
    "       {\n",
    "         \"name\": \"recall_5\",\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_5\",\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"recall_10\",\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_10\",\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    " ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our payload - we can send it to our Nemo Evaluator endpoint.\n",
    "\n",
    "We'll set up our Evaluator endpoint URL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_endpoint = f\"{EVAL_URL}/v1/evaluation/configs\"\n",
    "retriever_eval_response = requests.post(\n",
    "    eval_config_endpoint,\n",
    "    json=retriever_eval_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again capture our evaluation config for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Name: eval-config-EksLVqPuNX8xpLYVTefhvW, Config Namespace: default\n"
     ]
    }
   ],
   "source": [
    "retriever_config_name = retriever_eval_response[\"name\"]\n",
    "retriever_config_namespace = retriever_eval_response[\"namespace\"]\n",
    "print(f\"Config Name: {retriever_config_name}, Config Namespace: {retriever_config_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Evaluation Job\n",
    "\n",
    "Now that we have our `target_id` and `config_id` -  we have everything we need to run an evaluation.\n",
    "\n",
    "Let's see the process to create and run a job! \n",
    "\n",
    "First things first, we need to create a job payload to send to our endpoint - this will point to our target, and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = {\n",
    "    \"target\": retriever_target_namespace + \"/\" + retriever_target_name,\n",
    "    \"config\": retriever_config_namespace + \"/\" + retriever_config_name,\n",
    "    \"tags\": [\n",
    "        \"embedding-fiqa\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set the evaluation jobs endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_job_response = requests.post(\n",
    "    job_endpoint,\n",
    "    json=job_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: eval-AGd3e5Dz5Rr2xN8kpcPQbj\n"
     ]
    }
   ],
   "source": [
    "retriever_job_id = retriever_job_response[\"id\"]\n",
    "print(f\"Job ID: {retriever_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_monitoring_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs/{retriever_job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_monitoring_response = requests.get(\n",
    "    retriever_monitoring_endpoint,\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the status of our evaluation in the cell below. \n",
    "\n",
    "> NOTE: When the evaluation `status` becomes `succeeded`, the `evaluation_results` field will become populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retriever_monitoring_response[\"status\"][\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done - let's look at the full results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retriever_monitoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluation_results` field will contain our `metrics` along with their name, and their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ndcg_cut_5',\n",
       "  'value': 0.43179850619730425,\n",
       "  'metadata': {'name': 'beir'}},\n",
       " {'name': 'recall_10',\n",
       "  'value': 0.5212761004427672,\n",
       "  'metadata': {'name': 'beir'}},\n",
       " {'name': 'ndcg_cut_10',\n",
       "  'value': 0.455153721565557,\n",
       "  'metadata': {'name': 'beir'}},\n",
       " {'name': 'recall_5',\n",
       "  'value': 0.4460219435913878,\n",
       "  'metadata': {'name': 'beir'}}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_monitoring_response[\"evaluation_results\"][0][\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever + Reranking Evaluation on FiQA\n",
    "\n",
    "For our second evaluation, we're going to evaluate our Retrieval Model (`nvidia/nv-embedqa-e5-v5`) on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "Instead of simply using a Retriever model, however, this example will also leverage a Reranking model (`nvidia/nv-rerankqa-mistral-4b-v3`) to rerank the retrieved results.\n",
    "\n",
    "We'll rerun the same evaluation configuration as we did above - with a few extra parameters in our `retriever` configuration:\n",
    "\n",
    "- `ranker_url`, which will point to our reranking model\n",
    "- `ranker_model`, which will contain the name of our reranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_target_config = {\n",
    " \"type\": \"retriever\",\n",
    " \"retriever\": {\n",
    "   \"pipeline\": {\n",
    "     \"query_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": EMBEDDING_URL,\n",
    "         \"model_id\": EMBEDDING_MODEL_NAME\n",
    "       }\n",
    "     },\n",
    "     \"index_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": EMBEDDING_URL,\n",
    "         \"model_id\": EMBEDDING_MODEL_NAME\n",
    "       }\n",
    "     },\n",
    "     \"reranker_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": RERANKER_URL,\n",
    "         \"model_id\":RERANKER_MODEL_NAME\n",
    "       }\n",
    "     },\n",
    "     \"top_k\": 10\n",
    "   }\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to point our request at the `v1/evaluation/targets` endpoint to create the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_endpoint = f\"{EVAL_URL}/v1/evaluation/targets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are clear to fire off the request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_response = requests.post(\n",
    "    target_endpoint,\n",
    "    json=reranker_target_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll capture our target ID for the coming steps - but with this step we have created our target and are ready to create an evaluation configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Name: eval-target-26E1Gq39aVLL1mSvSxANhN, Target Namespace: default\n"
     ]
    }
   ],
   "source": [
    "reranker_target_name = reranker_response[\"name\"]\n",
    "reranker_target_namespace = reranker_response[\"namespace\"]\n",
    "print(f\"Target Name: {reranker_target_name}, Target Namespace: {reranker_target_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our payload - we can send it to our Nemo Evaluator endpoint.\n",
    "\n",
    "> NOTE: Notice how we don't have to re-create our evaluation configuration since we already created it for the Embedding model evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Evaluation Job\n",
    "\n",
    "Now that we have our `target_id` and `config_id` -  we have everything we need to run an evaluation.\n",
    "\n",
    "Let's see the process to create and run a job! \n",
    "\n",
    "First things first, we need to create a job payload to send to our endpoint - this will point to our target, and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_job_config = {\n",
    "    \"target\": reranker_target_namespace + \"/\" + reranker_target_name,\n",
    "    \"config\": retriever_config_namespace + \"/\" + retriever_config_name,\n",
    "    \"tags\": [\n",
    "        \"embedding-rerank-fiqa\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set the evaluation jobs endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_job_response = requests.post(\n",
    "    job_endpoint,\n",
    "    json=reranker_job_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: eval-XsxKZPjeRmzpga8GPPkbxJ\n"
     ]
    }
   ],
   "source": [
    "reranker_job_id = reranker_job_response[\"id\"]\n",
    "print(f\"Job ID: {reranker_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_monitoring_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs/{reranker_job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_monitoring_response = requests.get(\n",
    "    reranker_monitoring_endpoint,\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the status of our evaluation in the cell below. \n",
    "\n",
    "> NOTE: When the evaluation `status` becomes `succeeded`, the `evaluation_results` field will become populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reranker_monitoring_response[\"status\"][\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done - let's look at the full results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reranker_monitoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluation_results` field will contain our `metrics` along with their name, and their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_monitoring_response[\"evaluation_results\"][0][\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Evaluation on FIQA with Ragas Metrics\n",
    "\n",
    "With the most recent release of NeMo Evaluator microservice, not only can we evaluate Retrievers and Rerankers - we can also Evaluate RAG!\n",
    "\n",
    "Once again, we're going to evaluate on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "We're also going to evaluate our RAG pipeline on the [Ragas](https://docs.ragas.io/en/stable/howtos/index.html) metrics [\"Faithfulness\"](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html). This can be done by extending our evaluation configuration in the following ways:\n",
    "\n",
    "1. We can create the model type `rag`, and provide our `retriever` configuration we used in the first evaluation.\n",
    "2. We need to provide a `context_ordering` parameter, in this case we'll use `desc` which will order our context in descending score.\n",
    "3. We need to provide a \"generator\" (LLM) that can be used to generate responses based on the retrieved context!\n",
    "\n",
    "We'll also need to add in a number of `judge_` parameters to help calculate the Faithfulness metric.\n",
    "\n",
    "Let's look at an example evaluation configuration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_target_config = {\n",
    " \"type\": \"rag\",\n",
    " \"rag\": {\n",
    "   \"pipeline\": {\n",
    "     \"retriever\": {\n",
    "       \"pipeline\": {\n",
    "         \"query_embedding_model\": {\n",
    "           \"api_endpoint\": {\n",
    "             \"url\": EMBEDDING_URL,\n",
    "             \"model_id\": EMBEDDING_MODEL_NAME\n",
    "           }\n",
    "         },\n",
    "         \"index_embedding_model\": {\n",
    "           \"api_endpoint\": {\n",
    "             \"url\": EMBEDDING_URL,\n",
    "             \"model_id\": EMBEDDING_MODEL_NAME\n",
    "           }\n",
    "         }\n",
    "       }\n",
    "     },\n",
    "     \"model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": LLM_URL,\n",
    "         \"model_id\": LLM_MODEL_NAME\n",
    "       }\n",
    "     }\n",
    "   }\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to point our request at the `v1/evaluation/targets` endpoint to create the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_endpoint = f\"{EVAL_URL}/v1/evaluation/targets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are clear to fire off the request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_response = requests.post(\n",
    "    target_endpoint,\n",
    "    json=rag_target_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll capture our target ID for the coming steps - but with this step we have created our target and are ready to create an evaluation configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Name: eval-target-N1yeAWk2HU9aypHubpvdYW, Target Namespace: default\n"
     ]
    }
   ],
   "source": [
    "rag_target_name = rag_response[\"name\"]\n",
    "rag_target_namespace = rag_response[\"namespace\"]\n",
    "print(f\"Target Name: {rag_target_name}, Target Namespace: {rag_target_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab our evaluation configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_config = {\n",
    " \"type\": \"rag\",\n",
    " \"tasks\": [\n",
    "   {\n",
    "     \"type\": \"beir\",\n",
    "     \"params\": {\n",
    "       \"judge_llm\": {\n",
    "         \"api_endpoint\": {\n",
    "           \"url\": LLM_URL,\n",
    "           \"model_id\": LLM_MODEL_NAME\n",
    "         }\n",
    "       },\n",
    "       \"judge_embeddings\": {\n",
    "         \"api_endpoint\": {\n",
    "           \"url\": EMBEDDING_URL,\n",
    "           \"model_id\": EMBEDDING_MODEL_NAME\n",
    "         }\n",
    "       },\n",
    "       \"judge_timeout\": 300,\n",
    "       \"judge_max_retries\": 5,\n",
    "       \"judge_max_workers\": 16\n",
    "     },\n",
    "     \"dataset\": {\n",
    "       \"files_url\": \"fiqa\",\n",
    "       \"format\": \"beir\"\n",
    "     },\n",
    "     \"metrics\": [\n",
    "       {\n",
    "         \"name\": \"recall_5\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_5\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"recall_10\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_10\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"faithfulness\"\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    " ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our payload - we can send it to our Nemo Evaluator endpoint.\n",
    "\n",
    "We'll set up our Evaluator endpoint URL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_endpoint = f\"{EVAL_URL}/v1/evaluation/configs\"\n",
    "rag_eval_response = requests.post(\n",
    "    eval_config_endpoint,\n",
    "    json=rag_eval_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again capture our evaluation config for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Name: eval-config-F9xnHtPdwPYFMLjuTdcRpR, Config Namespace: default\n"
     ]
    }
   ],
   "source": [
    "rag_config_name = rag_eval_response[\"name\"]\n",
    "rag_config_namespace = rag_eval_response[\"namespace\"]\n",
    "print(f\"Config Name: {rag_config_name}, Config Namespace: {rag_config_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Evaluation Job\n",
    "\n",
    "Now that we have our `target_id` and `config_id` -  we have everything we need to run an evaluation.\n",
    "\n",
    "Let's see the process to create and run a job! \n",
    "\n",
    "First things first, we need to create a job payload to send to our endpoint - this will point to our target, and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_job_config = {\n",
    "    \"target\": rag_target_namespace + \"/\" + rag_target_name,\n",
    "    \"config\": rag_config_namespace + \"/\" + rag_config_name,\n",
    "    \"tags\": [\n",
    "        \"rag-eval\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set the evaluation jobs endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_job_response = requests.post(\n",
    "    job_endpoint,\n",
    "    json=rag_job_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: eval-SfNDFM4Ei28bp5GrYzzNy5\n"
     ]
    }
   ],
   "source": [
    "rag_job_id = rag_job_response[\"id\"]\n",
    "print(f\"Job ID: {rag_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_monitoring_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs/{rag_job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_monitoring_response = requests.get(\n",
    "    rag_monitoring_endpoint,\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_monitoring_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the status of our evaluation in the cell below. \n",
    "\n",
    "> NOTE: When the evaluation `status` becomes `succeeded`, the `evaluation_results` field will become populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "print(rag_monitoring_response[\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done - let's look at the full results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_monitoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluation_results` field will contain our `metrics` along with their name, and their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_monitoring_response[\"evaluation_results\"][0][\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Evaluation on Synthetically Generated Data with Ragas Metrics\n",
    "\n",
    "For our final evaluation, we're going to be leveraging work done in [this](https://github.com/NVIDIA/GenerativeAIExamples/blob/main/nemo/retriever-synthetic-data-generation/notebooks/quickstart.ipynb) notebook to create a BeIR format dataset created with Synthetic Data Generation. \n",
    "\n",
    "The output from the above notebook should be a dataset with the following items which can be found in the `outputs/sample_synthetic_data/beir/filtered/synthetic` directory after running the notebook:\n",
    "\n",
    "- `corpus.jsonl`\n",
    "- `qrels/test.tsv`\n",
    "- `queries.jsonl`\n",
    "\n",
    "This notebook assumes you've run the above notebook and have moved the `outputs/sample_synthetic_data/beir/filtered/synthetic` directory into the root folder of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following utility function to upload the folder contents to the NeMo Datastore microservice under the name \"SDG_BEIR_DATASET\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hh\n",
    "import requests\n",
    "\n",
    "DATASTORE_URL = \"<< YOUR DATASTORE URL >>\"\n",
    "\n",
    "## This token is not used in NDS, and so it could be any value.\n",
    "token = \"mock\"\n",
    "\n",
    "repo_name = \"nvidias/sdg_beir\"\n",
    "repo_type = \"dataset\"\n",
    "dir_path = \"./synthetic\"\n",
    "\n",
    "hf_api = hh.HfApi(endpoint=DATASTORE_URL, token=token)\n",
    "\n",
    "# create repo\n",
    "hf_api.create_repo(\n",
    "    repo_id=repo_name,\n",
    "    repo_type=repo_type,\n",
    ")\n",
    "\n",
    "# upload dir\n",
    "path_in_repo = \".\"\n",
    "result = hf_api.upload_folder(repo_id=repo_name, folder_path=dir_path, path_in_repo=path_in_repo, repo_type=repo_type)\n",
    "\n",
    "print(f\"Dataset folder uploaded to: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can once again create our RAG evaluation configuration while make a small change, which is to simply point at the newly updated dataset.\n",
    "\n",
    "Also, since we already have our target created - we do not need to reinitialize it - we can simple create a new evaluation configuration for this target!\n",
    "\n",
    "Also - we can use OpenAI API compatible models as our judge, like OpenAI's `gpt-4` model. However, we will need to provide our OpenAI API key. Let's do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please provide your OpenAI API key!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our new evaluation configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_gpt4_eval_config = {\n",
    " \"type\": \"rag\",\n",
    " \"tasks\": [\n",
    "   {\n",
    "     \"type\": \"beir\",\n",
    "     \"params\": {\n",
    "       \"judge_llm\": {\n",
    "         \"api_endpoint\": {\n",
    "           \"url\": \"https://api.openai.com/v1/chat/completions\",\n",
    "           \"model_id\": \"gpt-4\",\n",
    "           \"api_key\": os.environ[\"OPENAI_API_KEY\"]\n",
    "         }\n",
    "       },\n",
    "       \"judge_embeddings\": {\n",
    "         \"api_endpoint\": {\n",
    "           \"url\": \"https://api.openai.com/v1/embeddings\",\n",
    "           \"model_id\": \"text-embedding-3-small\",\n",
    "           \"api_key\": os.environ[\"OPENAI_API_KEY\"]\n",
    "         }\n",
    "       },\n",
    "       \"judge_timeout\": 300,\n",
    "       \"judge_max_retries\": 5,\n",
    "       \"judge_max_workers\": 16\n",
    "     },\n",
    "     \"dataset\": {\n",
    "       \"files_url\": \"nds:SDG_BEIR_DATASET\",\n",
    "       \"format\": \"beir\"\n",
    "     },\n",
    "     \"metrics\": [\n",
    "       {\n",
    "         \"name\": \"recall_5\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_5\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"recall_10\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_10\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"faithfulness\"\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    " ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Name: eval-config-AUCsyQzmD5ZMqamx1hQ3ZE, Config Namespace: default\n"
     ]
    }
   ],
   "source": [
    "eval_config_endpoint = f\"{EVAL_URL}/v1/evaluation/configs\"\n",
    "rag_gpt4_eval_response = requests.post(\n",
    "    eval_config_endpoint,\n",
    "    json=rag_gpt4_eval_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()\n",
    "rag_gpt4_config_name = rag_gpt4_eval_response[\"name\"]\n",
    "rag_gpt4_config_namespace = rag_gpt4_eval_response[\"namespace\"]\n",
    "print(f\"Config Name: {rag_gpt4_config_name}, Config Namespace: {rag_gpt4_config_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Evaluation Job\n",
    "\n",
    "Now that we have our `target_id` and `config_id` -  we have everything we need to run an evaluation.\n",
    "\n",
    "Let's see the process to create and run a job! \n",
    "\n",
    "First things first, we need to create a job payload to send to our endpoint - this will point to our target, and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_gpt4_job_config = {\n",
    "    \"name\": \"rag-gpt4-eval\",\n",
    "    \"target\": rag_target_namespace + \"/\" + rag_target_name,\n",
    "    \"config\": rag_gpt4_config_namespace + \"/\" + rag_gpt4_config_name,\n",
    "    \"tags\": [\n",
    "        \"rag-gpt4-eval\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set the evaluation jobs endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_gpt4_job_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_gpt4_job_response = requests.post(\n",
    "    rag_gpt4_job_endpoint,\n",
    "    json=rag_gpt4_job_config,\n",
    "    headers={'accept': 'application/json'}\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: eval-EHm3XUkrN2egwsos345rrE\n"
     ]
    }
   ],
   "source": [
    "rag_gpt4_job_id = rag_gpt4_job_response[\"id\"]\n",
    "print(f\"Job ID: {rag_gpt4_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_gpt4_monitoring_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs/{rag_gpt4_job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_gpt4_monitoring_response = requests.get(\n",
    "    rag_gpt4_monitoring_endpoint,\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the status of our evaluation in the cell below. \n",
    "\n",
    "> NOTE: When the evaluation `status` becomes `succeeded`, the `evaluation_results` field will become populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "print(rag_gpt4_monitoring_response[\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done - let's look at the full results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_gpt4_monitoring_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

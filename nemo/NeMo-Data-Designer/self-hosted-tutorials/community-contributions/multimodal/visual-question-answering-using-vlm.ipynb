{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® NeMo Data Designer: Visual Question Answering Dataset Generation\n",
    "\n",
    "### üìö What you'll learn\n",
    "\n",
    "This notebook demonstrates how to use NeMo Data Designer to generate high-quality synthetic question-answer datasets from visual documents.\n",
    "\n",
    "<br>\n",
    "\n",
    "> üëã **IMPORTANT** ‚Äì¬†Environment Setup\n",
    ">\n",
    "> - If you haven't already, follow the instructions in the [README](../../../README.md) to install the necessary dependencies.\n",
    ">\n",
    "> - You may need to restart your notebook's kernel after setting up the environment.\n",
    "> - In this notebook, we assume you have a self-hosted instance of Data Designer up and running.\n",
    ">\n",
    "> - For deployment instructions, see the [Installation Options](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/index.html#installation-options) section of the [NeMo Data Designer documentation](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/index.html).\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Import the essentials\n",
    "\n",
    "- The `data_designer` module of `nemo_microservices` exposes Data Designer's high-level SDK.\n",
    "\n",
    "- The `essentials` module provides quick access to the most commonly used objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import rich\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# NeMo Data Designer imports\n",
    "from nemo_microservices.data_designer.essentials import (\n",
    "    CategorySamplerParams,\n",
    "    DataDesignerConfigBuilder,\n",
    "    ImageContext,\n",
    "    ImageFormat,\n",
    "    InferenceParameters,\n",
    "    LLMStructuredColumnConfig,\n",
    "    ModelConfig,\n",
    "    ModalityDataType,\n",
    "    NeMoDataDesignerClient,\n",
    "    SamplerColumnConfig,\n",
    "    SamplerType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Initialize the NeMo Data Designer Client\n",
    "\n",
    "- `NeMoDataDesignerClient` is responsible for submitting generation requests to the microservice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEMO_MICROSERVICES_BASE_URL = \"http://localhost:8080\"\n",
    "\n",
    "data_designer_client = NeMoDataDesignerClient(base_url=NEMO_MICROSERVICES_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Define model configurations\n",
    "\n",
    "- Each `ModelConfig` defines a model that can be used during the generation process.\n",
    "\n",
    "- The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).\n",
    "\n",
    "- The \"model provider\" is the external service that hosts the model (see [the model config docs](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/configure-models.html) for more details).\n",
    "\n",
    "- By default, the microservice uses [build.nvidia.com](https://build.nvidia.com/models) as the model provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This name is set in the microservice deployment configuration.\n",
    "MODEL_PROVIDER = \"nvidiabuild\"\n",
    "\n",
    "# The model ID is from build.nvidia.com.\n",
    "MODEL_ID = \"meta/llama-4-maverick-17b-128e-instruct\"\n",
    "\n",
    "# We choose this alias to be descriptive for our use case.\n",
    "MODEL_ALIAS = \"llama-4-maverick-17b-128e-instruct\"\n",
    "\n",
    "model_configs = [\n",
    "    ModelConfig(\n",
    "        alias=MODEL_ALIAS,\n",
    "        model=MODEL_ID,\n",
    "        provider=MODEL_PROVIDER,\n",
    "        inference_parameters=InferenceParameters(\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            max_tokens=1024,\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Initialize the Data Designer Config Builder\n",
    "\n",
    "- The Data Designer config defines the dataset schema and generation process.\n",
    "\n",
    "- The config builder provides an intuitive interface for building this configuration.\n",
    "\n",
    "- The list of model configs is provided to the builder at initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = DataDesignerConfigBuilder(model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå± Loading Seed Data\n",
    "\n",
    "In this section, we'll prepare our visual documents as a seed dataset. The seed dataset provides the foundation for synthetic data generation by:\n",
    "\n",
    "- **Loading Visual Documents**: We use the ColPali dataset containing document images\n",
    "- **Image Processing**: Convert images to base64 format for model consumption\n",
    "- **Metadata Extraction**: Preserve relevant document information\n",
    "- **Sampling Strategy**: Configure how the seed data is utilized during generation\n",
    "\n",
    "The seed dataset can be referenced in generation prompts using Jinja templating.\n",
    "\n",
    "<br>\n",
    "\n",
    "> üå± **Why use a seed dataset?**\n",
    ">\n",
    "> - Seed datasets let you steer the generation process by providing context that is specific to your use case.\n",
    ">\n",
    "> - Seed datasets are also an excellent way to inject real-world diversity into your synthetic data.\n",
    ">\n",
    "> - During generation, prompt templates can reference any of the seed dataset fields.\n",
    "\n",
    "<br>\n",
    "\n",
    "> üí° **About datastores**\n",
    ">\n",
    "> - You can use seed datasets from _either_ the Hugging Face Hub or a locally deployed datastore.\n",
    ">\n",
    "> - By default, we use the local datastore deployed with the Data Designer microservice.\n",
    ">\n",
    "> - The datastore endpoint is specified in the deployment configuration.\n",
    "\n",
    "üëã **Note**: At this time, we only support using a single file as the seed. If you have multiple files you would like to use as \\\n",
    "seeds, it is recommended you consolidated these into a single file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset processing configuration\n",
    "IMG_COUNT = 512  # Number of images to process\n",
    "BASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n",
    "\n",
    "# Load ColPali dataset for visual documents\n",
    "img_dataset_cfg = {\n",
    "    \"path\": \"vidore/colpali_train_set\",\n",
    "    \"split\": \"train\",\n",
    "    \"streaming\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions to preprocess the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, height: int):\n",
    "    \"\"\"\n",
    "    Resize image while maintaining aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        height: Target height in pixels\n",
    "\n",
    "    Returns:\n",
    "        Resized PIL Image object\n",
    "    \"\"\"\n",
    "    original_width, original_height = image.size\n",
    "    width = int(original_width * (height / original_height))\n",
    "    return image.resize((width, height))\n",
    "\n",
    "\n",
    "def convert_image_to_chat_format(record, height: int) -> dict:\n",
    "    \"\"\"\n",
    "    Convert PIL image to base64 format for chat template usage.\n",
    "\n",
    "    Args:\n",
    "        record: Dataset record containing image and metadata\n",
    "        height: Target height for image resizing\n",
    "\n",
    "    Returns:\n",
    "        Updated record with base64_image and uuid fields\n",
    "    \"\"\"\n",
    "    # Resize image for consistent processing\n",
    "    image = resize_image(record[\"image\"], height)\n",
    "\n",
    "    # Convert to base64 string\n",
    "    img_buffer = io.BytesIO()\n",
    "    image.save(img_buffer, format=\"PNG\")\n",
    "    byte_data = img_buffer.getvalue()\n",
    "    base64_encoded_data = base64.b64encode(byte_data)\n",
    "    base64_string = base64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "    # Return updated record\n",
    "    return record | {\"base64_image\": base64_string, \"uuid\": str(uuid.uuid4())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the visual document dataset\n",
    "print(\"üì• Loading and processing document images...\")\n",
    "\n",
    "img_dataset_iter = iter(\n",
    "    load_dataset(**img_dataset_cfg).map(\n",
    "        convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT}\n",
    "    )\n",
    ")\n",
    "img_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the seed dataset to a csv file locally\n",
    "os.makedirs(\"./data/\", exist_ok=True)\n",
    "\n",
    "df_seed = pd.DataFrame(img_dataset)[\n",
    "    [\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]\n",
    "]\n",
    "df_seed.to_csv(\"./data/colpali_train_set.csv\", index=False)\n",
    "\n",
    "df_seed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the seed dataset containing our processed images\n",
    "dataset_reference = data_designer_client.upload_seed_dataset(\n",
    "    repo_id=\"data-designer-advanced/visual-qna\",\n",
    "    dataset=\"./data/colpali_train_set.csv\",\n",
    "    datastore_settings={\"endpoint\": \"http://localhost:3000/v1/hf\"},\n",
    ")\n",
    "\n",
    "config_builder.with_seed_dataset(\n",
    "    dataset_reference=dataset_reference,\n",
    "    sampling_strategy=\"ordered\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶ú Generating Summary of Image Contents\n",
    "\n",
    "- We instruct the model to ‚Äúlook‚Äù at each image and write a short, Markdown\n",
    "  summary.\n",
    "\n",
    "- We ask it to read the page from top ‚û°Ô∏è bottom, then include a quick wrap-up\n",
    "  at the end.\n",
    "\n",
    "- That summary becomes helpful context we‚Äôll reuse to generate focused\n",
    "  questions and answers about the document later.\n",
    "\n",
    "### üñºÔ∏è How the image is provided\n",
    "\n",
    "We pass the image via `multi_modal_context` using `ImageContext`:\n",
    "\n",
    "- **Column**: `base64_image` (your image bytes encoded as Base64)\n",
    "- **Modality**: `ModalityDataType.BASE64`\n",
    "- **Format**: `ImageFormat.PNG`\n",
    "\n",
    "In other words, `ImageContext` tells the model ‚Äúthis is an image, encoded as Base64,\n",
    "and it‚Äôs a PNG,‚Äù so it knows exactly how to \\\n",
    "use it during summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to generate detailed document summaries\n",
    "config_builder.add_column(\n",
    "    name=\"summary\",\n",
    "    column_type=\"llm-text\",\n",
    "    model_alias=MODEL_ALIAS,\n",
    "    prompt=(\n",
    "        \"Provide a detailed summary of the content in this image in Markdown format.\"\n",
    "        \"Start from the top of the image and then describe it from top to bottom.\"\n",
    "        \"Place a summary at the bottom.\"\n",
    "    ),\n",
    "    multi_modal_context=[\n",
    "        ImageContext(\n",
    "            column_name=\"base64_image\",\n",
    "            data_type=ModalityDataType.BASE64,\n",
    "            image_format=ImageFormat.PNG,\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Designing our Data Schema\n",
    "\n",
    "Structured outputs ensure consistent and predictable data generation. Data Designer supports schemas defined using:\n",
    "\n",
    "- **JSON Schema**: For basic structure definition\n",
    "- **Pydantic Models**: For advanced validation and type safety (recommended)\n",
    "\n",
    "We'll use Pydantic models to define our Question-Answer schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    \"\"\"Schema for generated questions\"\"\"\n",
    "\n",
    "    question: str = Field(description=\"The question to be generated\")\n",
    "\n",
    "\n",
    "class QuestionTopic(BaseModel):\n",
    "    \"\"\"Schema for question topics\"\"\"\n",
    "\n",
    "    topic: str = Field(description=\"The topic/category of the question\")\n",
    "\n",
    "\n",
    "class Options(BaseModel):\n",
    "    \"\"\"Schema for multiple choice options\"\"\"\n",
    "\n",
    "    option_a: str = Field(description=\"The first answer choice\")\n",
    "    option_b: str = Field(description=\"The second answer choice\")\n",
    "    option_c: str = Field(description=\"The third answer choice\")\n",
    "    option_d: str = Field(description=\"The fourth answer choice\")\n",
    "\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Schema for question answers\"\"\"\n",
    "\n",
    "    answer: Literal[\"option_a\", \"option_b\", \"option_c\", \"option_d\"] = Field(\n",
    "        description=\"The correct answer to the question\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Adding Sampler Columns\n",
    "\n",
    "- Sampler columns offer non-LLM based generation of synthetic data.\n",
    "\n",
    "- They are particularly useful for **steering the diversity** of the generated data, as we demonstrate below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"difficulty\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(values=[\"easy\", \"medium\", \"hard\"]),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶ú Adding LLM Generated columns\n",
    "\n",
    "Now define the columns that the model will generate. These prompts instruct the LLM to produce:\n",
    "\n",
    "- question\n",
    "- options\n",
    "- topic\n",
    "- answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    LLMStructuredColumnConfig(\n",
    "        name=\"question\",\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        prompt=(\n",
    "            \"Generate a question based on the following context: {{ summary }}. \"\n",
    "            \"The difficulty of the generated question should be {{ difficulty }}\"\n",
    "        ),\n",
    "        system_prompt=(\n",
    "            \"You are a helpful assistant that generates questions based on the given context. \"\n",
    "            \"The context are sourced from documents pertaining to the petroleum industry. \"\n",
    "            \"You will be given a context and you will need to generate a question based on the context. \"\n",
    "            \"The difficulty of the generated question should be {{ difficulty }}\"\n",
    "            \"Ensure you generate just the question and no other text.\"\n",
    "        ),\n",
    "        output_format=Question,\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    LLMStructuredColumnConfig(\n",
    "        name=\"options\",\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        prompt=(\n",
    "            \"Generate four answer choices for the question: {{ question }} based on the following context: {{ summary }}. \"\n",
    "            \"The option you generate should match the difficulty of the generated question, {{ difficulty }}.\"\n",
    "        ),\n",
    "        output_format=Options,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "config_builder.add_column(\n",
    "    LLMStructuredColumnConfig(\n",
    "        name=\"answer\",\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        prompt=(\n",
    "            \"Choose the correct answer for the question: {{ question }} based on the following context: {{ summary }}\"\n",
    "            \"and options choices. The options are {{ options }}. Only select one of the options as the answer.\"\n",
    "        ),\n",
    "        output_format=Answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "config_builder.add_column(\n",
    "    LLMStructuredColumnConfig(\n",
    "        name=\"topic\",\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        system_prompt=(\n",
    "            \"Generate a short 1-3 word topic for the question: {{ question }} \"\n",
    "            \"based on the given context. {{ summary }}\"\n",
    "        ),\n",
    "        prompt=(\n",
    "            \"Generate the topic of the question: {{ question }} based on the following context: {{ summary }}\"\n",
    "            \"The topic should be a single word or phrase that is relevant to the question and context. \"\n",
    "        ),\n",
    "        output_format=QuestionTopic,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÅ Iteration is key ‚Äì¬†preview the dataset!\n",
    "\n",
    "1. Use the `preview` method to generate a sample of records quickly.\n",
    "\n",
    "2. Inspect the results for quality and format issues.\n",
    "\n",
    "3. Adjust column configurations, prompts, or parameters as needed.\n",
    "\n",
    "4. Re-run the preview until satisfied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a few records\n",
    "preview = data_designer_client.preview(config_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More previews\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Analyze the generated data\n",
    "\n",
    "- Data Designer automatically generates a basic statistical analysis of the generated data.\n",
    "\n",
    "- This analysis is available via the `analysis` property of generation result objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the analysis as a table.\n",
    "preview.analysis.to_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîé View Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original document with generated outputs\n",
    "index = 0  # Change this to view different examples\n",
    "\n",
    "# Merge preview data with original images for comparison\n",
    "comparison_dataset = preview.dataset.merge(\n",
    "    pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\"\n",
    ")\n",
    "\n",
    "print(\"üìÑ Original Document Image:\")\n",
    "display(resize_image(comparison_dataset.image[index], BASE64_IMAGE_HEIGHT))\n",
    "\n",
    "print(\"\\nüìù Generated Summary:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        comparison_dataset.summary[index], title=\"Document Summary\", title_align=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nüî¢ Generated Difficulty:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.difficulty[index]),\n",
    "        title=\"Difficulty\",\n",
    "        title_align=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n‚ùì Generated Question:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.question[index]),\n",
    "        title=\"Question\",\n",
    "        title_align=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nüî¢ Generated Options:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.options[index]),\n",
    "        title=\"Answer Choices\",\n",
    "        title_align=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nüî¢ Generated Topic:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.topic[index]), title=\"Topic\", title_align=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Generated Answer:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.answer[index]),\n",
    "        title=\"Correct Answer\",\n",
    "        title_align=\"left\",\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üÜô Scale up!\n",
    "\n",
    "- Happy with your preview data?\n",
    "\n",
    "- Use the `create` method to submit larger Data Designer generation jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_results = data_designer_client.create(config_builder, num_records=20)\n",
    "\n",
    "# This will block until the job is complete.\n",
    "job_results.wait_until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generated dataset as a pandas DataFrame.\n",
    "dataset = job_results.load_dataset()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the analysis results into memory.\n",
    "analysis = job_results.load_analysis()\n",
    "\n",
    "analysis.to_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUTORIAL_OUTPUT_PATH = \"data-designer-tutorial-output\"\n",
    "\n",
    "# Download the job artifacts and save them to disk.\n",
    "job_results.download_artifacts(\n",
    "    output_path=TUTORIAL_OUTPUT_PATH,\n",
    "    artifacts_folder_name=\"artifacts-community-contributions-multimodal-visual-question-answering\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original document with generated outputs\n",
    "index = 0  # Change this to view different examples\n",
    "\n",
    "# Merge preview data with original images for comparison\n",
    "comparison_dataset = dataset.merge(\n",
    "    pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]], how=\"left\", on=\"uuid\"\n",
    ")\n",
    "\n",
    "print(\"üìÑ Original Document Image:\")\n",
    "display(resize_image(comparison_dataset.image[index], BASE64_IMAGE_HEIGHT))\n",
    "\n",
    "print(\"\\nüìù Generated Summary:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        comparison_dataset.summary[index], title=\"Document Summary\", title_align=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nüî¢ Generated Difficulty:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.difficulty[index]),\n",
    "        title=\"Difficulty\",\n",
    "        title_align=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n‚ùì Generated Question:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.question[index]),\n",
    "        title=\"Question\",\n",
    "        title_align=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nüî¢ Generated Options:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.options[index]),\n",
    "        title=\"Answer Choices\",\n",
    "        title_align=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nüî¢ Generated Topic:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.topic[index]), title=\"Topic\", title_align=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Generated Answer:\")\n",
    "rich.print(\n",
    "    Panel(\n",
    "        json.dumps(comparison_dataset.answer[index]),\n",
    "        title=\"Correct Answer\",\n",
    "        title_align=\"left\",\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdg_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

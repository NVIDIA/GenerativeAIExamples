{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8f02ab",
   "metadata": {},
   "source": [
    "# üé® NeMo Data Designer: Product Information Dataset Generator with Q&A using Open Router's Distillation endpoints API\n",
    "\n",
    "#### üìö What you'll learn\n",
    "\n",
    "This notebook demonstrates how to use NeMo Data Designer to create a synthetic dataset of product information with corresponding questions and answers using OpenRouter's distillation endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a471c7",
   "metadata": {},
   "source": [
    "### Install the package if not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install data-designer==0.1.5\n",
    "# !pip install dotenv==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b83e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "env = load_dotenv()\n",
    "\n",
    "Open_Router_Api_Key = os.getenv('OPEN_ROUTER')\n",
    "Base_URL = \"https://openrouter.ai/api/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d3c6f",
   "metadata": {},
   "source": [
    "### üì¶ Import the essentials\n",
    "\n",
    "- The `essentials` module provides quick access to the most commonly used objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_designer.essentials import (\n",
    "    CategorySamplerParams,\n",
    "    DataDesigner,\n",
    "    DataDesignerConfigBuilder,\n",
    "    LLMTextColumnConfig,\n",
    "    PersonSamplerParams,\n",
    "    SamplerColumnConfig,\n",
    "    SamplerType,\n",
    "    ModelProvider,\n",
    "    UniformSamplerParams,\n",
    "    Score,\n",
    "    ModelConfig,\n",
    "    InferenceParameters,\n",
    "    ExpressionColumnConfig,\n",
    "    LLMStructuredColumnConfig,\n",
    "    BernoulliSamplerParams,\n",
    "    LLMJudgeColumnConfig,\n",
    ")\n",
    "\n",
    "# If we want debugging - uncomment the following\n",
    "# from data_designer.logging import configure_logging, LoggingConfig, OutputConfig\n",
    "# import sys\n",
    "\n",
    "# # Enable debug logging with structured JSON output to see full API call details\n",
    "# configure_logging(\n",
    "#     LoggingConfig(\n",
    "#         logger_configs=[LoggingConfig.debug().logger_configs[0]],\n",
    "#         output_configs=[OutputConfig(destination=sys.stderr, structured=True)],  # Use JSON format\n",
    "#         root_level=\"DEBUG\"\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011e937",
   "metadata": {},
   "source": [
    "### Try to call the OpenRouter endpoint first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87966af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=Base_URL,\n",
    "  api_key=Open_Router_Api_Key,\n",
    ")\n",
    "\n",
    "# First API call with reasoning\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"what is 1+1\"}],\n",
    "    max_tokens=100,\n",
    "    stream=False,\n",
    "    extra_body={\"reasoning\": {\"enabled\": False}}\n",
    ")\n",
    "\n",
    "reasoning = getattr(completion.choices[0].message, \"reasoning\", None)\n",
    "if reasoning:\n",
    "  print(reasoning)\n",
    "  print('---')\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17dedd",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Initialize the NeMo Data Designer Client\n",
    "\n",
    "- `NeMoDataDesignerClient` is responsible for submitting generation requests to the microservice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_provider = ModelProvider(\n",
    "    name = \"nvidia\",\n",
    "    endpoint = \"https://openrouter.ai/api/v1/\",\n",
    "    provider_type = \"openai\",\n",
    "    api_key = Open_Router_Api_Key\n",
    ")\n",
    "\n",
    "data_designer_client = DataDesigner(model_providers=[model_provider])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0b816",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Define model configurations\n",
    "\n",
    "- Each `ModelConfig` defines a model that can be used during the generation process.\n",
    "\n",
    "- The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).\n",
    "\n",
    "- The \"model provider\" is the external service that hosts the model (see [the model config docs](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/configure-models.html) for more details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_alias=\"nemotron-3-nano-30b-a3b\"\n",
    "\n",
    "inference_parameters = InferenceParameters(\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    max_tokens=10000,\n",
    "    max_parallel_requests=10,  # Number of concurrent workers\n",
    "    extra_body={\n",
    "        \"provider\": {\n",
    "            \"enforce_distillable_text\": True,\n",
    "            # optionally, prefer NVIDIA endpoints\n",
    "            \"only\": [\"nvidia\"]\n",
    "        },\n",
    "        \"reasoning\": {\"enabled\": False}\n",
    "    },\n",
    ")\n",
    "\n",
    "model_configs = [\n",
    "    ModelConfig(\n",
    "        alias=model_alias,\n",
    "        model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n",
    "        provider=\"nvidia\",\n",
    "        inference_parameters=inference_parameters\n",
    "        )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d247a",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Initialize the Data Designer Config Builder\n",
    "\n",
    "- The Data Designer config defines the dataset schema and generation process.\n",
    "\n",
    "- The config builder provides an intuitive interface for building this configuration.\n",
    "\n",
    "- The list of model configs is provided to the builder at initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = DataDesignerConfigBuilder(model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb3311",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Defining Data Structures\n",
    "\n",
    "Now we'll define the data models and evaluation rubrics for our product information dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "# Define product information structure\n",
    "class ProductInfo(BaseModel):\n",
    "    product_name: str = Field(\n",
    "        ..., description=\"A realistic product name for the market.\"\n",
    "    )\n",
    "    key_features: list[str] = Field(\n",
    "        ..., min_length=1, max_length=3, description=\"Key product features.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"A short, engaging description of what the product does, highlighting a unique but believable feature.\",\n",
    "    )\n",
    "    price_usd: float = Field(..., description=\"The stated price in USD.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed416ec",
   "metadata": {},
   "source": [
    "## üé≤ Adding Sampler Columns\n",
    "\n",
    "- Sampler columns offer non-LLM based generation of synthetic data.\n",
    "\n",
    "- They are particularly useful for **steering the diversity** of the generated data, as we demonstrate below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01cc94-6268-4aac-9e69-a57ef8c9bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define product category options\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"category\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(\n",
    "            values=[\n",
    "                \"Electronics\",\n",
    "                \"Clothing\",\n",
    "                \"Home Appliances\",\n",
    "                \"Groceries\",\n",
    "                \"Toiletries\",\n",
    "                \"Sports Equipment\",\n",
    "                \"Toys\",\n",
    "                \"Books\",\n",
    "                \"Pet Supplies\",\n",
    "                \"Tools & Home Improvement\",\n",
    "                \"Beauty\",\n",
    "                \"Health & Wellness\",\n",
    "                \"Outdoor Gear\",\n",
    "                \"Automotive\",\n",
    "                \"Jewelry\",\n",
    "                \"Watches\",\n",
    "                \"Office Supplies\",\n",
    "                \"Gifts\",\n",
    "                \"Arts & Crafts\",\n",
    "                \"Baby & Kids\",\n",
    "                \"Music\",\n",
    "                \"Video Games\",\n",
    "                \"Movies\",\n",
    "                \"Software\",\n",
    "                \"Tech Devices\",\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define price range to seed realistic product types\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"price_tens_of_dollars\",\n",
    "        sampler_type=SamplerType.UNIFORM,\n",
    "        params=UniformSamplerParams(low=1, high=200),\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    ExpressionColumnConfig(\n",
    "        name=\"product_price\",\n",
    "        expr=\"{{ (price_tens_of_dollars * 10) - 0.01 | round(2) }}\",\n",
    "        dtype=\"float\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Generate first letter for product name to ensure diversity\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"first_letter\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(values=list(string.ascii_uppercase)),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Determine if this example will include hallucination\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"is_hallucination\",\n",
    "        sampler_type=SamplerType.BERNOULLI,\n",
    "        params=BernoulliSamplerParams(p=0.5),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b2669",
   "metadata": {},
   "source": [
    "## ü¶ú LLM-generated columns\n",
    "\n",
    "- When prompting the LLM, we can use Jinja templating to reference other columns in the dataset.\n",
    "\n",
    "- As we see below, nested json fields can be accessed using dot notation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d59972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate product information\n",
    "config_builder.add_column(\n",
    "    LLMStructuredColumnConfig(\n",
    "        name=\"product_info\",\n",
    "        model_alias=model_alias,\n",
    "        prompt=(\n",
    "            \"Generate a realistic product description for a product in the {{ category }} \"\n",
    "            \"category that costs {{ product_price }}.\\n\"\n",
    "            \"The name of the product MUST start with the letter {{ first_letter }}.\\n\"\n",
    "        ),\n",
    "        output_format=ProductInfo,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Generate user questions about the product\n",
    "config_builder.add_column(\n",
    "    LLMTextColumnConfig(\n",
    "        name=\"question\",\n",
    "        model_alias=model_alias,\n",
    "        prompt=(\"Ask a question about the following product:\\n\\n {{ product_info }}\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Generate answers to the questions\n",
    "config_builder.add_column(\n",
    "    LLMTextColumnConfig(\n",
    "        name=\"answer\",\n",
    "        model_alias=model_alias,\n",
    "        prompt=(\n",
    "            \"{%- if is_hallucination == 0 -%}\\n\"\n",
    "            \"<product_info>\\n\"\n",
    "            \"{{ product_info }}\\n\"\n",
    "            \"</product_info>\\n\"\n",
    "            \"{%- endif -%}\\n\"\n",
    "            \"User Question: {{ question }}\\n\"\n",
    "            \"Directly and succinctly answer the user's question.\\n\"\n",
    "            \"{%- if is_hallucination == 1 -%}\\n\"\n",
    "            \"Make up whatever information you need to in order to answer the user's request.\\n\"\n",
    "            \"{%- endif -%}\"\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9c108",
   "metadata": {},
   "source": [
    "## üîç Quality Assessment: LLM-as-a-Judge\n",
    "\n",
    "When generating our synthetic dataset, we need to determine the quality of the generated data \\\n",
    "We use the LLM-as-a-Judge strategy to do this.\n",
    "\n",
    "To do so, we need to define the rubric that the LLM should use to assess generation quality along with a prompt\n",
    "that provides relavant instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation rubrics for answer quality\n",
    "CompletenessRubric = Score(\n",
    "    name=\"Completeness\",\n",
    "    description=\"Evaluation of AI assistant's thoroughness in addressing all aspects of the user's query.\",\n",
    "    options={\n",
    "        \"Complete\": \"The response thoroughly covers all key points requested in the question, providing sufficient detail to satisfy the user's information needs.\",\n",
    "        \"PartiallyComplete\": \"The response addresses the core question but omits certain important details or fails to elaborate on relevant aspects that were requested.\",\n",
    "        \"Incomplete\": \"The response significantly lacks necessary information, missing major components of what was asked and leaving the query largely unanswered.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "AccuracyRubric = Score(\n",
    "    name=\"Accuracy\",\n",
    "    description=\"Evaluation of how factually correct the AI assistant's response is relative to the product information.\",\n",
    "    options={\n",
    "        \"Accurate\": \"The information provided aligns perfectly with the product specifications without introducing any misleading or incorrect details.\",\n",
    "        \"PartiallyAccurate\": \"While some information is correctly stated, the response contains minor factual errors or potentially misleading statements about the product.\",\n",
    "        \"Inaccurate\": \"The response presents significantly wrong information about the product, with claims that contradict the actual product details.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate answer quality\n",
    "config_builder.add_column(\n",
    "    LLMJudgeColumnConfig(\n",
    "        name=\"llm_answer_metrics\",\n",
    "        model_alias=model_alias,\n",
    "        prompt=(\n",
    "            \"<product_info>\\n\"\n",
    "            \"{{ product_info }}\\n\"\n",
    "            \"</product_info>\\n\"\n",
    "            \"User Question: {{question }}\\n\"\n",
    "            \"AI Assistant Answer: {{ answer }}\\n\"\n",
    "            \"Judge the AI assistant's response to the user's question about the product described in <product_info>.\"\n",
    "        ),\n",
    "        scores=[CompletenessRubric, AccuracyRubric],\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Extract metric scores for easier analysis\n",
    "config_builder.add_column(\n",
    "    ExpressionColumnConfig(\n",
    "        name=\"completeness_result\",\n",
    "        expr=\"{{ llm_answer_metrics.completeness.score }}\",\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    ExpressionColumnConfig(\n",
    "        name=\"accuracy_result\",\n",
    "        expr=\"{{ llm_answer_metrics.accuracy.score }}\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c605da",
   "metadata": {},
   "source": [
    "### üîÅ Iteration is key ‚Äì¬†preview the dataset!\n",
    "\n",
    "1. Use the `preview` method to generate a sample of records quickly.\n",
    "\n",
    "2. Inspect the results for quality and format issues.\n",
    "\n",
    "3. Adjust column configurations, prompts, or parameters as needed.\n",
    "\n",
    "4. Re-run the preview until satisfied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a few records\n",
    "preview = data_designer_client.preview(config_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28be08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More previews\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317248f8",
   "metadata": {},
   "source": [
    "### View rows generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57805bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = preview.dataset.copy()\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9e59d",
   "metadata": {},
   "source": [
    "### üìä Analyze the generated data\n",
    "\n",
    "- Data Designer automatically generates a basic statistical analysis of the generated data.\n",
    "\n",
    "- This analysis is available via the `analysis` property of generation result objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the analysis as a table.\n",
    "preview.analysis.to_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae79c08",
   "metadata": {},
   "source": [
    "### üÜô Scale up!\n",
    "\n",
    "- Happy with your preview data?\n",
    "\n",
    "- Use the `create` method to submit larger Data Designer generation jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374280bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_results = data_designer_client.create(config_builder, num_records=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60fc25",
   "metadata": {},
   "source": [
    "### View the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generated dataset as a pandas DataFrame.\n",
    "dataset = job_results.load_dataset()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0133c5",
   "metadata": {},
   "source": [
    "### View the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245cb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the analysis results into memory.\n",
    "analysis = job_results.load_analysis()\n",
    "\n",
    "analysis.to_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c75dc",
   "metadata": {},
   "source": [
    "### Output CSV to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7336dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Folder_Name = \"data-designer-tutorial-output\"\n",
    "File_Name = \"dataset_OR.csv\"\n",
    "\n",
    "TUTORIAL_OUTPUT_PATH = Path(Folder_Name)\n",
    "TUTORIAL_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset.to_csv(TUTORIAL_OUTPUT_PATH / File_Name, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

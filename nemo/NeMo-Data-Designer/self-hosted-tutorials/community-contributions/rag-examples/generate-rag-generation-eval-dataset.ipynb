{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667b36b1",
   "metadata": {},
   "source": [
    "# üé® NeMo Data Designer: Generate Diverse RAG Evaluations\n",
    "\n",
    "#### üìö What you'll learn\n",
    "\n",
    "This tutorial demonstrates how to generate comprehensive evaluation datasets for Retrieval-Augmented Generation (RAG) systems, customized to your content and use cases.\n",
    "\n",
    "<br>\n",
    "\n",
    "> üëã **IMPORTANT** ‚Äì¬†Environment Setup\n",
    ">\n",
    "> - If you haven't already, follow the instructions in the [README](../../../README.md) to install the necessary dependencies.\n",
    ">\n",
    "> - You may need to restart your notebook's kernel after setting up the environment.\n",
    "> - In this notebook, we assume you have a self-hosted instance of Data Designer up and running.\n",
    ">\n",
    "> - For deployment instructions, see the [Installation Options](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/index.html#installation-options) section of the [NeMo Data Designer documentation](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3f8fe",
   "metadata": {},
   "source": [
    "### üì¶ Import the essentials\n",
    "\n",
    "- The `data_designer` module of `nemo_microservices` exposes Data Designer's high-level SDK.\n",
    "\n",
    "- The `essentials` module provides quick access to the most commonly used objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ca1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_microservices.data_designer.essentials import (\n",
    "    CategorySamplerParams,\n",
    "    DataDesignerConfigBuilder,\n",
    "    ExpressionColumnConfig,\n",
    "    InferenceParameters,\n",
    "    LLMJudgeColumnConfig,\n",
    "    LLMStructuredColumnConfig,\n",
    "    ModelConfig,\n",
    "    NeMoDataDesignerClient,\n",
    "    SamplerColumnConfig,\n",
    "    SamplerType,\n",
    "    Score,\n",
    "    UniformSamplerParams,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55b35d",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Initialize the NeMo Data Designer Client\n",
    "\n",
    "- `NeMoDataDesignerClient` is responsible for submitting generation requests to the microservice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3706d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEMO_MICROSERVICES_BASE_URL = \"http://localhost:8080\"\n",
    "\n",
    "data_designer_client = NeMoDataDesignerClient(base_url=NEMO_MICROSERVICES_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6839a60",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Define model configurations\n",
    "\n",
    "- Each `ModelConfig` defines a model that can be used during the generation process.\n",
    "\n",
    "- The \"model alias\" is used to reference the model in the Data Designer config (as we will see below).\n",
    "\n",
    "- The \"model provider\" is the external service that hosts the model (see [the model config docs](https://docs.nvidia.com/nemo/microservices/latest/design-synthetic-data-from-scratch-or-seeds/configure-models.html) for more details).\n",
    "\n",
    "- By default, the microservice uses [build.nvidia.com](https://build.nvidia.com/models) as the model provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This name is set in the microservice deployment configuration.\n",
    "MODEL_PROVIDER = \"nvidiabuild\"\n",
    "\n",
    "# The model ID is from build.nvidia.com.\n",
    "MODEL_ID = \"nvidia/nvidia-nemotron-nano-9b-v2\"\n",
    "\n",
    "# We choose this alias to be descriptive for our use case.\n",
    "MODEL_ALIAS = \"nemotron-nano-v2\"\n",
    "\n",
    "# This sets reasoning to False for the nemotron-nano-v2 model.\n",
    "SYSTEM_PROMPT = \"/no_think\"\n",
    "\n",
    "model_configs = [\n",
    "    ModelConfig(\n",
    "        alias=MODEL_ALIAS,\n",
    "        model=MODEL_ID,\n",
    "        provider=MODEL_PROVIDER,\n",
    "        inference_parameters=InferenceParameters(\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            max_tokens=1024,\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5dde1a",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Initialize the Data Designer Config Builder\n",
    "\n",
    "- The Data Designer config defines the dataset schema and generation process.\n",
    "\n",
    "- The config builder provides an intuitive interface for building this configuration.\n",
    "\n",
    "- The list of model configs is provided to the builder at initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceafed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = DataDesignerConfigBuilder(model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5e591",
   "metadata": {},
   "source": [
    "## üå± Loading Seed Data\n",
    "\n",
    "- We'll use the symptom-to-diagnosis dataset as our seed data.\n",
    "\n",
    "- This dataset contains patient symptoms and corresponding diagnoses which will help generate realistic medical scenarios.\n",
    "\n",
    "<br>\n",
    "\n",
    "> üå± **Why use a seed dataset?**\n",
    ">\n",
    "> - Seed datasets let you steer the generation process by providing context that is specific to your use case.\n",
    ">\n",
    "> - Seed datasets are also an excellent way to inject real-world diversity into your synthetic data.\n",
    ">\n",
    "> - During generation, prompt templates can reference any of the seed dataset fields.\n",
    "\n",
    "<br>\n",
    "\n",
    "> üí° **About datastores**\n",
    ">\n",
    "> - You can use seed datasets from _either_ the Hugging Face Hub or a locally deployed datastore.\n",
    ">\n",
    "> - By default, we use the local datastore deployed with the Data Designer microservice.\n",
    ">\n",
    "> - The datastore endpoint is specified in the deployment configuration.\n",
    "\n",
    "üëã **Note**: At this time, we only support using a single file as the seed. If you have multiple files you would like to use as \\\n",
    "seeds, it is recommended you consolidated these into a single file.\n",
    "<br>\n",
    "\n",
    "### ‚öôÔ∏è Document Processing\n",
    "\n",
    "Now we'll create a Document Processor class that handles loading and chunking the source documents.\n",
    "\n",
    "This class uses langchain's RecursiveCharacterTextSplitter and unstructured.io for robust document parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from unstructured.partition.auto import partition\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Handles loading and chunking source documents for RAG evaluation.\"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int = 4192, chunk_overlap: int = 200):\n",
    "        \"\"\"Initialize with configurable chunk size and overlap.\"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "    def parse_document(self, uri: str) -> str:\n",
    "        \"\"\"Parse a single document from URI into raw text.\"\"\"\n",
    "        with open(uri, \"rb\") as file:\n",
    "            content = file.read()\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                temp_file.write(content)\n",
    "                temp_file.flush()\n",
    "                elements = partition(temp_file.name)\n",
    "\n",
    "        os.unlink(temp_file.name)\n",
    "        return \"\\n\\n\".join([str(element) for element in elements])\n",
    "\n",
    "    def process_documents(self, uris: Union[str, List[str]]) -> List[str]:\n",
    "        \"\"\"Process one or more documents into chunks for RAG evaluation.\"\"\"\n",
    "        if isinstance(uris, str):\n",
    "            uris = [uris]\n",
    "\n",
    "        all_chunks = []\n",
    "        for uri in uris:\n",
    "            text = self.parse_document(uri)\n",
    "            chunks = self.text_splitter.split_text(text)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44785c",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Data Models\n",
    "\n",
    "- Let's define Pydantic models for structured output generation.\n",
    "\n",
    "- These schemas will ensure our generated data has consistent structure and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class QAPair(BaseModel):\n",
    "    question: str = Field(\n",
    "        ..., description=\"A specific question related to the domain of the context\"\n",
    "    )\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"Either a context-supported answer or explanation of why the question cannot be answered\",\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"A clear and traceable explanation of the reasoning behind the answer\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Process document chunks\n",
    "DOCUMENT_LIST = [\"./data/databricks-state-of-data-ai-report.pdf\"]\n",
    "\n",
    "processor = DocumentProcessor(chunk_size=4192, chunk_overlap=200)\n",
    "chunks = processor.process_documents(DOCUMENT_LIST)\n",
    "\n",
    "# Create a seed DataFrame with the document chunks\n",
    "seed_df = pd.DataFrame({\"context\": chunks})\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "seed_df.to_csv(\"data/document_chunks.csv\", index=False)\n",
    "\n",
    "seed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8152fe1e-6d47-435d-91c1-85a6d17b1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reference = data_designer_client.upload_seed_dataset(\n",
    "    repo_id=\"data-designer-demo/rag-evaluation-dataset\",\n",
    "    dataset=seed_df,\n",
    "    datastore_settings={\"endpoint\": \"http://localhost:3000/v1/hf\"},\n",
    ")\n",
    "\n",
    "config_builder.with_seed_dataset(dataset_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e2fec",
   "metadata": {},
   "source": [
    "## üé≤ Adding Categorical Columns for Controlled Diversity\n",
    "\n",
    "Now we'll add categorical columns to control the diversity of our RAG evaluation pairs. We'll define:\n",
    "\n",
    "1. **Difficulty levels**: easy, medium, hard\n",
    "\n",
    "2. **Reasoning types**: factual recall, inferential reasoning, etc.\n",
    "\n",
    "3. **Question types**: answerable vs. unanswerable (with weighting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e27cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure categorical columns for controlled diversity\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"difficulty\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(\n",
    "            values=[\"easy\", \"medium\", \"hard\"],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"reasoning_type\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(\n",
    "            values=[\n",
    "                \"factual recall\",\n",
    "                \"inferential reasoning\",\n",
    "                \"comparative analysis\",\n",
    "                \"procedural understanding\",\n",
    "                \"cause and effect\",\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"question_type\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(\n",
    "            values=[\"answerable\", \"unanswerable\"],\n",
    "            # 10:1 ratio of answerable to unanswerable questions.\n",
    "            weights=[10, 1],\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735cbbea",
   "metadata": {},
   "source": [
    "## ü¶ú Adding LLM-Structured Column for Q&A Pair Generation\n",
    "\n",
    "Now let's set up the core of our data generation: the Q&A pair column that will produce structured question-answer \\\n",
    "pairs based on our document context and control parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf44d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Q&A pair generation column\n",
    "config_builder.add_column(\n",
    "    LLMStructuredColumnConfig(\n",
    "        name=\"qa_pair\",\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        prompt=(\n",
    "            \"{{context}}\\n\"\n",
    "            \"\\n\"\n",
    "            \"Generate a {{difficulty}} {{reasoning_type}} question-answer pair.\\n\"\n",
    "            \"The question should be {{question_type}} using the provided context.\\n\"\n",
    "            \"\\n\"\n",
    "            \"For answerable questions:\\n\"\n",
    "            \"- Ensure the answer is fully supported by the context\\n\"\n",
    "            \"\\n\"\n",
    "            \"For unanswerable questions:\\n\"\n",
    "            \"- Keep the question topically relevant\\n\"\n",
    "            \"- Make it clearly beyond the context's scope\\n\"\n",
    "        ),\n",
    "        output_format=QAPair,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7058d0a",
   "metadata": {},
   "source": [
    "## üîç Quality Assessment: LLM-as-a-Judge\n",
    "\n",
    "When generating our synthetic dataset, we need to determine the quality of the generated data \\\n",
    "We use the LLM-as-a-Judge strategy to do this.\n",
    "\n",
    "To do so, we need to define the rubric that the LLM should use to assess generation quality along with a prompt\n",
    "that provides relavant instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevance_rubric = Score(\n",
    "    name=\"Context Relevance\",\n",
    "    description=\"Evaluates how relevant the answer is to the provided context\",\n",
    "    options={\n",
    "        \"5\": \"Perfect relevance to context with no extraneous information\",\n",
    "        \"4\": \"Highly relevant with minor deviations from context\",\n",
    "        \"3\": \"Moderately relevant but includes some unrelated information\",\n",
    "        \"2\": \"Minimally relevant with significant departure from context\",\n",
    "        \"1\": \"Almost entirely irrelevant to the provided context\",\n",
    "    },\n",
    ")\n",
    "\n",
    "answer_precision_rubric = Score(\n",
    "    name=\"Answer Precision\",\n",
    "    description=\"Evaluates the accuracy and specificity of the answer\",\n",
    "    options={\n",
    "        \"5\": \"Extremely precise with exact, specific information\",\n",
    "        \"4\": \"Very precise with minor imprecisions\",\n",
    "        \"3\": \"Adequately precise but could be more specific\",\n",
    "        \"2\": \"Imprecise with vague or ambiguous information\",\n",
    "        \"1\": \"Completely imprecise or inaccurate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "answer_completeness_rubric = Score(\n",
    "    name=\"Answer Completeness\",\n",
    "    description=\"Evaluates how thoroughly the answer addresses all aspects of the question\",\n",
    "    options={\n",
    "        \"5\": \"Fully complete, addressing all aspects of the question\",\n",
    "        \"4\": \"Mostly complete with minor omissions\",\n",
    "        \"3\": \"Adequately complete but missing some details\",\n",
    "        \"2\": \"Substantially incomplete, missing important aspects\",\n",
    "        \"1\": \"Severely incomplete, barely addresses the question\",\n",
    "    },\n",
    ")\n",
    "\n",
    "hallucination_avoidance_rubric = Score(\n",
    "    name=\"Hallucination Avoidance\",\n",
    "    description=\"Evaluates the absence of made-up or incorrect information\",\n",
    "    options={\n",
    "        \"5\": \"No hallucinations, all information is factual and verifiable\",\n",
    "        \"4\": \"Minimal hallucinations that don't impact the core answer\",\n",
    "        \"3\": \"Some hallucinations that partially affect the answer quality\",\n",
    "        \"2\": \"Significant hallucinations that undermine the answer\",\n",
    "        \"1\": \"Severe hallucinations making the answer entirely unreliable\",\n",
    "    },\n",
    ")\n",
    "\n",
    "EVAL_METRICS_PROMPT_TEMPLATE = (\n",
    "    \"You are an expert evaluator of question-answer pairs. Analyze the following Q&A pair and evaluate it objectively.\\n\\n\"\n",
    "    \"For this {{difficulty}} {{reasoning_type}} Q&A pair:\\n\"\n",
    "    \"{{qa_pair}}\\n\\n\"\n",
    "    \"Take a deep breath and carefully evaluate each criterion based on the provided rubrics, considering the \"\n",
    "    \"difficulty level and reasoning type indicated.\"\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    LLMJudgeColumnConfig(\n",
    "        name=\"eval_metrics\",\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        prompt=EVAL_METRICS_PROMPT_TEMPLATE,\n",
    "        scores=[\n",
    "            context_relevance_rubric,\n",
    "            answer_precision_rubric,\n",
    "            answer_completeness_rubric,\n",
    "            hallucination_avoidance_rubric,\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff88d0",
   "metadata": {},
   "source": [
    "### üîÅ Iteration is key ‚Äì¬†preview the dataset!\n",
    "\n",
    "1. Use the `preview` method to generate a sample of records quickly.\n",
    "\n",
    "2. Inspect the results for quality and format issues.\n",
    "\n",
    "3. Adjust column configurations, prompts, or parameters as needed.\n",
    "\n",
    "4. Re-run the preview until satisfied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a few records\n",
    "preview = data_designer_client.preview(config_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More previews\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd025b",
   "metadata": {},
   "source": [
    "### üìä Analyze the generated data\n",
    "\n",
    "- Data Designer automatically generates a basic statistical analysis of the generated data.\n",
    "\n",
    "- This analysis is available via the `analysis` property of generation result objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79185c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the analysis as a table.\n",
    "preview.analysis.to_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fc118",
   "metadata": {},
   "source": [
    "### üÜô Scale up!\n",
    "\n",
    "- Happy with your preview data?\n",
    "\n",
    "- Use the `create` method to submit larger Data Designer generation jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_results = data_designer_client.create(config_builder, num_records=20)\n",
    "\n",
    "# This will block until the job is complete.\n",
    "job_results.wait_until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa51230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generated dataset as a pandas DataFrame.\n",
    "dataset = job_results.load_dataset()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the analysis results into memory.\n",
    "analysis = job_results.load_analysis()\n",
    "\n",
    "analysis.to_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUTORIAL_OUTPUT_PATH = \"data-designer-tutorial-output\"\n",
    "\n",
    "# Download the job artifacts and save them to disk.\n",
    "job_results.download_artifacts(\n",
    "    output_path=TUTORIAL_OUTPUT_PATH,\n",
    "    artifacts_folder_name=\"artifacts-community-contributions-rag-examples-generate-rag-generation-eval-dataset\",\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sdg_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

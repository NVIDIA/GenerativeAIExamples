{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎨 NeMo Data Designer: Visual Question Answering Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Warning**: NeMo Data Designer is current in Early Release and is not recommended for production use.\n",
    ">\n",
    "> **Note**: In order to run this notebook, you must have the NeMo Data Designer microservice deployed locally via docker compose. See the [deployment guide](http://docs.nvidia.com/nemo/microservices/latest/set-up/deploy-as-microservices/data-designer/docker-compose.html) for more details.\n",
    ">\n",
    "> Alternatively, you can use the [NeMo Data Designer managed service](https://build.nvidia.com/nemo/data-designer). Please refer the [intro-tutorials](../../intro-tutorials/1-the-basics.ipynb) on how to connect to it. \n",
    ">\n",
    "> **Note**: If you are using the NeMo Data Designer managed service, you will only be able to launch preview jobs. You will not be able to launch jobs using the `create` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use NeMo Data Designer to generate high-quality synthetic Question-Answer datasets from visual documents. \n",
    "\n",
    "### Key Features Demonstrated\n",
    "\n",
    "- ✨ **Visual Document Processing**: Converting images to chat-ready format\n",
    "- 🏗️ **Structured Output Generation**: Using Pydantic models for consistent data schemas\n",
    "- 🎯 **Multi-step Generation Pipeline**: Summary → Question → Answer generation workflow\n",
    "- 🔄 **Iterative Development**: Preview functionality for rapid iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 💾 Install dependencies\n",
    "\n",
    "**IMPORTANT** 👉 If you haven't already, follow the instructions in the [README](../../README.md) to install the necessary dependencies. Note you may need to restart your kernel after setting up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import uuid\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import rich\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# NeMo Data Designer imports\n",
    "from nemo_microservices import NeMoMicroservices\n",
    "from nemo_microservices.beta.data_designer import (\n",
    "    DataDesignerConfigBuilder,\n",
    "    DataDesignerClient\n",
    ")\n",
    "from nemo_microservices.beta.data_designer.config import columns as C\n",
    "from nemo_microservices.beta.data_designer.config import params as P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Initialize the NeMo Data Designer Client\n",
    "\n",
    "- The data designer client is responsible for submitting generation requests to the Data Designer microservice.\n",
    "- In this notebook, we connect to a local deployment of data designer. You can deploy your own instance of data designer by following the deployment instructions [here](https://docs.nvidia.com/nemo/microservices/latest/set-up/deploy-as-microservices/data-designer/docker-compose.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_designer_client = DataDesignerClient(client=NeMoMicroservices(base_url=\"http://localhost:8080\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏗️ Initialize the Data Designer Config Builder\n",
    "\n",
    "- The Data Designer config defines the dataset schema and generation process.\n",
    "\n",
    "- The config builder provides an intuitive interface for building this configuration.\n",
    "\n",
    "- You must provide a list of model configs to the builder at initialization.\n",
    "\n",
    "- This list contains the models you can choose from (via the `model_alias` argument) during the generation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify the endpoint of the model during deployment using the model_provider_registry.\n",
    "model_id = \"meta/llama-4-maverick-17b-128e-instruct\"\n",
    "model_alias = \"llama-4-maverick-17b-128e-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = DataDesignerConfigBuilder(\n",
    "    model_configs=[\n",
    "        P.ModelConfig(\n",
    "            alias=model_alias,\n",
    "            provider=\"nvidiabuild\",\n",
    "            model=model_id,\n",
    "            inference_parameters=P.InferenceParameters(\n",
    "                max_tokens=1024,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "            ),\n",
    "            is_reasoner=False\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌱 Seed Dataset Creation\n",
    "\n",
    "In this section, we'll prepare our visual documents as a seed dataset. The seed dataset provides the foundation for synthetic data generation by:\n",
    "\n",
    "- **Loading Visual Documents**: We use the ColPali dataset containing document images\n",
    "- **Image Processing**: Convert images to base64 format for model consumption  \n",
    "- **Metadata Extraction**: Preserve relevant document information\n",
    "- **Sampling Strategy**: Configure how the seed data is utilized during generation\n",
    "\n",
    "The seed dataset can be referenced in generation prompts using Jinja templating.\n",
    "\n",
    "**Note**: At this time, we only support using a single file as the seed. If you have multiple files you would like to use as seeds, it is recommended you consolidated these into a single file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset processing configuration\n",
    "IMG_COUNT = 512  # Number of images to process\n",
    "BASE64_IMAGE_HEIGHT = 512  # Standardized height for model input\n",
    "\n",
    "# Load ColPali dataset for visual documents\n",
    "img_dataset_cfg = {\n",
    "    \"path\": \"vidore/colpali_train_set\",\n",
    "    \"split\": \"train\",\n",
    "    \"streaming\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, height: int):\n",
    "    \"\"\"\n",
    "    Resize image while maintaining aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        height: Target height in pixels\n",
    "\n",
    "    Returns:\n",
    "        Resized PIL Image object\n",
    "    \"\"\"\n",
    "    original_width, original_height = image.size\n",
    "    width = int(original_width * (height / original_height))\n",
    "    return image.resize((width, height))\n",
    "\n",
    "def convert_image_to_chat_format(record, height: int) -> dict:\n",
    "    \"\"\"\n",
    "    Convert PIL image to base64 format for chat template usage.\n",
    "\n",
    "    Args:\n",
    "        record: Dataset record containing image and metadata\n",
    "        height: Target height for image resizing\n",
    "\n",
    "    Returns:\n",
    "        Updated record with base64_image and uuid fields\n",
    "    \"\"\"\n",
    "    # Resize image for consistent processing\n",
    "    image = resize_image(record[\"image\"], height)\n",
    "\n",
    "    # Convert to base64 string\n",
    "    img_buffer = io.BytesIO()\n",
    "    image.save(img_buffer, format=\"PNG\")\n",
    "    byte_data = img_buffer.getvalue()\n",
    "    base64_encoded_data = base64.b64encode(byte_data)\n",
    "    base64_string = base64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "    # Return updated record\n",
    "    return record | {\n",
    "        \"base64_image\": base64_string,\n",
    "        \"uuid\": str(uuid.uuid4())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the visual document dataset\n",
    "print(\"📥 Loading and processing document images...\")\n",
    "\n",
    "img_dataset_iter = iter(\n",
    "    load_dataset(**img_dataset_cfg)\n",
    "    .map(convert_image_to_chat_format, fn_kwargs={\"height\": BASE64_IMAGE_HEIGHT})\n",
    ")\n",
    "img_dataset = pd.DataFrame([next(img_dataset_iter) for _ in range(IMG_COUNT)])\n",
    "\n",
    "print(f\"✅ Loaded {len(img_dataset)} images with columns: {list(img_dataset.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data/\", exist_ok=True)\n",
    "\n",
    "df_seed = pd.DataFrame(img_dataset)[[\"uuid\", \"image_filename\", \"base64_image\", \"page\", \"options\", \"source\"]]\n",
    "df_seed.to_csv(\"./data/colpali_train_set.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the seed dataset containing our processed images\n",
    "config_builder.with_seed_dataset(\n",
    "    repo_id=\"advanced/visual-qna\",\n",
    "    filename=\"colpali_train_set.csv\",\n",
    "    dataset_path=\"./data/colpali_train_set.csv\",\n",
    "    sampling_strategy=\"ordered\",\n",
    "    with_replacement=True,\n",
    "    datastore={\"endpoint\": \"http://localhost:3000/v1/hf\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to generate detailed document summaries\n",
    "config_builder.add_column(\n",
    "    name=\"summary\",\n",
    "    type=\"llm-code\",\n",
    "    model_alias=model_alias,\n",
    "    prompt=(\"Provide a detailed summary of the content in this image in Markdown format.\"\n",
    "            \"Start from the top of the image and then describe it from top to bottom.\"\n",
    "            \"Place a summary at the bottom.\"),\n",
    "    output_format=\"markdown\",\n",
    "    multi_modal_context=[\n",
    "        P.ImageContext(\n",
    "            column_name=\"base64_image\",\n",
    "            data_type=P.ModalityDataType.BASE64,\n",
    "            image_format=P.ImageFormat.PNG,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎨 Designing our Data Schema\n",
    "\n",
    "Structured outputs ensure consistent and predictable data generation. Data Designer supports schemas defined using:\n",
    "- **JSON Schema**: For basic structure definition\n",
    "- **Pydantic Models**: For advanced validation and type safety (recommended)\n",
    "\n",
    "We'll use Pydantic models to define our Question-Answer schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    \"\"\"Schema for generated questions\"\"\"\n",
    "    question: str = Field(description=\"The question to be generated\")\n",
    "\n",
    "class QuestionTopic(BaseModel):\n",
    "    \"\"\"Schema for question topics\"\"\"\n",
    "    topic: str = Field(description=\"The topic/category of the question\")\n",
    "\n",
    "class Options(BaseModel):\n",
    "    \"\"\"Schema for multiple choice options\"\"\"\n",
    "    option_a: str = Field(description=\"The first answer choice\")\n",
    "    option_b: str = Field(description=\"The second answer choice\")\n",
    "    option_c: str = Field(description=\"The third answer choice\")\n",
    "    option_d: str = Field(description=\"The fourth answer choice\")\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Schema for question answers\"\"\"\n",
    "    answer: Literal[\"option_a\", \"option_b\", \"option_c\", \"option_d\"] = Field(description=\"The correct answer to the question\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    C.SamplerColumn(\n",
    "        name=\"difficulty\",\n",
    "        type=P.SamplerType.CATEGORY,\n",
    "        params=P.CategorySamplerParams(values=[\"easy\", \"medium\", \"hard\"]),\n",
    "        description=\"The difficulty of the generated question\",\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    C.LLMStructuredColumn(\n",
    "        name=\"question\",\n",
    "        model_alias=model_alias,\n",
    "        prompt=(\"Generate a question based on the following context: {{ summary }}. \"\n",
    "        \"The difficulty of the generated question should be {{ difficulty }}\"),\n",
    "        system_prompt=(\"You are a helpful assistant that generates questions based on the given context. \"\n",
    "        \"The context are sourced from documents pertaining to the petroleum industry. \"\n",
    "        \"You will be given a context and you will need to generate a question based on the context. \"\n",
    "        \"The difficulty of the generated question should be {{ difficulty }}\"\n",
    "        \"Ensure you generate just the question and no other text.\"),\n",
    "        output_format=Question,\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    C.LLMStructuredColumn(\n",
    "        name=\"options\",\n",
    "        model_alias=model_alias,\n",
    "        prompt=(\"Generate four answer choices for the question: {{ question }} based on the following context: {{ summary }}. \"\n",
    "        \"The option you generate should match the difficulty of the generated question, {{ difficulty }}.\"),\n",
    "        output_format=Options,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "config_builder.add_column(\n",
    "    C.LLMStructuredColumn(\n",
    "        name=\"answer\",\n",
    "        prompt=(\"Choose the correct answer for the question: {{ question }} based on the following context: {{ summary }}\"\n",
    "                \"and options choices. The options are {{ options }}. Only select one of the options as the answer.\"),\n",
    "        output_format=Answer,\n",
    "        model_alias=model_alias,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "config_builder.add_column(\n",
    "    C.LLMStructuredColumn(\n",
    "        name=\"topic\",\n",
    "        model_alias=model_alias,\n",
    "        prompt=(\"Generate the topic of the question: {{ question }} based on the following context: {{ summary }}\"\n",
    "        \"The topic should be a single word or phrase that is relevant to the question and context. \"),\n",
    "        system_prompt=(\"Generate a short 1-3 word topic for the question: {{ question }} based on the given context. {{ summary }}\"),\n",
    "        output_format=QuestionTopic,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 👀 Preview Generation\n",
    "\n",
    "Before scaling up, it's crucial to validate your configuration with a small sample. The preview functionality:\n",
    "\n",
    "- **Generates Sample Data**: Creates 10 records for quick inspection\n",
    "- **Enables Rapid Iteration**: Test and refine your prompts and schemas\n",
    "- **Provides Detailed Logging**: Understand the generation process with verbose output\n",
    "\n",
    "Use this step to fine-tune your configuration before full-scale generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Please ignore the validation warning, `PROMPT_WITHOUT_REFERENCES` that shows up. The image context is being passed to the LLM using the `multi_modal_context` and so the prompt does not need to reference any other column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = data_designer_client.preview(config_builder, verbose_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample record from the preview\n",
    "# Run this cell multiple times to cycle through different records\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preview dataset is available as a pandas DataFrame.\n",
    "preview.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original document with generated outputs\n",
    "index = 0  # Change this to view different examples\n",
    "\n",
    "# Merge preview data with original images for comparison\n",
    "comparison_dataset = preview.dataset.merge(\n",
    "    pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]],\n",
    "    how=\"left\",\n",
    "    on=\"uuid\"\n",
    ")\n",
    "\n",
    "print(\"📄 Original Document Image:\")\n",
    "display(resize_image(comparison_dataset.image[index], BASE64_IMAGE_HEIGHT))\n",
    "\n",
    "print(\"\\n📝 Generated Summary:\")\n",
    "rich.print(Panel(comparison_dataset.summary[index], title=\"Document Summary\", title_align=\"left\"))\n",
    "\n",
    "print(\"\\n❓ Generated Question:\")\n",
    "rich.print(Panel(comparison_dataset.question[index], title=\"Question\", title_align=\"left\"))\n",
    "\n",
    "print(\"\\n🔢 Generated Options:\")\n",
    "rich.print(Panel(comparison_dataset.options[index], title=\"Answer Choices\", title_align=\"left\"))\n",
    "\n",
    "print(\"\\n✅ Generated Answer:\")\n",
    "rich.print(Panel(comparison_dataset.answer[index], title=\"Correct Answer\", title_align=\"left\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 Scale Up Generations\n",
    "\n",
    "Once satisfied with the preview results, scale up to generate the full dataset. The generation process offers flexible execution modes:\n",
    "\n",
    "#### Synchronous Generation\n",
    "Set `wait_until_done=True` to block until completion - ideal for smaller datasets or interactive workflows.\n",
    "\n",
    "#### Asynchronous Generation  \n",
    "Set `wait_until_done=False` for batch processing - returns a `job_id` for later retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_results = data_designer_client.create(config_builder, num_records=1, wait_until_done=False)\n",
    "\n",
    "job_results.wait_until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset into a pandas DataFrame\n",
    "dataset = job_results.load_dataset()\n",
    "\n",
    "print(f\"Generated {len(dataset)} records\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎 View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original document with generated outputs\n",
    "index = 0  # Change this to view different examples\n",
    "\n",
    "# Merge preview data with original images for comparison\n",
    "comparison_dataset = dataset.merge(\n",
    "    pd.DataFrame(img_dataset)[[\"uuid\", \"image\"]],\n",
    "    how=\"left\",\n",
    "    on=\"uuid\"\n",
    ")\n",
    "\n",
    "print(\"📄 Original Document Image:\")\n",
    "display(resize_image(comparison_dataset.image[index], BASE64_IMAGE_HEIGHT))\n",
    "\n",
    "print(\"\\n📝 Generated Summary:\")\n",
    "rich.print(Panel(comparison_dataset.summary[index], title=\"Document Summary\", title_align=\"left\"))\n",
    "\n",
    "print(\"\\n❓ Generated Question:\")\n",
    "rich.print(Panel(comparison_dataset.question[index], title=\"Question\", title_align=\"left\"))\n",
    "\n",
    "# print(\"\\n🔢 Generated Options:\")\n",
    "# rich.print(Panel(comparison_dataset.options[index], title=\"Answer Choices\", title_align=\"left\"))\n",
    "\n",
    "print(\"\\n✅ Generated Answer:\")\n",
    "rich.print(Panel(comparison_dataset.answer[index], title=\"Correct Answer\", title_align=\"left\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdg_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

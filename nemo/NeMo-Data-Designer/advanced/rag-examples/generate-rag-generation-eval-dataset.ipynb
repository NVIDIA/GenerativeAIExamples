{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 🎨 NeMo Data Designer: Generate Diverse RAG Evaluations"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e04cb1b",
            "metadata": {},
            "source": [
                "> ⚠️ **Warning**: NeMo Data Designer is current in Early Release and is not recommended for production use.\n",
                ">\n",
                "> **Note**: In order to run this notebook, you must have the NeMo Data Designer microservice deployed locally via docker compose. See the [deployment guide](http://docs.nvidia.com/nemo/microservices/latest/set-up/deploy-as-microservices/data-designer/docker-compose.html) for more details.\n",
                ">\n",
                "> Alternatively, you can use the [NeMo Data Designer managed service](https://build.nvidia.com/nemo/data-designer). Please refer the [intro-tutorials](../../intro-tutorials/1-the-basics.ipynb) on how to connect to it. \n",
                ">\n",
                "> **Note**: If you are using the NeMo Data Designer managed service, you will only be able to launch preview jobs. You will not be able to launch jobs using the `create` method."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "118d856f",
            "metadata": {},
            "source": [
                "This tutorial demonstrates how to generate comprehensive evaluation datasets for Retrieval-Augmented Generation (RAG) systems, customized to your content and use cases. \n",
                "\n",
                "You'll learn how to create diverse question-answer pairs at scale, covering a variety of difficulty levels and reasoning types, including both answerable and unanswerable scenarios.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f56b0d2",
            "metadata": {},
            "source": [
                "\n",
                "#### 💾 Install dependencies\n",
                "\n",
                "**IMPORTANT** 👉 If you haven't already, follow the instructions in the [README](../../README.md) to install the necessary dependencies. Note you may need to restart your kernel after setting up the environment.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e89ca1ab",
            "metadata": {},
            "outputs": [],
            "source": [
                "from nemo_microservices import NeMoMicroservices\n",
                "from nemo_microservices.beta.data_designer import (\n",
                "    DataDesignerConfigBuilder,\n",
                "    DataDesignerClient,\n",
                ")\n",
                "from nemo_microservices.beta.data_designer.config import columns as C\n",
                "from nemo_microservices.beta.data_designer.config import params as P"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7601ded0",
            "metadata": {},
            "source": [
                "### ⚙️ Initialize the NeMo Data Designer Client\n",
                "\n",
                "- The data designer client is responsible for submitting generation requests to the Data Designer microservice.\n",
                "- In this notebook, we connect to a local deployment of data designer. You can deploy your own instance of data designer by following the deployment instructions [here](https://docs.nvidia.com/nemo/microservices/latest/set-up/deploy-as-microservices/data-designer/docker-compose.html).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3b6dc059",
            "metadata": {},
            "outputs": [],
            "source": [
                "data_designer_client = DataDesignerClient(client=NeMoMicroservices(base_url=\"http://localhost:8080\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "277f1aaf",
            "metadata": {},
            "source": [
                "### 🏗️ Initialize the Data Designer Config Builder\n",
                "\n",
                "- The Data Designer config defines the dataset schema and generation process.\n",
                "\n",
                "- The config builder provides an intuitive interface for building this configuration.\n",
                "\n",
                "- You must provide a list of model configs to the builder at initialization.\n",
                "\n",
                "- This list contains the models you can choose from (via the `model_alias` argument) during the generation process.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "259ec052",
            "metadata": {},
            "outputs": [],
            "source": [
                "# We specify the endpoint of the model during deployment using the model_provider_registry.\n",
                "model_id = \"/raid/models/nemotron-nano-9b-v2\"\n",
                "# model_id = \"nvidia/nvidia-nemotron-nano-9b-v2\"\n",
                "model_alias = \"nemotron-nano-9b-v2\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "77b59e8e",
            "metadata": {},
            "outputs": [],
            "source": [
                "config_builder = DataDesignerConfigBuilder(\n",
                "    model_configs=[\n",
                "        P.ModelConfig(\n",
                "            alias=model_alias,\n",
                "            provider=\"local-llm\",\n",
                "            model=model_id,\n",
                "            inference_parameters=P.InferenceParameters(\n",
                "                max_tokens=1024,\n",
                "                temperature=0.6,\n",
                "                top_p=0.95,\n",
                "            ),\n",
                "            is_reasoner=True\n",
                "        ),\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e78f8070",
            "metadata": {},
            "source": [
                "### 📕 Source Document Configuration\n",
                "\n",
                "Let's define our source documents and the total number of evaluation pairs we want to generate. You can replace the document list with your own PDFs, web pages, or other text sources."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd6f9e64",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define source documents and total number of evaluation pairs to generate\n",
                "# You can replace this with your own documents\n",
                "DOCUMENT_LIST = [\"./data/databricks-state-of-data-ai-report.pdf\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e0c98449",
            "metadata": {},
            "source": [
                "### ⚙️ Document Processing\n",
                "\n",
                "Now we'll create a Document Processor class that handles loading and chunking the source documents. \n",
                "\n",
                "This class uses langchain's RecursiveCharacterTextSplitter and unstructured.io for robust document parsing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bfec3608",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List, Union\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "from unstructured.partition.auto import partition\n",
                "import tempfile\n",
                "import os"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "38ec64d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "class DocumentProcessor:\n",
                "    \"\"\"Handles loading and chunking source documents for RAG evaluation.\"\"\"\n",
                "\n",
                "    def __init__(self, chunk_size: int = 4192, chunk_overlap: int = 200):\n",
                "        \"\"\"Initialize with configurable chunk size and overlap.\"\"\"\n",
                "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
                "            chunk_size=chunk_size,\n",
                "            chunk_overlap=chunk_overlap,\n",
                "            length_function=len,\n",
                "        )\n",
                "\n",
                "    def parse_document(self, uri: str) -> str:\n",
                "        \"\"\"Parse a single document from URI into raw text.\"\"\"\n",
                "        with open(uri, 'rb') as file:\n",
                "            content = file.read()\n",
                "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
                "                temp_file.write(content)\n",
                "                temp_file.flush()\n",
                "                elements = partition(temp_file.name)\n",
                "\n",
                "        os.unlink(temp_file.name)\n",
                "        return \"\\n\\n\".join([str(element) for element in elements])\n",
                "\n",
                "    def process_documents(self, uris: Union[str, List[str]]) -> List[str]:\n",
                "        \"\"\"Process one or more documents into chunks for RAG evaluation.\"\"\"\n",
                "        if isinstance(uris, str):\n",
                "            uris = [uris]\n",
                "\n",
                "        all_chunks = []\n",
                "        for uri in uris:\n",
                "            text = self.parse_document(uri)\n",
                "            chunks = self.text_splitter.split_text(text)\n",
                "            all_chunks.extend(chunks)\n",
                "\n",
                "        return all_chunks"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c44785c",
            "metadata": {},
            "source": [
                "### Data Models\n",
                "\n",
                "Let's define Pydantic models for structured output generation. These schemas will ensure our generated data has consistent structure and validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9cab035f",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field\n",
                "\n",
                "class QAPair(BaseModel):\n",
                "    question: str = Field(\n",
                "        ..., description=\"A specific question related to the domain of the context\"\n",
                "    )\n",
                "    answer: str = Field(\n",
                "        ..., description=\"Either a context-supported answer or explanation of why the question cannot be answered\"\n",
                "    )\n",
                "    reasoning: str = Field(\n",
                "        ..., description=\"A clear and traceable explanation of the reasoning behind the answer\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ada29f90",
            "metadata": {},
            "source": [
                "### Processing Documents and Setting Up Data Designer\n",
                "\n",
                "Now we'll process our document chunks and set up the Data Designer with our seed dataset.\n",
                "\n",
                "**Note**: At this time, we only support using a single file as the seed. If you have multiple files you would like to use as seeds, it is recommended you consolidated these into a single file. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5325b303",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Process document chunks\n",
                "processor = DocumentProcessor(chunk_size=4192, chunk_overlap=200)\n",
                "chunks = processor.process_documents(DOCUMENT_LIST)\n",
                "\n",
                "# Create a seed DataFrame with the document chunks\n",
                "seed_df = pd.DataFrame({\"context\": chunks})\n",
                "seed_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0bfa504d",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(\"data\", exist_ok=True)\n",
                "seed_df.to_csv(\"data/document_chunks.csv\", index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f09836d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload the seed dataset with document chunks\n",
                "# Using shuffle with replacement allows the model to reuse context chunks\n",
                "config_builder.with_seed_dataset(\n",
                "    repo_id=\"advanced-tutorials/rag_evaluation_dataset\",\n",
                "    filename=\"document_chunks.csv\",\n",
                "    dataset_path=\"./data/document_chunks.csv\",\n",
                "    sampling_strategy=\"shuffle\",\n",
                "    with_replacement=True,\n",
                "    datastore={\"endpoint\": \"http://localhost:3000/v1/hf\"},\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "280e2fec",
            "metadata": {},
            "source": [
                "### Adding Categorical Columns for Controlled Diversity\n",
                "\n",
                "Now we'll add categorical columns to control the diversity of our RAG evaluation pairs. We'll define:\n",
                "\n",
                "1. **Difficulty levels**: easy, medium, hard\n",
                "\n",
                "2. **Reasoning types**: factual recall, inferential reasoning, etc.\n",
                "\n",
                "3. **Question types**: answerable vs. unanswerable (with weighting)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e3e27cac",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure categorical columns for controlled diversity\n",
                "config_builder.add_column(\n",
                "    C.SamplerColumn(\n",
                "        name=\"difficulty\",\n",
                "        type=P.SamplerType.CATEGORY,\n",
                "        params=P.CategorySamplerParams(\n",
                "            values=[\"easy\", \"medium\", \"hard\"],\n",
                "            description=\"The difficulty level of the question\"\n",
                "        )\n",
                "    )\n",
                ")\n",
                "\n",
                "config_builder.add_column(\n",
                "    C.SamplerColumn(\n",
                "        name=\"reasoning_type\",\n",
                "        type=P.SamplerType.CATEGORY,\n",
                "        params=P.CategorySamplerParams(\n",
                "            values=[\n",
                "                \"factual recall\",\n",
                "                \"inferential reasoning\",\n",
                "                \"comparative analysis\",\n",
                "                \"procedural understanding\",\n",
                "                \"cause and effect\"\n",
                "            ],\n",
                "            description=\"The type of reasoning required to answer the question\"\n",
                "        )\n",
                "    )\n",
                ")\n",
                "\n",
                "config_builder.add_column(\n",
                "    C.SamplerColumn(\n",
                "        name=\"question_type\",\n",
                "        type=P.SamplerType.CATEGORY,\n",
                "        params=P.CategorySamplerParams(\n",
                "            values=[\"answerable\", \"unanswerable\"],\n",
                "            # 10:1 ratio of answerable to unanswerable questions.\n",
                "            weights=[10, 1],\n",
                "        )\n",
                "    )\n",
                ")\n",
                "\n",
                "config_builder.validate()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "735cbbea",
            "metadata": {},
            "source": [
                "### Adding LLM-Structured Column for Q&A Pair Generation\n",
                "\n",
                "Now let's set up the core of our data generation: the Q&A pair column that will produce structured question-answer pairs based on our document context and control parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ecf44d9e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add Q&A pair generation column\n",
                "config_builder.add_column(\n",
                "    C.LLMStructuredColumn(\n",
                "        name=\"qa_pair\",\n",
                "        model_alias=model_alias,\n",
                "        system_prompt=(\n",
                "            \"You are an expert at generating high-quality RAG evaluation pairs. \"\n",
                "            \"You are very careful in assessing whether the question can be answered from the provided context. \"\n",
                "        ),\n",
                "        prompt=\"\"\"\\\n",
                "{{context}}\n",
                "\n",
                "Generate a {{difficulty}} {{reasoning_type}} question-answer pair.\n",
                "The question should be {{question_type}} using the provided context.\n",
                "\n",
                "For answerable questions:\n",
                "- Ensure the answer is fully supported by the context\n",
                "\n",
                "For unanswerable questions:\n",
                "- Keep the question topically relevant\n",
                "- Make it clearly beyond the context's scope\n",
                "\"\"\",\n",
                "        output_format=QAPair\n",
                "    )\n",
                ")\n",
                "\n",
                "config_builder.validate()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "41e6cc02",
            "metadata": {},
            "source": [
                "### Adding Evaluation Metrics with Custom Rubrics\n",
                "\n",
                "To assess the quality of our generated Q&A pairs, we'll add evaluation metrics using detailed rubrics for scoring. \n",
                "\n",
                "We use Data Designer's `LLMJudgeColumn` for this, defining a set of custom Rubrics designed for our task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "953bca63",
            "metadata": {},
            "outputs": [],
            "source": [
                "context_relevance_rubric = P.Rubric(\n",
                "    name=\"Context Relevance\",\n",
                "    description=\"Evaluates how relevant the answer is to the provided context\",\n",
                "    scoring={\n",
                "        \"5\": \"Perfect relevance to context with no extraneous information\",\n",
                "        \"4\": \"Highly relevant with minor deviations from context\",\n",
                "        \"3\": \"Moderately relevant but includes some unrelated information\",\n",
                "        \"2\": \"Minimally relevant with significant departure from context\",\n",
                "        \"1\": \"Almost entirely irrelevant to the provided context\"\n",
                "    }\n",
                ")\n",
                "\n",
                "answer_precision_rubric = P.Rubric(\n",
                "    name=\"Answer Precision\",\n",
                "    description=\"Evaluates the accuracy and specificity of the answer\",\n",
                "    scoring={\n",
                "        \"5\": \"Extremely precise with exact, specific information\",\n",
                "        \"4\": \"Very precise with minor imprecisions\",\n",
                "        \"3\": \"Adequately precise but could be more specific\",\n",
                "        \"2\": \"Imprecise with vague or ambiguous information\",\n",
                "        \"1\": \"Completely imprecise or inaccurate\"\n",
                "    }\n",
                ")\n",
                "\n",
                "answer_completeness_rubric = P.Rubric(\n",
                "    name=\"Answer Completeness\",\n",
                "    description=\"Evaluates how thoroughly the answer addresses all aspects of the question\",\n",
                "    scoring={\n",
                "        \"5\": \"Fully complete, addressing all aspects of the question\",\n",
                "        \"4\": \"Mostly complete with minor omissions\",\n",
                "        \"3\": \"Adequately complete but missing some details\",\n",
                "        \"2\": \"Substantially incomplete, missing important aspects\",\n",
                "        \"1\": \"Severely incomplete, barely addresses the question\"\n",
                "    }\n",
                ")\n",
                "\n",
                "hallucination_avoidance_rubric = P.Rubric(\n",
                "    name=\"Hallucination Avoidance\",\n",
                "    description=\"Evaluates the absence of made-up or incorrect information\",\n",
                "    scoring={\n",
                "        \"5\": \"No hallucinations, all information is factual and verifiable\",\n",
                "        \"4\": \"Minimal hallucinations that don't impact the core answer\",\n",
                "        \"3\": \"Some hallucinations that partially affect the answer quality\",\n",
                "        \"2\": \"Significant hallucinations that undermine the answer\",\n",
                "        \"1\": \"Severe hallucinations making the answer entirely unreliable\"\n",
                "    }\n",
                ")\n",
                "\n",
                "EVAL_METRICS_PROMPT_TEMPLATE = \"\"\"\\\n",
                "You are an expert evaluator of question-answer pairs. Analyze the following Q&A pair and evaluate it objectively.\n",
                "\n",
                "For this {{difficulty}} {{reasoning_type}} Q&A pair:\n",
                "{{qa_pair}}\n",
                "\n",
                "Take a deep breath and carefully evaluate each criterion based on the provided rubrics, considering the difficulty level and reasoning type indicated.\n",
                "\"\"\"\n",
                "\n",
                "config_builder.add_column(\n",
                "    C.LLMJudgeColumn(\n",
                "        name=\"eval_metrics\",\n",
                "        model_alias=model_alias,\n",
                "        prompt=EVAL_METRICS_PROMPT_TEMPLATE,\n",
                "        rubrics=[context_relevance_rubric, answer_precision_rubric, answer_completeness_rubric, hallucination_avoidance_rubric],\n",
                "    )\n",
                ")\n",
                "\n",
                "config_builder.validate()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8fb3dc84",
            "metadata": {},
            "source": [
                "### 👀 Preview Sample Records\n",
                "\n",
                "Let's generate a preview to see what our data will look like before running the full generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b55913d0",
            "metadata": {},
            "outputs": [],
            "source": [
                "preview = data_designer_client.preview(config_builder, verbose_logging=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a6b1b895",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run this cell multiple times to cycle through the 10 preview records.\n",
                "preview.display_sample_record()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b655a45f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# The preview dataset is available as a pandas DataFrame.\n",
                "preview.dataset.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "40099da2",
            "metadata": {},
            "source": [
                "### Generate the Full Dataset\n",
                "\n",
                "Now let's generate our full dataset of RAG evaluation pairs, analyze the coverage, and export it to a JSONL file for use in evaluating RAG systems."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2270b104",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate the full dataset.\n",
                "job_results = data_designer_client.create(config_builder, num_records=20, wait_until_done=False)\n",
                "\n",
                "job_results.wait_until_done()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac909600",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = job_results.load_dataset()\n",
                "print(\"\\nGenerated dataset shape:\", dataset.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "596c8a4c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export the dataset to JSONL format.\n",
                "dataset.to_json('./data/rag_evals.jsonl', orient='records', lines=True)\n",
                "print(\"\\nDataset exported to ./data/rag_evals.jsonl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "19c674f4",
            "metadata": {},
            "source": [
                "### Using Your RAG Evaluation Dataset\n",
                "\n",
                "Now that you've generated a diverse RAG evaluation dataset, here are some ways to use it:\n",
                "\n",
                "1. **Benchmarking**: Test your RAG system against these evaluation pairs to measure performance\n",
                "\n",
                "2. **Error Analysis**: Identify patterns in where your RAG system struggles\n",
                "\n",
                "3. **Optimization**: Use insights to tune retrieval and generation parameters\n",
                "\n",
                "4. **Regression Testing**: Track performance over time as you improve your system\n",
                "\n",
                "5. **Model Comparison**: Compare different LLMs, retrievers, or RAG architectures\n",
                "\n",
                "The JSONL file contains structured data with questions, ground truth answers, and quality metrics that you can use with most evaluation frameworks."
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "sdg_venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

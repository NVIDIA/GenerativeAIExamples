{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeMo Evaluator microservice: Retriever and RAG Evaluation\n",
    "\n",
    "In the following notebook, we'll be exploring how to use [NeMo Evaluator microservice](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/overview.html) to evaluate [Retriever Models](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/models/models_retriever.html) as well as [Retrieval Augmented Generation (RAG) Models](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/models/models_rag.html)!\n",
    "\n",
    "We'll look at the following examples: \n",
    "\n",
    "- Retriever Model Evaluation on FiQA\n",
    "- Retriever + Reranking Evaluation on FiQA\n",
    "- Retrieval Augmented Generation (RAG) Evaluation on FiQA with Ragas Metrics\n",
    "- Retrieval Augmented Generation (RAG) Evaluation on Synthetically Generated Data with Ragas Metrics\n",
    "\n",
    "In order to get started, we'll need to make sure our Evaluation Microservice is running, alongside our Retriever, Re-Rank, and LLM NIMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Set-up and Notebook Dependencies\n",
    "\n",
    "In order to run this notebook, the following will need to be up and running: \n",
    "\n",
    "- Evaluator Microservice, which can be conveniently deployed through the [Deploying with Helm](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/deploy-helm.html) guide\n",
    "- NVIDIA NIM Text Embedding, `nvidia/nv-embedqa-e5-v5`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/getting-started.html) guide\n",
    "- NVIDIA NIM Text Reranking, `nvidia/nv-rerankqa-mistral-4b-v3`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/getting-started.html) guide\n",
    "- NVIDIA NIM for LLM, `meta/llama-3.1-8b-instruct`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html) guide\n",
    "\n",
    "Once all of our services are up and running, we can install the Python `requests` library, which we will use to communicate with the Evaluator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU requests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to provide the Evaluation API URL in the cell below.\n",
    "\n",
    "> NOTE: Your evaluation URL will be provided as part of your deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_URL = \"<< YOUR EVALUATOR MICROSERVICE URL >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to provide the endpoints for your model addresses and model names, which will be set-up as part of the deployment process for each NIM.\n",
    "\n",
    "Below is an example of the default value for the embedding NIM:\n",
    "\n",
    "- Embedding: \n",
    "  - EMBEDDING_URL: `http://localhost:8000/v1/embeddings`\n",
    "  - EMBEDDING_MODEL_NAME: `nvidia/nv-embedqa-e5-v5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "EMBEDDING_URL = \"<< YOUR EMBEDDING MODEL NIM URL >>\"\n",
    "EMBEDDING_MODEL_NAME = \" << YOUR EMBEDDING MODEL NAME >>\"\n",
    "\n",
    "# reranker\n",
    "RERANKER_URL = \"<< YOUR RERANKER MODEL NIM URL >>\"\n",
    "RERANKER_MODEL_NAME = \"<< YOUR RERANKER MODEL NAME >>\"\n",
    "\n",
    "# llm\n",
    "LLM_URL = \"<< YOUR LLM MODEL NIM URL >>\"\n",
    "LLM_MODEL_NAME = \"<< YOUR LLM MODEL NAME >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify our Evaluation API is up and running with the built-in health check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'healthy'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "endpoint = f\"{EVAL_URL}/health\"\n",
    "response = requests.get(endpoint).json()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Model Evaluation on FiQA\n",
    "\n",
    "For our first evaluation, we're going to evaluate our Retrieval Model (`nvidia/nv-embedqa-e5-v5`) on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "The core pieces we need to provide are: \n",
    "\n",
    "- `top_k`, how many documents to retriever through our retriever model\n",
    "- `query_embedding_url`, the address of your currently running `nvidia/nv-embedqa-e5-v5` NIM if you're following the notebook exactly.\n",
    "- `query_embedding_model`, this will be `nvidia/nv-embedqa-e5-v5` if you're following the notebook exactly.\n",
    "- `index_embedding_url`, which will mirror the `query_embedding_url` assuming that you're using the same NIM deployment for both Query Embedding and Index embedding.\n",
    "- `index_embedding_model`, this will mirror the `query_embedding_model` assuming that you're using the same NIM deployment for both Query Embedding and Index embedding.\n",
    "\n",
    "> NOTE: While it's possible to use different NIM *deployments* for Query/Index Embedding - you will need to ensure the underlying model is the same between both.\n",
    "\n",
    "We'll also want to ensure we've set-up our evaluations correctly by following the available [documentation](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/evaluations/evaluations_retriever.html) for Retriever evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_eval_config = {\n",
    "  \"model\": {\n",
    "    \"retriever\": {\n",
    "      \"top_k\": 10,\n",
    "      \"query_embedding_url\": EMBEDDING_URL,\n",
    "      \"query_embedding_model\": EMBEDDING_MODEL_NAME,\n",
    "      \"index_embedding_url\": EMBEDDING_URL,\n",
    "      \"index_embedding_model\": EMBEDDING_MODEL_NAME\n",
    "    }\n",
    "  },\n",
    "  \"evaluations\": [\n",
    "    {\n",
    "      \"eval_type\": \"automatic\",\n",
    "      \"eval_subtype\": \"beir\",\n",
    "      \"dataset_path\" : \"fiqa\",\n",
    "      \"metrics\": \"recall_5,ndcg_cut_5,recall_10,ndcg_cut_10\",\n",
    "      \"dataset_format\": \"beir\"\n",
    "    }\n",
    "  ],\n",
    "  \"tag\": \"retriever-eval-beir\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now point to our `evaluations` endpoint at our Evaluation URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_endpoint = f\"{EVAL_URL}/v1/evaluations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will kick-off an evaluation job, and provide the Evaluation ID which can be used to monitor, and later download, the evaluation and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ID: eval-YMUZ99TaMo4Bm9ba4StNCi\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(evaluator_endpoint, json=retriever_eval_config).json()\n",
    "retriever_evaluation_id = response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {retriever_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the status of our evaluation in the cell below. \n",
    "\n",
    "> NOTE: When the evaluation `status` becomes `succeeded`, the `evaluation_results` field will become populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_id': 'eval-YMUZ99TaMo4Bm9ba4StNCi',\n",
       " 'status': 'succeeded',\n",
       " 'model': {'llm_name': None,\n",
       "  'inference_url': None,\n",
       "  'llm': None,\n",
       "  'retriever': {'top_k': 10,\n",
       "   'query_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'query_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'index_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'index_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'ranker_model': None,\n",
       "   'ranker_url': None},\n",
       "  'rag': None},\n",
       " 'evaluations': [{'eval_type': 'automatic',\n",
       "   'eval_subtype': 'beir',\n",
       "   'dataset_path': 'fiqa',\n",
       "   'metrics': 'recall_5,ndcg_cut_5,recall_10,ndcg_cut_10',\n",
       "   'dataset_format': 'beir'}],\n",
       " 'tag': 'retriever-eval-beir',\n",
       " 'created_at': '2024-09-26T18:42:14',\n",
       " 'created_by': None,\n",
       " 'evaluation_results': [{'level_name': 'evaluation',\n",
       "   'isRecommended': True,\n",
       "   'extra_grouping_fields': None,\n",
       "   'metrics': [{'name': 'ndcg_cut_5',\n",
       "     'value': 0.43179850619730425,\n",
       "     'metadata': {'name': 'beir'}},\n",
       "    {'name': 'recall_10',\n",
       "     'value': 0.5212761004427672,\n",
       "     'metadata': {'name': 'beir'}},\n",
       "    {'name': 'ndcg_cut_10',\n",
       "     'value': 0.455153721565557,\n",
       "     'metadata': {'name': 'beir'}},\n",
       "    {'name': 'recall_5',\n",
       "     'value': 0.4460219435913878,\n",
       "     'metadata': {'name': 'beir'}}],\n",
       "   'evaluation_results': None}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_id_endpoint = evaluator_endpoint + f\"/{retriever_evaluation_id}\"\n",
    "retriever_response = requests.get(evaluation_id_endpoint).json()\n",
    "retriever_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluation_results` field will contain our `metrics` along with their name, and their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ndcg_cut_5',\n",
       "  'value': 0.43179850619730425,\n",
       "  'metadata': {'name': 'beir'}},\n",
       " {'name': 'recall_10',\n",
       "  'value': 0.5212761004427672,\n",
       "  'metadata': {'name': 'beir'}},\n",
       " {'name': 'ndcg_cut_10',\n",
       "  'value': 0.455153721565557,\n",
       "  'metadata': {'name': 'beir'}},\n",
       " {'name': 'recall_5',\n",
       "  'value': 0.4460219435913878,\n",
       "  'metadata': {'name': 'beir'}}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_response[\"evaluation_results\"][0][\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever + Reranking Evaluation on FiQA\n",
    "\n",
    "For our second evaluation, we're going to evaluate our Retrieval Model (`nvidia/nv-embedqa-e5-v5`) on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "Instead of simply using a Retriever model, however, this example will also leverage a Reranking model (`nvidia/nv-rerankqa-mistral-4b-v3`) to rerank the retrieved results.\n",
    "\n",
    "We'll rerun the same evaluation configuration as we did above - with a few extra parameters in our `retriever` configuration:\n",
    "\n",
    "- `ranker_url`, which will point to our reranking model\n",
    "- `ranker_model`, which will contain the name of our reranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_reranker_eval_config = {\n",
    "  \"model\": {\n",
    "    \"retriever\": {\n",
    "      \"top_k\": 10,\n",
    "      \"query_embedding_url\": EMBEDDING_URL,\n",
    "      \"query_embedding_model\": EMBEDDING_MODEL_NAME,\n",
    "      \"index_embedding_url\": EMBEDDING_URL,\n",
    "      \"index_embedding_model\": EMBEDDING_MODEL_NAME,\n",
    "      \"ranker_url\": RERANKER_URL,\n",
    "      \"ranker_model\": RERANKER_MODEL_NAME,\n",
    "    }\n",
    "  },\n",
    "  \"evaluations\": [\n",
    "    {\n",
    "      \"eval_type\": \"automatic\",\n",
    "      \"eval_subtype\": \"beir\",\n",
    "      \"dataset_path\" : \"fiqa\",\n",
    "      \"metrics\": \"recall_5,ndcg_cut_5,recall_10,ndcg_cut_10\",\n",
    "      \"dataset_format\": \"beir\"\n",
    "    }\n",
    "  ],\n",
    "  \"tag\": \"retriever-reranker-eval-beir\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we can kick-off the evaluation job by sending a request to the evaluation URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ID: eval-Rrma88ZE5fKZMXghs4Pe27\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(evaluator_endpoint, json=retriever_reranker_eval_config).json()\n",
    "retriever_reranker_evaluation_id = response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {retriever_reranker_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the job using the following request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_id': 'eval-Rrma88ZE5fKZMXghs4Pe27',\n",
       " 'status': 'succeeded',\n",
       " 'model': {'llm_name': None,\n",
       "  'inference_url': None,\n",
       "  'llm': None,\n",
       "  'retriever': {'top_k': 10,\n",
       "   'query_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'query_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'index_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'index_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'ranker_model': 'nvidia/nv-rerankqa-mistral-4b-v3',\n",
       "   'ranker_url': 'http://nemo-ranking-ms.nemo-retrieval.svc.cluster.local:8080/v1'},\n",
       "  'rag': None},\n",
       " 'evaluations': [{'eval_type': 'automatic',\n",
       "   'eval_subtype': 'beir',\n",
       "   'dataset_path': 'fiqa',\n",
       "   'metrics': 'recall_5,ndcg_cut_5,recall_10,ndcg_cut_10',\n",
       "   'dataset_format': 'beir'}],\n",
       " 'tag': 'retriever-reranker-eval-beir',\n",
       " 'created_at': '2024-09-26T18:42:51',\n",
       " 'created_by': None,\n",
       " 'evaluation_results': [{'level_name': 'evaluation',\n",
       "   'isRecommended': True,\n",
       "   'extra_grouping_fields': None,\n",
       "   'metrics': [{'name': 'ndcg_cut_5',\n",
       "     'value': 0.4452853398692082,\n",
       "     'metadata': {'name': 'beir'}},\n",
       "    {'name': 'ndcg_cut_10',\n",
       "     'value': 0.4436852514995383,\n",
       "     'metadata': {'name': 'beir'}},\n",
       "    {'name': 'recall_10',\n",
       "     'value': 0.4503484598392005,\n",
       "     'metadata': {'name': 'beir'}},\n",
       "    {'name': 'recall_5',\n",
       "     'value': 0.42792290870994565,\n",
       "     'metadata': {'name': 'beir'}}],\n",
       "   'evaluation_results': None}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_id_endpoint = evaluator_endpoint + f\"/{retriever_reranker_evaluation_id}\"\n",
    "retriever_reranker_response = requests.get(evaluation_id_endpoint).json()\n",
    "retriever_reranker_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the results, as compared to the retriever-only results, below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ndcg_cut_5',\n",
       "  'value': 0.4452853398692082,\n",
       "  'metadata': {'name': 'beir'}},\n",
       " {'name': 'ndcg_cut_10',\n",
       "  'value': 0.4436852514995383,\n",
       "  'metadata': {'name': 'beir'}},\n",
       " {'name': 'recall_10',\n",
       "  'value': 0.4503484598392005,\n",
       "  'metadata': {'name': 'beir'}},\n",
       " {'name': 'recall_5',\n",
       "  'value': 0.42792290870994565,\n",
       "  'metadata': {'name': 'beir'}}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_reranker_response[\"evaluation_results\"][0][\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Evaluation on FIQA with Ragas Metrics\n",
    "\n",
    "With the most recent release of NeMo Evaluator microservice, not only can we evaluate Retrievers and Rerankers - we can also Evaluate RAG!\n",
    "\n",
    "Once again, we're going to evaluate on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "We're also going to evaluate our RAG pipeline on the [Ragas](https://docs.ragas.io/en/stable/howtos/index.html) metrics [\"Faithfulness\"](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html). This can be done by extending our evaluation configuration in the following ways:\n",
    "\n",
    "1. We can create the model type `rag`, and provide our `retriever` configuration we used in the first evaluation.\n",
    "2. We need to provide a `context_ordering` parameter, in this case we'll use `desc` which will order our context in descending score.\n",
    "3. We need to provide a \"generator\" (LLM) that can be used to generate responses based on the retrieved context!\n",
    "\n",
    "We'll also need to add in a number of `judge_` parameters to help calculate the Faithfulness metric.\n",
    "\n",
    "Let's look at an example evaluation configuration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_config = {\n",
    "  \"model\": {\n",
    "    \"rag\": {\n",
    "        \"context_ordering\": \"desc\",\n",
    "        \"retriever\": {\n",
    "            \"top_k\": 10,\n",
    "            \"query_embedding_url\": EMBEDDING_URL,\n",
    "            \"query_embedding_model\": EMBEDDING_MODEL_NAME,\n",
    "            \"index_embedding_url\": EMBEDDING_URL,\n",
    "            \"index_embedding_model\": EMBEDDING_MODEL_NAME,\n",
    "        },\n",
    "        \"llm\": {\n",
    "            \"inference_url\": LLM_URL,\n",
    "            \"llm_name\": LLM_MODEL_NAME,\n",
    "        }\n",
    "    }\n",
    "  },\n",
    "  \"evaluations\": [\n",
    "    {\n",
    "        \"eval_type\": \"automatic\",\n",
    "        \"eval_subtype\": \"beir\",\n",
    "        \"dataset_path\": \"fiqa\",\n",
    "        \"dataset_format\" : \"beir\",\n",
    "        \"retriever_metrics\": \"recall_5,ndcg_cut_5,recall_10,ndcg_cut_10\",\n",
    "        \"rag_metrics\": \"faithfulness\",\n",
    "        \"judge_llm\": LLM_MODEL_NAME,\n",
    "        \"judge_llm_url\": LLM_URL,\n",
    "        \"judge_llm_api_key\": None,\n",
    "        \"judge_embeddings\": EMBEDDING_MODEL_NAME,\n",
    "        \"judge_embeddings_url\": EMBEDDING_URL,\n",
    "        \"judge_embeddings_api_key\": None,\n",
    "        \"judge_timeout\": 120,\n",
    "        \"judge_max_retries\": 2,\n",
    "        \"judge_max_workers\": 16\n",
    "    }\n",
    "  ],\n",
    "  \"tag\": \"rag-eval-beir\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set-up our evaluation configuration, we're ready to fire off the evaluation job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ID: eval-LYNykPmbJsE8aUqqxUvBiE\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(evaluator_endpoint, json=rag_eval_config).json()\n",
    "rag_evaluation_id = response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {rag_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can poll the API to determine the status of our job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_id': 'eval-LYNykPmbJsE8aUqqxUvBiE',\n",
       " 'status': 'running',\n",
       " 'model': {'llm_name': None,\n",
       "  'inference_url': 'http://meta-llama3-8b-instruct.nim-meta-llama3-8b-instruct.svc.cluster.local:8000/v1',\n",
       "  'llm': None,\n",
       "  'retriever': None,\n",
       "  'rag': {'retriever': {'top_k': 10,\n",
       "    'query_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "    'query_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "    'index_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "    'index_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "    'ranker_model': None,\n",
       "    'ranker_url': None},\n",
       "   'llm': {'llm_name': 'meta/llama-3_1-8b-instruct',\n",
       "    'inference_url': 'http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1'}}},\n",
       " 'evaluations': [{'eval_type': 'automatic',\n",
       "   'eval_subtype': 'beir',\n",
       "   'dataset_path': 'fiqa',\n",
       "   'dataset_format': 'beir',\n",
       "   'retriever_metrics': 'recall_5,ndcg_cut_5,recall_10,ndcg_cut_10',\n",
       "   'rag_metrics': 'faithfulness',\n",
       "   'judge_llm': 'meta/llama-3_1-8b-instruct',\n",
       "   'judge_llm_url': 'http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1',\n",
       "   'judge_embeddings': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'judge_embeddings_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'judge_timeout': 120,\n",
       "   'judge_max_retries': 2,\n",
       "   'judge_max_workers': 16}],\n",
       " 'tag': 'rag-eval-beir',\n",
       " 'created_at': '2024-09-26T18:44:32',\n",
       " 'created_by': None,\n",
       " 'evaluation_results': []}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_id_endpoint = evaluator_endpoint + f\"/{rag_evaluation_id}\"\n",
    "response = requests.get(evaluation_id_endpoint).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going beyond just looking at the results in the response object - we can download the results from the NeMo Datastore microservice as showcased below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "url = evaluator_endpoint + f\"/{rag_evaluation_id}\" + \"/download-results\"\n",
    "headers = {'accept': 'application/json'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open('result.zip', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Results downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download results. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unzip the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  result.zip\n",
      " extracting: results/metadata.json   \n",
      " extracting: results/.gitattributes  \n",
      " extracting: results/automatic/beir/model_config_custom_rag_model.yaml  \n",
      " extracting: results/automatic/beir/haystack_yaml/index.yaml  \n",
      " extracting: results/automatic/beir/haystack_yaml/query.yaml  \n",
      " extracting: results/automatic/beir/results/cleanup_milvus-run.log  \n",
      " extracting: results/automatic/beir/results/scores.jsonl  \n",
      " extracting: results/automatic/beir/results/README.md  \n",
      " extracting: results/automatic/beir/results/rag-run.log  \n"
     ]
    }
   ],
   "source": [
    "!unzip result.zip -d results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use a helper function to display them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def display_jsonl_scores(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            for category, scores in data.items():\n",
    "                print(f\"{category.capitalize()} Scores:\")\n",
    "                for key, value in scores.items():\n",
    "                    print(f\"  {key}: {value:.4f}\")\n",
    "            print()  # Add a blank line between objects for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever Scores:\n",
      "  ndcg_cut_10: 0.4548\n",
      "  recall_5: 0.4455\n",
      "  ndcg_cut_5: 0.4314\n",
      "  recall_10: 0.5208\n",
      "\n",
      "Generation Scores:\n",
      "  faithfulness: 0.7632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_jsonl_scores(\"./results/automatic/beir/results/scores.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Evaluation on Synthetically Generated Data with Ragas Metrics\n",
    "\n",
    "For our final evaluation, we're going to be leveraging work done in [this](https://github.com/NVIDIA/GenerativeAIExamples/blob/main/nemo/retriever-synthetic-data-generation/notebooks/quickstart.ipynb) notebook to create a BeIR format dataset created with Synthetic Data Generation. \n",
    "\n",
    "The output from the above notebook should be a dataset with the following items which can be found in the `outputs/sample_synthetic_data/beir/filtered/synthetic` directory after running the notebook:\n",
    "\n",
    "- `corpus.jsonl`\n",
    "- `qrels/test.tsv`\n",
    "- `queries.jsonl`\n",
    "\n",
    "This notebook assumes you've run the above notebook and have moved the `outputs/sample_synthetic_data/beir/filtered/synthetic` directory into the root folder of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following utility function to upload the folder contents to the NeMo Datastore microservice under the name \"SDG_BEIR_DATASET\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hh\n",
    "import requests\n",
    "\n",
    "DATASTORE_URL = \"\"\n",
    "\n",
    "## This token is not used in NDS, and so it could be any value.\n",
    "token = \"mock\"\n",
    "\n",
    "repo_name = \"SDG_BEIR_DATASET\"\n",
    "dir_path = \"./synthetic\"\n",
    "\n",
    "# create repo\n",
    "datasets_endpoint = DATASTORE_URL + \"/v1/datasets\"\n",
    "post_body = {\n",
    "  \"name\": repo_name,\n",
    "  \"description\": \"BeIR Data Created with SDG Pipeline\"\n",
    "}\n",
    "repo_response = requests.post(datasets_endpoint, json=post_body, allow_redirects=True)\n",
    "\n",
    "# upload dir\n",
    "repo_full_name = f\"nvidia/{repo_name}\"\n",
    "path_in_repo = \".\"\n",
    "repo_type =\"dataset\"\n",
    "hf_api = hh.HfApi(endpoint=DATASTORE_URL, token=token)\n",
    "result = hf_api.upload_folder(repo_id=repo_full_name, folder_path=dir_path, path_in_repo=path_in_repo, repo_type=repo_type)\n",
    "\n",
    "print(f\"Dataset folder uploaded to: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can once again create our RAG evaluation configuration while make a small change, which is to simply point at the newly updated dataset.\n",
    "\n",
    "You'll notice that we're using the same LLM to generate and evaluate - this is for showcase purposes, and it is recommended that you use a different model, or a larger model (`meta/llama-3.1-405b-instruct`, for example).\n",
    "\n",
    "> NOTE: We'll also include the [Answer Relevance](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html) metric from Ragas for this run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_sdg_config = {\n",
    "  \"model\": {\n",
    "    \"rag\": {\n",
    "        \"context_ordering\": \"desc\",\n",
    "        \"retriever\": {\n",
    "            \"top_k\": 10,\n",
    "            \"query_embedding_url\": EMBEDDING_URL,\n",
    "            \"query_embedding_model\": EMBEDDING_MODEL_NAME,\n",
    "            \"index_embedding_url\": EMBEDDING_URL,\n",
    "            \"index_embedding_model\": EMBEDDING_MODEL_NAME,\n",
    "        },\n",
    "        \"llm\": {\n",
    "            \"inference_url\": LLM_URL,\n",
    "            \"llm_name\": LLM_MODEL_NAME,\n",
    "        }\n",
    "    }\n",
    "  },\n",
    "  \"evaluations\": [\n",
    "    {\n",
    "        \"eval_type\": \"automatic\",\n",
    "        \"eval_subtype\": \"beir\",\n",
    "        \"dataset_path\": \"nds:SDG_BEIR_DATASET\",\n",
    "        \"dataset_format\" : \"beir\",\n",
    "        \"retriever_metrics\": \"recall_5,ndcg_cut_5,recall_10,ndcg_cut_10\",\n",
    "        \"rag_metrics\": \"faithfulness,answer_relevancy\",\n",
    "        \"judge_llm\": LLM_MODEL_NAME,\n",
    "        \"judge_llm_url\": LLM_URL,\n",
    "        \"judge_llm_api_key\": None,\n",
    "        \"judge_embeddings\": EMBEDDING_MODEL_NAME,\n",
    "        \"judge_embeddings_url\": EMBEDDING_URL,\n",
    "        \"judge_embeddings_api_key\": None,\n",
    "        \"judge_timeout\": 120,\n",
    "        \"judge_max_retries\": 2,\n",
    "        \"judge_max_workers\": 16\n",
    "    }\n",
    "  ],\n",
    "  \"tag\": \"rag-eval-sdg-beir\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fire off that evaluation job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ID: eval-T1SuFi8K9uJvbiEx46fhHh\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(evaluator_endpoint, json=rag_eval_sdg_config).json()\n",
    "rag_sdg_evaluation_id = response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {rag_sdg_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can use the API to determine when the run has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_id': 'eval-T1SuFi8K9uJvbiEx46fhHh',\n",
       " 'status': 'succeeded',\n",
       " 'model': {'llm_name': None,\n",
       "  'inference_url': 'http://meta-llama3-8b-instruct.nim-meta-llama3-8b-instruct.svc.cluster.local:8000/v1',\n",
       "  'llm': None,\n",
       "  'retriever': None,\n",
       "  'rag': {'retriever': {'top_k': 10,\n",
       "    'query_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "    'query_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "    'index_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "    'index_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "    'ranker_model': None,\n",
       "    'ranker_url': None},\n",
       "   'llm': {'llm_name': 'meta/llama-3_1-8b-instruct',\n",
       "    'inference_url': 'http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1'}}},\n",
       " 'evaluations': [{'eval_type': 'automatic',\n",
       "   'eval_subtype': 'beir',\n",
       "   'dataset_path': 'nds:SGD_BEIR_DATASET',\n",
       "   'dataset_format': 'beir',\n",
       "   'retriever_metrics': 'recall_5,ndcg_cut_5,recall_10,ndcg_cut_10',\n",
       "   'rag_metrics': 'faithfulness,answer_relevancy',\n",
       "   'judge_llm': 'meta/llama-3_1-8b-instruct',\n",
       "   'judge_llm_url': 'http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1',\n",
       "   'judge_embeddings': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'judge_embeddings_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'judge_timeout': 120,\n",
       "   'judge_max_retries': 2,\n",
       "   'judge_max_workers': 16}],\n",
       " 'tag': 'rag-eval-sdg-beir',\n",
       " 'created_at': '2024-09-27T01:51:34',\n",
       " 'created_by': None,\n",
       " 'evaluation_results': [{'level_name': 'evaluation',\n",
       "   'isRecommended': True,\n",
       "   'extra_grouping_fields': None,\n",
       "   'metrics': [{'name': 'recall_5',\n",
       "     'value': 1.0,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'ndcg_cut_10',\n",
       "     'value': 1.0,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'recall_10',\n",
       "     'value': 1.0,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'ndcg_cut_5',\n",
       "     'value': 1.0,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'faithfulness',\n",
       "     'value': 0.7421487603305783,\n",
       "     'metadata': {'name': 'beir', 'category': 'generation'}},\n",
       "    {'name': 'answer_relevancy',\n",
       "     'value': 0.48819483649612827,\n",
       "     'metadata': {'name': 'beir', 'category': 'generation'}}],\n",
       "   'evaluation_results': None}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_id_endpoint = evaluator_endpoint + f\"/{rag_sdg_evaluation_id}\"\n",
    "response = requests.get(evaluation_id_endpoint).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download, unzip, and view the results on our synthetically created evaluation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "url = evaluator_endpoint + f\"/{rag_sdg_evaluation_id}\" + \"/download-results\"\n",
    "headers = {'accept': 'application/json'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open('result_sdg.zip', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Results downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download results. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  result_sdg.zip\n",
      " extracting: results_sdg_beir/metadata.json  \n",
      " extracting: results_sdg_beir/.gitattributes  \n",
      " extracting: results_sdg_beir/automatic/beir/model_config_custom_rag_model.yaml  \n",
      " extracting: results_sdg_beir/automatic/beir/haystack_yaml/index.yaml  \n",
      " extracting: results_sdg_beir/automatic/beir/haystack_yaml/query.yaml  \n",
      " extracting: results_sdg_beir/automatic/beir/results/cleanup_milvus-run.log  \n",
      " extracting: results_sdg_beir/automatic/beir/results/scores.jsonl  \n",
      " extracting: results_sdg_beir/automatic/beir/results/README.md  \n",
      " extracting: results_sdg_beir/automatic/beir/results/rag-run.log  \n",
      " extracting: results_sdg_beir/automatic/beir/SGD_BEIR_DATASET/queries.jsonl  \n",
      " extracting: results_sdg_beir/automatic/beir/SGD_BEIR_DATASET/corpus.jsonl  \n",
      " extracting: results_sdg_beir/automatic/beir/SGD_BEIR_DATASET/.gitattributes  \n",
      " extracting: results_sdg_beir/automatic/beir/SGD_BEIR_DATASET/qrels/test.tsv  \n"
     ]
    }
   ],
   "source": [
    "!unzip result_sdg.zip -d results_sdg_beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever Scores:\n",
      "  recall_5: 1.0000\n",
      "  ndcg_cut_10: 1.0000\n",
      "  recall_10: 1.0000\n",
      "  ndcg_cut_5: 1.0000\n",
      "\n",
      "Generation Scores:\n",
      "  faithfulness: 0.7421\n",
      "  answer_relevancy: 0.4882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_jsonl_scores(\"./results_sdg_beir/automatic/beir/results/scores.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeMo Evaluator microservice: Retriever and RAG Evaluation\n",
    "\n",
    "In the following notebook, we'll be exploring how to use [NeMo Evaluator microservice](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/overview.html) to evaluate [Retriever Models](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/evaluations/evaluations_retriever.html) as well as [Retrieval Augmented Generation (RAG) Models](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/models/models_rag.html)!\n",
    "\n",
    "We'll look at the following examples: \n",
    "\n",
    "- [Retriever Model Evaluation on FiQA](##Retriever-Model-Evaluation-on-FiQA)\n",
    "- [Retriever + Reranking Evaluation on FiQA](##retriever--reranking-evaluation-on-fiqa)\n",
    "- [Retrieval Augmented Generation (RAG) Evaluation on FiQA with Ragas Metrics](##retrieval-augmented-generation-rag-evaluation-on-fiqa-with-ragas-metrics)\n",
    "- [Retrieval Augmented Generation (RAG) Evaluation on Synthetically Generated Data with Ragas Metrics](##retrieval-augmented-generation-rag-evaluation-on-synthetically-generated-data-with-ragas-metrics)\n",
    "\n",
    "In order to get started, we'll need to make sure our Evaluation Microservice is running, alongside our Retriever, Re-Rank, and LLM NIMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Set-up and Notebook Dependencies\n",
    "\n",
    "In order to run this notebook, the following will need to be up and running: \n",
    "\n",
    "- Evaluator Microservice, which can be deployed through the convenient [Deploying with Helm](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/deploy-helm.html)\n",
    "- NVIDIA NIM Text Embedding, `nvidia/nv-embedqa-e5-v5`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/getting-started.html) guide\n",
    "- NVIDIA NIM Text Reranking, `nv-rerank-qa-mistral-4b`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/getting-started.html) guide\n",
    "- NVIDIA NIM for LLM (this notebook used Llama 3.1 8B Instruct), which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html) guide\n",
    "\n",
    "Once all of our services are up and running, we can install the Python `requests` library, which we will use to communicate with the Evaluator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU requests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to provide the Evaluation API URL in the cell below.\n",
    "\n",
    "> NOTE: Your evaluation URL will be provided as part of your deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_URL = \"<< YOUR EVAL URL HERE >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify our Evaluation API is up and running with the built-in health check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'healthy'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "endpoint = f\"{EVAL_URL}/health\"\n",
    "response = requests.get(endpoint).json()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Model Evaluation on FiQA\n",
    "\n",
    "For our first evaluation, we're going to evaluate our Retrieval Model (`nvidia/nv-embedqa-e5-v5`) on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "The core pieces we need to provide are: \n",
    "\n",
    "- `top_k`, how many documents to retriever through our retriever model\n",
    "- `query_embedding_url`, the address of your currently running `nvidia/nv-embedqa-e5-v5` NIM if you're following the notebook exactly.\n",
    "- `query_embedding_model`, this will be `nvidia/nv-embedqa-e5-v5` if you're following the notebook exactly.\n",
    "- `index_embedding_url`, which will mirror the `query_embedding_url` assuming that you're using the same NIM deployment for both Query Embedding and Index embedding.\n",
    "- `index_embedding_model`, this will mirror the `query_embedding_model` assuming that you're using the same NIM deployment for both Query Embedding and Index embedding.\n",
    "\n",
    "> NOTE: While it's possible to use different NIM *deployments* for Query/Index Embedding - you will need to ensure the underlying model is the same between both.\n",
    "\n",
    "We'll also want to ensure we've set-up our evaluations correctly by following the available [documentation](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/evaluations/evaluations_retriever.html) for Retriever evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_eval_config = {\n",
    "  \"model\": {\n",
    "    \"retriever\": {\n",
    "      \"top_k\": 10,\n",
    "      \"query_embedding_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "      \"query_embedding_model\": \"nvidia/nv-embedqa-e5-v5\",\n",
    "      \"index_embedding_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "      \"index_embedding_model\": \"nvidia/nv-embedqa-e5-v5\"\n",
    "    }\n",
    "  },\n",
    "  \"evaluations\": [\n",
    "    {\n",
    "      \"eval_type\": \"automatic\",\n",
    "      \"eval_subtype\": \"beir\",\n",
    "      \"dataset_path\" : \"fiqa\",\n",
    "      \"metrics\": \"recall_5,ndcg_cut_5,recall_10,ndcg_cut_10\",\n",
    "      \"dataset_format\": \"beir\"\n",
    "    }\n",
    "  ],\n",
    "  \"tag\": \"retriever-eval-beir\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now point to our `evaluations` endpoint at our Evaluation URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_endpoint = f\"{EVAL_URL}/v1/evaluations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will kick-off an evaluation job, and provide the Evaluation ID which can be used to monitor, and later download, the evaluation and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ID: eval-P75Yh9icc58inYuzn82AVo\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(evaluator_endpoint, json=retriever_eval_config).json()\n",
    "retriever_evaluation_id = response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {retriever_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the status of our evaluation in the cell below. \n",
    "\n",
    "> NOTE: When the evaluation `status` becomes `succeeded`, the `evaluation_results` field will become populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_id': 'eval-P75Yh9icc58inYuzn82AVo',\n",
       " 'status': 'succeeded',\n",
       " 'model': {'llm_name': None,\n",
       "  'inference_url': None,\n",
       "  'llm': None,\n",
       "  'retriever': {'top_k': 10,\n",
       "   'query_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'query_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'index_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'index_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'ranker_model': None,\n",
       "   'ranker_url': None},\n",
       "  'rag': None},\n",
       " 'evaluations': [{'eval_type': 'automatic',\n",
       "   'eval_subtype': 'beir',\n",
       "   'dataset_path': 'fiqa',\n",
       "   'metrics': 'recall_5,ndcg_cut_5,recall_10,ndcg_cut_10',\n",
       "   'dataset_format': 'beir'}],\n",
       " 'tag': 'retriever-eval-beir',\n",
       " 'created_at': '2024-09-25T22:03:28',\n",
       " 'created_by': None,\n",
       " 'evaluation_results': [{'level_name': 'evaluation',\n",
       "   'isRecommended': True,\n",
       "   'extra_grouping_fields': None,\n",
       "   'metrics': [{'name': 'ndcg_cut_5',\n",
       "     'value': 0.43179850619730425,\n",
       "     'metadata': {'name': 'beir'}},\n",
       "    {'name': 'recall_10',\n",
       "     'value': 0.5212761004427672,\n",
       "     'metadata': {'name': 'beir'}},\n",
       "    {'name': 'ndcg_cut_10',\n",
       "     'value': 0.455153721565557,\n",
       "     'metadata': {'name': 'beir'}},\n",
       "    {'name': 'recall_5',\n",
       "     'value': 0.4460219435913878,\n",
       "     'metadata': {'name': 'beir'}}],\n",
       "   'evaluation_results': None}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_id_endpoint = evaluator_endpoint + f\"/{retriever_evaluation_id}\"\n",
    "response = requests.get(evaluation_id_endpoint).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluation_results` field will contain our `metrics` along with their name, and their score.\n",
    "\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    'name': 'ndcg_cut_5',\n",
    "    'value': 0.43179850619730425,\n",
    "    'metadata': {'name': 'beir'}\n",
    "  },\n",
    "  {\n",
    "    'name': 'recall_10',\n",
    "    'value': 0.5212761004427672,\n",
    "    'metadata': {'name': 'beir'}\n",
    "  },\n",
    "  {\n",
    "    'name': 'ndcg_cut_10',\n",
    "    'value': 0.455153721565557,\n",
    "    'metadata': {'name': 'beir'}\n",
    "  },\n",
    "  {\n",
    "    'name': 'recall_5',\n",
    "    'value': 0.4460219435913878,\n",
    "    'metadata': {'name': 'beir'}\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever + Reranking Evaluation on FiQA\n",
    "\n",
    "For our second evaluation, we're going to evaluate our Retrieval Model (`nvidia/nv-embedqa-e5-v5`) on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "Instead of simply using a Retriever model, however, this example will also leverage a Reranking model (`nvidia/nv-rerank-qa-mistral-4b`) to rerank the retrieved results.\n",
    "\n",
    "We'll rerun the same evaluation configuration as we did above - with a few extra parameters in our `retriever` configuration:\n",
    "\n",
    "- `ranker_url`, which will point to our reranking model\n",
    "- `ranker_model`, which will contain the name of our reranking model\n",
    "\n",
    "Aside from that change - this evaluation is identical so we can observe the improvements gained by using a reranking NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_reranker_eval_config = {\n",
    "  \"model\": {\n",
    "    \"retriever\": {\n",
    "      \"top_k\": 10,\n",
    "      \"query_embedding_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "      \"query_embedding_model\": \"nvidia/nv-embedqa-e5-v5\",\n",
    "      \"index_embedding_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "      \"index_embedding_model\": \"nvidia/nv-embedqa-e5-v5\",\n",
    "      \"ranker_url\" : \"https://ranking.dev.aire.nvidia.com/v1\",\n",
    "      \"ranker_model\" : \"nvidia/nv-rerank-qa-mistral-4b\"\n",
    "    }\n",
    "  },\n",
    "  \"evaluations\": [\n",
    "    {\n",
    "      \"eval_type\": \"automatic\",\n",
    "      \"eval_subtype\": \"beir\",\n",
    "      \"dataset_path\" : \"fiqa\",\n",
    "      \"metrics\": \"recall_5,ndcg_cut_5,recall_10,ndcg_cut_10\",\n",
    "      \"dataset_format\": \"beir\"\n",
    "    }\n",
    "  ],\n",
    "  \"tag\": \"retriever-reranker-eval-beir\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we can kick-off the evaluation job by sending a request to the evaluation URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ID: eval-J7HN4DmDTFuRaqBVp3WUxT\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(evaluator_endpoint, json=retriever_reranker_eval_config).json()\n",
    "retriever_reranker_evaluation_id = response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {retriever_reranker_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the job using the following request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_id': 'eval-J7HN4DmDTFuRaqBVp3WUxT',\n",
       " 'status': 'failed',\n",
       " 'model': {'llm_name': None,\n",
       "  'inference_url': None,\n",
       "  'llm': None,\n",
       "  'retriever': {'top_k': 10,\n",
       "   'query_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'query_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'index_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'index_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'ranker_model': 'nvidia/nv-rerank-qa-mistral-4b',\n",
       "   'ranker_url': 'https://ranking.dev.aire.nvidia.com/v1'},\n",
       "  'rag': None},\n",
       " 'evaluations': [{'eval_type': 'automatic',\n",
       "   'eval_subtype': 'beir',\n",
       "   'dataset_path': 'fiqa',\n",
       "   'metrics': 'recall_5,ndcg_cut_5,recall_10,ndcg_cut_10',\n",
       "   'dataset_format': 'beir'}],\n",
       " 'tag': 'retriever-reranker-eval-beir',\n",
       " 'created_at': '2024-09-26T00:11:52',\n",
       " 'created_by': None,\n",
       " 'evaluation_results': []}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_id_endpoint = evaluator_endpoint + f\"/{retriever_reranker_evaluation_id}\"\n",
    "response = requests.get(evaluation_id_endpoint).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the results, as compared to the retriever-only results, below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED JOB TO SUCCEED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Evaluation on FIQA with Ragas Metrics\n",
    "\n",
    "With the most recent release of NeMo Evaluator microservice, not only can we evaluate Retrievers and Rerankers - we can also Evaluate RAG!\n",
    "\n",
    "Once again, we're going to evaluate on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "We're also going to evaluate our RAG pipeline on the [Ragas](https://docs.ragas.io/en/stable/howtos/index.html) metrics [\"Faithfulness\"](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html). This can be done by extending our evaluation configuration in the following ways:\n",
    "\n",
    "1. We can create the model type `rag`, and provide our `retriever` configuration we used in the first evaluation.\n",
    "2. We need to provide a `context_ordering` parameter, in this case we'll use `desc` which will order our context in descending score.\n",
    "3. We need to provide a \"generator\" (LLM) that can be used to generate responses based on the retrieved context!\n",
    "\n",
    "We'll also need to add in a number of `judge_` parameters to help calculate the Faithfulness metric.\n",
    "\n",
    "Let's look at an example evaluation configuration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_config = {\n",
    "  \"model\": {\n",
    "    \"rag\": {\n",
    "        \"context_ordering\": \"desc\",\n",
    "        \"retriever\": {\n",
    "            \"top_k\": 10,\n",
    "            \"query_embedding_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "            \"query_embedding_model\": \"nvidia/nv-embedqa-e5-v5\",\n",
    "            \"index_embedding_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "            \"index_embedding_model\": \"nvidia/nv-embedqa-e5-v5\"\n",
    "        },\n",
    "        \"llm\": {\n",
    "            \"inference_url\": \"http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1\",\n",
    "            \"llm_name\": \"meta/llama-3_1-8b-instruct\"\n",
    "        }\n",
    "    }\n",
    "  },\n",
    "  \"evaluations\": [\n",
    "    {\n",
    "        \"eval_type\": \"automatic\",\n",
    "        \"eval_subtype\": \"beir\",\n",
    "        \"dataset_path\": \"fiqa\",\n",
    "        \"dataset_format\" : \"beir\",\n",
    "        \"retriever_metrics\": \"recall_5,ndcg_cut_5,recall_10,ndcg_cut_10\",\n",
    "        \"rag_metrics\": \"faithfulness\",\n",
    "        \"judge_llm\": \"meta/llama-3_1-8b-instruct\",\n",
    "        \"judge_llm_url\": \"http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1\",\n",
    "        \"judge_llm_api_key\": None,\n",
    "        \"judge_embeddings\": \"nvidia/nv-embedqa-e5-v5\",\n",
    "        \"judge_embeddings_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "        \"judge_embeddings_api_key\": None,\n",
    "        \"judge_timeout\": 120,\n",
    "        \"judge_max_retries\": 2,\n",
    "        \"judge_max_workers\": 16\n",
    "    }\n",
    "  ],\n",
    "  \"tag\": \"rag-eval-beir\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set-up our evaluation configuration, we're ready to fire off the evaluation job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evaluation_id': 'eval-4JHowsb6JsttyYjdQ6JBdt', 'status': 'running', 'model': {'llm_name': None, 'inference_url': 'http://meta-llama3-8b-instruct.nim-meta-llama3-8b-instruct.svc.cluster.local:8000/v1', 'llm': None, 'retriever': None, 'rag': {'retriever': {'top_k': 10, 'query_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings', 'query_embedding_model': 'nvidia/nv-embedqa-e5-v5', 'index_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings', 'index_embedding_model': 'nvidia/nv-embedqa-e5-v5', 'ranker_model': None, 'ranker_url': None}, 'llm': {'llm_name': 'meta/llama-3_1-8b-instruct', 'inference_url': 'http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1'}}}, 'evaluations': [{'eval_type': 'automatic', 'eval_subtype': 'beir'}], 'tag': 'rag-eval-beir', 'created_at': '2024-09-25T22:15:55', 'created_by': None}\n",
      "Evaluation ID: eval-4JHowsb6JsttyYjdQ6JBdt\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(evaluator_endpoint, json=rag_eval_config).json()\n",
    "rag_evaluation_id = response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {rag_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can poll the API to determine the status of our job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_id': 'eval-4JHowsb6JsttyYjdQ6JBdt',\n",
       " 'status': 'succeeded',\n",
       " 'model': {'llm_name': None,\n",
       "  'inference_url': 'http://meta-llama3-8b-instruct.nim-meta-llama3-8b-instruct.svc.cluster.local:8000/v1',\n",
       "  'llm': None,\n",
       "  'retriever': None,\n",
       "  'rag': {'retriever': {'top_k': 10,\n",
       "    'query_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "    'query_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "    'index_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "    'index_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "    'ranker_model': None,\n",
       "    'ranker_url': None},\n",
       "   'llm': {'llm_name': 'meta/llama-3_1-8b-instruct',\n",
       "    'inference_url': 'http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1'}}},\n",
       " 'evaluations': [{'eval_type': 'automatic',\n",
       "   'eval_subtype': 'beir',\n",
       "   'dataset_path': 'fiqa',\n",
       "   'dataset_format': 'beir',\n",
       "   'retriever_metrics': 'recall_5,ndcg_cut_5,recall_10,ndcg_cut_10',\n",
       "   'rag_metrics': 'faithfulness',\n",
       "   'judge_llm': 'meta/llama-3_1-8b-instruct',\n",
       "   'judge_llm_url': 'http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1',\n",
       "   'judge_embeddings': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'judge_embeddings_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'judge_timeout': 120,\n",
       "   'judge_max_retries': 2,\n",
       "   'judge_max_workers': 16}],\n",
       " 'tag': 'rag-eval-beir',\n",
       " 'created_at': '2024-09-25T22:15:55',\n",
       " 'created_by': None,\n",
       " 'evaluation_results': [{'level_name': 'evaluation',\n",
       "   'isRecommended': True,\n",
       "   'extra_grouping_fields': None,\n",
       "   'metrics': [{'name': 'ndcg_cut_10',\n",
       "     'value': 0.45477874573146465,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'recall_5',\n",
       "     'value': 0.4455075402992067,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'ndcg_cut_5',\n",
       "     'value': 0.43142353036321196,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'recall_10',\n",
       "     'value': 0.5207616971505862,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'faithfulness',\n",
       "     'value': 0.7631576586387824,\n",
       "     'metadata': {'name': 'beir', 'category': 'generation'}}],\n",
       "   'evaluation_results': None}]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_id_endpoint = evaluator_endpoint + f\"/{rag_evaluation_id}\"\n",
    "response = requests.get(evaluation_id_endpoint).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going beyond just looking at the results in the response object - we can download the results from the NeMo Datastore microservice as showcased below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "url = evaluator_endpoint + f\"/{rag_evaluation_id}\" + \"/download-results\"\n",
    "headers = {'accept': 'application/json'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open('result.zip', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Results downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download results. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unzip the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  result.zip\n",
      " extracting: results/metadata.json   \n",
      " extracting: results/.gitattributes  \n",
      " extracting: results/automatic/beir/model_config_custom_rag_model.yaml  \n",
      " extracting: results/automatic/beir/haystack_yaml/index.yaml  \n",
      " extracting: results/automatic/beir/haystack_yaml/query.yaml  \n",
      " extracting: results/automatic/beir/results/cleanup_milvus-run.log  \n",
      " extracting: results/automatic/beir/results/scores.jsonl  \n",
      " extracting: results/automatic/beir/results/README.md  \n",
      " extracting: results/automatic/beir/results/rag-run.log  \n"
     ]
    }
   ],
   "source": [
    "!unzip result.zip -d results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use a helper function to display them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def display_jsonl_scores(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            for category, scores in data.items():\n",
    "                print(f\"{category.capitalize()} Scores:\")\n",
    "                for key, value in scores.items():\n",
    "                    print(f\"  {key}: {value:.4f}\")\n",
    "            print()  # Add a blank line between objects for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever Scores:\n",
      "  ndcg_cut_10: 0.4548\n",
      "  recall_5: 0.4455\n",
      "  ndcg_cut_5: 0.4314\n",
      "  recall_10: 0.5208\n",
      "\n",
      "Generation Scores:\n",
      "  faithfulness: 0.7632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_jsonl_scores(\"./results/automatic/beir/results/scores.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Evaluation on Synthetically Generated Data with Ragas Metrics\n",
    "\n",
    "For our final evaluation, we're going to be leveraging work done in [this](https://github.com/NVIDIA/GenerativeAIExamples/blob/main/nemo/retriever-synthetic-data-generation/notebooks/quickstart.ipynb) notebook to create a BeIR format dataset created with Synthetic Data Generation. \n",
    "\n",
    "The output from the above notebook should be a dataset with the following items which can be found in the `outputs/sample_synthetic_data/beir/filtered/synthetic` directory after running the notebook:\n",
    "\n",
    "- `corpus.jsonl`\n",
    "- `qrels/test.tsv`\n",
    "- `queries.jsonl`\n",
    "\n",
    "This notebook assumes you've run the above notebook and have moved the `outputs/sample_synthetic_data/beir/filtered/synthetic` directory into the root folder of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following utility function to upload the folder contents to the NeMo Datastore microservice under the name \"SDG_BEIR\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset folder uploaded to: https://datastore.stg.llm.ngc.nvidia.com/datasets/nvidia/SDG_BEIR/tree/main/.\n"
     ]
    }
   ],
   "source": [
    "!python upload_sdg_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can once again create our RAG evaluation configuration while make a small change, which is to simply point at the newly updated dataset.\n",
    "\n",
    "> NOTE: We'll also include the [Answer Relevance](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html) metric from Ragas for this run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_sdg_config = {\n",
    "  \"model\": {\n",
    "    \"rag\": {\n",
    "        \"context_ordering\": \"desc\",\n",
    "        \"retriever\": {\n",
    "            \"top_k\": 10,\n",
    "            \"query_embedding_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "            \"query_embedding_model\": \"nvidia/nv-embedqa-e5-v5\",\n",
    "            \"index_embedding_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "            \"index_embedding_model\": \"nvidia/nv-embedqa-e5-v5\"\n",
    "        },\n",
    "        \"llm\": {\n",
    "            \"inference_url\": \"http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1\",\n",
    "            \"llm_name\": \"meta/llama-3_1-8b-instruct\"\n",
    "        }\n",
    "    }\n",
    "  },\n",
    "  \"evaluations\": [\n",
    "    {\n",
    "        \"eval_type\": \"automatic\",\n",
    "        \"eval_subtype\": \"beir\",\n",
    "        \"dataset_path\": \"nds:SDG_BEIR\",\n",
    "        \"dataset_format\" : \"beir\",\n",
    "        \"retriever_metrics\": \"recall_5,ndcg_cut_5,recall_10,ndcg_cut_10\",\n",
    "        \"rag_metrics\": \"faithfulness,answer_relevancy\",\n",
    "        \"judge_llm\": \"meta/llama-3_1-8b-instruct\",\n",
    "        \"judge_llm_url\": \"http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1\",\n",
    "        \"judge_llm_api_key\": None,\n",
    "        \"judge_embeddings\": \"nvidia/nv-embedqa-e5-v5\",\n",
    "        \"judge_embeddings_url\": \"http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings\",\n",
    "        \"judge_embeddings_api_key\": None,\n",
    "        \"judge_timeout\": 120,\n",
    "        \"judge_max_retries\": 2,\n",
    "        \"judge_max_workers\": 16\n",
    "    }\n",
    "  ],\n",
    "  \"tag\": \"rag-eval-sdg-beir\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fire off that evaluation job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ID: eval-7wNGSbr3vAWJrdkktjrc3J\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(evaluator_endpoint, json=rag_eval_sdg_config).json()\n",
    "rag_sdg_evaluation_id = response[\"evaluation_id\"]\n",
    "print(f\"Evaluation ID: {rag_sdg_evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can use the API to determine when the run has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_id': 'eval-7wNGSbr3vAWJrdkktjrc3J',\n",
       " 'status': 'succeeded',\n",
       " 'model': {'llm_name': None,\n",
       "  'inference_url': 'http://meta-llama3-8b-instruct.nim-meta-llama3-8b-instruct.svc.cluster.local:8000/v1',\n",
       "  'llm': None,\n",
       "  'retriever': None,\n",
       "  'rag': {'retriever': {'top_k': 10,\n",
       "    'query_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "    'query_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "    'index_embedding_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "    'index_embedding_model': 'nvidia/nv-embedqa-e5-v5',\n",
       "    'ranker_model': None,\n",
       "    'ranker_url': None},\n",
       "   'llm': {'llm_name': 'meta/llama-3_1-8b-instruct',\n",
       "    'inference_url': 'http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1'}}},\n",
       " 'evaluations': [{'eval_type': 'automatic',\n",
       "   'eval_subtype': 'beir',\n",
       "   'dataset_path': 'nds:SDG_BEIR',\n",
       "   'dataset_format': 'beir',\n",
       "   'retriever_metrics': 'recall_5,ndcg_cut_5,recall_10,ndcg_cut_10',\n",
       "   'rag_metrics': 'faithfulness,answer_relevancy',\n",
       "   'judge_llm': 'meta/llama-3_1-8b-instruct',\n",
       "   'judge_llm_url': 'http://meta-llama3-1-8b-instruct.nim-meta-llama3-1-8b-instruct.svc.cluster.local:8000/v1',\n",
       "   'judge_embeddings': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'judge_embeddings_url': 'http://nemo-embedding-ms.nemo-retrieval.svc.cluster.local:8080/v1/embeddings',\n",
       "   'judge_timeout': 120,\n",
       "   'judge_max_retries': 2,\n",
       "   'judge_max_workers': 16}],\n",
       " 'tag': 'rag-eval-sdg-beir',\n",
       " 'created_at': '2024-09-26T00:37:26',\n",
       " 'created_by': None,\n",
       " 'evaluation_results': [{'level_name': 'evaluation',\n",
       "   'isRecommended': True,\n",
       "   'extra_grouping_fields': None,\n",
       "   'metrics': [{'name': 'ndcg_cut_5',\n",
       "     'value': 1.0,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'ndcg_cut_10',\n",
       "     'value': 1.0,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'recall_5',\n",
       "     'value': 1.0,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'recall_10',\n",
       "     'value': 1.0,\n",
       "     'metadata': {'name': 'beir', 'category': 'retriever'}},\n",
       "    {'name': 'faithfulness',\n",
       "     'value': 0.7383918550585218,\n",
       "     'metadata': {'name': 'beir', 'category': 'generation'}},\n",
       "    {'name': 'answer_relevancy',\n",
       "     'value': 0.4565319083735558,\n",
       "     'metadata': {'name': 'beir', 'category': 'generation'}}],\n",
       "   'evaluation_results': None}]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_id_endpoint = evaluator_endpoint + f\"/{rag_sdg_evaluation_id}\"\n",
    "response = requests.get(evaluation_id_endpoint).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download, unzip, and view the results on our synthetically created evaluation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "url = evaluator_endpoint + f\"/{rag_sdg_evaluation_id}\" + \"/download-results\"\n",
    "headers = {'accept': 'application/json'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open('result_sdg.zip', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Results downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download results. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  result_sdg.zip\n",
      " extracting: results_sdg/metadata.json  \n",
      " extracting: results_sdg/.gitattributes  \n",
      " extracting: results_sdg/automatic/beir/model_config_custom_rag_model.yaml  \n",
      " extracting: results_sdg/automatic/beir/haystack_yaml/index.yaml  \n",
      " extracting: results_sdg/automatic/beir/haystack_yaml/query.yaml  \n",
      " extracting: results_sdg/automatic/beir/results/cleanup_milvus-run.log  \n",
      " extracting: results_sdg/automatic/beir/results/scores.jsonl  \n",
      " extracting: results_sdg/automatic/beir/results/README.md  \n",
      " extracting: results_sdg/automatic/beir/results/rag-run.log  \n",
      " extracting: results_sdg/automatic/beir/SDG_BEIR/queries.jsonl  \n",
      " extracting: results_sdg/automatic/beir/SDG_BEIR/corpus.jsonl  \n",
      " extracting: results_sdg/automatic/beir/SDG_BEIR/.gitattributes  \n",
      " extracting: results_sdg/automatic/beir/SDG_BEIR/qrels/test.tsv  \n"
     ]
    }
   ],
   "source": [
    "!unzip result_sdg.zip -d results_sdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever Scores:\n",
      "  ndcg_cut_5: 1.0000\n",
      "  ndcg_cut_10: 1.0000\n",
      "  recall_5: 1.0000\n",
      "  recall_10: 1.0000\n",
      "\n",
      "Generation Scores:\n",
      "  faithfulness: 0.7384\n",
      "  answer_relevancy: 0.4565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_jsonl_scores(\"./results_sdg/automatic/beir/results/scores.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® NeMo Data Designer 101: Seeding synthetic data generation with an external dataset\n",
    "\n",
    "> ‚ö†Ô∏è **Warning**: NeMo Data Designer is current in Early Release and is not recommended for production use.\n",
    ">\n",
    "> **Note**: In order to run this notebook, you must have the NeMo Data Designer microservice deployed locally via docker compose. See the [deployment guide](https://aire.gitlab-master-pages.nvidia.com/microservices/nmp/latest/nemo-microservices/latest/set-up/deploy-as-microservices/data-designer/docker-compose.html) for more details.\n",
    "\n",
    "<br>\n",
    "\n",
    "In this notebook, we will demonstrate how to seed synthetic data generation in Data Designer with an external dataset.\n",
    "\n",
    "If this is your first time using Data Designer, we recommend starting with the [first notebook](./1-the-basics.ipynb) in this 101 series.\n",
    "\n",
    "#### üíæ Install `nemo-microservices` with the `[data-designer]` extra option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \"nemo-microservices[data-designer]==1.1.0rc4\" --index-url https://urm.nvidia.com/artifactory/api/pypi/nv-shared-pypi/simple\n",
    "%pip install huggingface-hub\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the installation worked, you should be able to make the following imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_microservices import NeMoMicroservices\n",
    "from nemo_microservices.beta.data_designer import (\n",
    "    DataDesignerConfigBuilder,\n",
    "    DataDesignerClient,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Initialize the NeMo Data Designer (NDD) Client\n",
    "\n",
    "- The NDD client is responsible for submitting generation requests to the Data Designer microservice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndd = DataDesignerClient(client=NeMoMicroservices(base_url=\"http://localhost:8000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Initialize the Data Designer Config Builder\n",
    "\n",
    "- The Data Designer config defines the dataset schema and generation process.\n",
    "\n",
    "- The config builder provides an intuitive interface for building this configuration.\n",
    "\n",
    "- You must provide a list of model configs to the builder at initialization.\n",
    "\n",
    "- This list contains the models you can choose from (via the `model_alias` argument) during the generation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also load the model configs from a YAML string or file.\n",
    "# For the API key to be visible in the microservice, you must set\n",
    "# the MODEL_API_KEY_NIM environment variable in the data-designer\n",
    "# section of the docker-compose file. Note the custom environment\n",
    "# variable names must start with the prefix \"MODEL_API_KEY_\".\n",
    "\n",
    "model_configs_yaml = \"\"\"\\\n",
    "model_configs:\n",
    "  - alias: llama\n",
    "    inference_parameters:\n",
    "      max_tokens: 1024\n",
    "      temperature: 0.5\n",
    "      top_p: 1.0\n",
    "    model:\n",
    "      api_endpoint:\n",
    "        api_key: \"@os.environ/MODEL_API_KEY_NIM\"\n",
    "        model_id: \"meta/llama-3.3-70b-instruct\"\n",
    "        url: \"https://nim.int.aire.nvidia.com/v1\"\n",
    "\"\"\"\n",
    "\n",
    "config_builder = DataDesignerConfigBuilder(model_configs=model_configs_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè• Download a seed dataset\n",
    "\n",
    "- For this notebook, we'll change gears and create a synthetic dataset of patient notes.\n",
    "\n",
    "- To steer the generation process, we will use an open-source [symptom-to-diagnosis dataset](https://huggingface.co/datasets/gretelai/symptom_to_diagnosis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "df_seed = load_dataset(\"gretelai/symptom_to_diagnosis\")[\"train\"].to_pandas()\n",
    "\n",
    "# Rename the columns to something more descriptive.\n",
    "df_seed = df_seed.rename(\n",
    "    columns={\"output_text\": \"diagnosis\", \"input_text\": \"patient_summary\"}\n",
    ")\n",
    "\n",
    "print(f\"Number of records: {len(df_seed)}\")\n",
    "\n",
    "# Save the file so we can upload it to the microservice.\n",
    "df_seed.to_csv(\"symptom_to_diagnosis.csv\", index=False)\n",
    "\n",
    "df_seed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Designing our synthetic patient notes dataset\n",
    "\n",
    "- We set the seed dataset using the `with_seed_dataset` method.\n",
    "\n",
    "- We use the `shuffle` sampling strategy, which shuffles the seed dataset before sampling.\n",
    "\n",
    "- We set `with_replacement=False`, which limits our max number of records to 853, which is the number of records in the seed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The repo_id and filename arguments follow the Hugging Face Hub API format.\n",
    "# Passing the dataset_path argument signals that we need to upload the dataset\n",
    "# to the datastore. Note we need to pass in the datastore's endpoint, which\n",
    "# must match the endpoint in the docker-compose file.\n",
    "config_builder.with_seed_dataset(\n",
    "    repo_id=\"into-tutorials/seeding-with-a-dataset\",\n",
    "    filename=\"symptom_to_diagnosis.csv\",\n",
    "    dataset_path=\"./symptom_to_diagnosis.csv\",\n",
    "    sampling_strategy=\"shuffle\",\n",
    "    with_replacement=False,\n",
    "    datastore={\"endpoint\": \"http://localhost:3000/v1/hf\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we often just want a few attributes from Person objects, we can use\n",
    "# Data Designer's `with_person_samplers` method to create multiple person samplers\n",
    "# at once and drop the person object columns from the final dataset.\n",
    "\n",
    "# Empty dictionaries mean use default settings for the person samplers.\n",
    "config_builder.with_person_samplers({\"patient_sampler\": {}, \"doctor_sampler\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we demonstrate how you can add a column by calling `add_column` with the\n",
    "# column name, column type, and any parameters for that column type. This is in\n",
    "# contrast to using the column and parameter type objects, via `C` and `P`, as we\n",
    "# did in the previous notebooks. Generally, we recommend using the concrete column\n",
    "# and parameter type objects, but this is a convenient shorthand when you are\n",
    "# familiar with the required arguments for each type.\n",
    "\n",
    "config_builder.add_column(\n",
    "    name=\"patient_id\",\n",
    "    type=\"uuid\",\n",
    "    params={\"prefix\": \"PT-\", \"short_form\": True, \"uppercase\": True},\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    name=\"first_name\",\n",
    "    type=\"expression\",\n",
    "    expr=\"{{ patient_sampler.first_name}} \",\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    name=\"last_name\",\n",
    "    type=\"expression\",\n",
    "    expr=\"{{ patient_sampler.last_name }}\",\n",
    ")\n",
    "\n",
    "\n",
    "config_builder.add_column(\n",
    "    name=\"dob\", type=\"expression\", expr=\"{{ patient_sampler.birth_date }}\"\n",
    ")\n",
    "\n",
    "\n",
    "config_builder.add_column(\n",
    "    name=\"patient_email\",\n",
    "    type=\"expression\",\n",
    "    expr=\"{{ patient_sampler.email_address }}\",\n",
    ")\n",
    "\n",
    "\n",
    "config_builder.add_column(\n",
    "    name=\"symptom_onset_date\",\n",
    "    type=\"datetime\",\n",
    "    params={\"start\": \"2024-01-01\", \"end\": \"2024-12-31\"},\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    name=\"date_of_visit\",\n",
    "    type=\"timedelta\",\n",
    "    params={\"dt_min\": 1, \"dt_max\": 30, \"reference_column_name\": \"symptom_onset_date\"},\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    name=\"physician\",\n",
    "    type=\"expression\",\n",
    "    expr=\"Dr. {{ doctor_sampler.last_name }}\",\n",
    ")\n",
    "\n",
    "# Note we have access to the seed data fields.\n",
    "config_builder.add_column(\n",
    "    name=\"physician_notes\",\n",
    "    prompt=\"\"\"\\\n",
    "You are a primary-care physician who just had an appointment with {{ first_name }} {{ last_name }},\n",
    "who has been struggling with symptoms from {{ diagnosis }} since {{ symptom_onset_date }}.\n",
    "The date of today's visit is {{ date_of_visit }}.\n",
    "\n",
    "{{ patient_summary }}\n",
    "\n",
    "Write careful notes about your visit with {{ first_name }},\n",
    "as Dr. {{ doctor_sampler.first_name }} {{ doctor_sampler.last_name }}.\n",
    "\n",
    "Format the notes as a busy doctor might.\n",
    "\"\"\",\n",
    "    model_alias=\"llama\",\n",
    ")\n",
    "\n",
    "config_builder.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÄ Preview the dataset\n",
    "\n",
    "- Iteration is key to generating high-quality synthetic data.\n",
    "\n",
    "- Use the `preview` method to generate 10 records for inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = ndd.preview(config_builder, verbose_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preview dataset is available as a pandas DataFrame.\n",
    "preview.dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell multiple times to cycle through the 10 preview records.\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ Generate your dataset\n",
    "\n",
    "- Once you are happy with the preview, scale up to a larger dataset.\n",
    "\n",
    "- The `create` method will submit your generation job to the microservice and return a results object.\n",
    "\n",
    "- If you want to wait for the job to complete, set `wait_until_done=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ndd.create(config_builder, num_records=20, wait_until_done=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset into a pandas DataFrame\n",
    "dataset = results.load_dataset()\n",
    "\n",
    "dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

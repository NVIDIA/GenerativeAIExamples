{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f70a20",
   "metadata": {},
   "source": [
    "# Part III: Model Evaluation Using NeMo Evaluator\n",
    "\n",
    "This notebook covers the following:\n",
    "\n",
    "* [Pre-requisites: Configurations and Health Checks](#step-0)\n",
    "* [Evaluate the Custom Model on Scidocs (BEIR) zero-shot benchmark](#step-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb04eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep, time\n",
    "from nemo_microservices import NeMoMicroservices\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbe8b5",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-0\"></a>\n",
    "## Prerequisites: Configurations and Health Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076335b3",
   "metadata": {},
   "source": [
    "Before you proceed, make sure that you completed the previous notebooks on data preparation and model fine-tuning to obtain the assets required to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b0ffff",
   "metadata": {},
   "source": [
    "### Configure NeMo Microservices Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d85f8",
   "metadata": {},
   "source": [
    "The following code imports necessary configurations for the NeMo Data Store, Entity Store, Customizer, Evaluator, and NIM, as well as the namespace and base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NeMo Microservices SDK client\n",
    "nemo_client = NeMoMicroservices(\n",
    "    base_url=NEMO_URL,\n",
    "    inference_base_url=NIM_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b017a8",
   "metadata": {},
   "source": [
    "Paste the Embedding Model Name from your previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = f\"{NMS_NAMESPACE}/{OUTPUT_MODEL_NAME_EMBEDDING}\" # update this if you used a different name\n",
    "\n",
    "# Check if the embedding model is running locally as an NVIDIA NIM (pod in your cluster)\n",
    "models = nemo_client.inference.models.list()\n",
    "model_names = [model.id for model in models.data]\n",
    "\n",
    "assert EMBEDDING_MODEL_NAME in model_names, \\\n",
    "    f\"Model {EMBEDDING_MODEL_NAME} not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d6c31",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-1\"></a>\n",
    "## Evaluate the Custom Model\n",
    "\n",
    "For the purposes of showcasing zero-shot generalization, we will run the `SciDocs` benchmark from the [Benchmarking Information Retrieval (BEIR)](https://github.com/beir-cellar/beir) benchmark suite.\n",
    "\n",
    "We choose the `SciDocs` benchmark because its core purpose is to assess a model's ability to find and retrieve a scientific paper that should be cited by another given paper. While this benchmark data has differences from the `SPECTER` dataset we used for training (such as the length of the passages), it remains within the scientific domain and serves as a good test of the model's generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd56cc3",
   "metadata": {},
   "source": [
    "### Create a Target Configuration\n",
    "\n",
    "The first step in evaluation is to create a target configuration. This specifies parameters for the target endpoint being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d476a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_URL = f\"{NIM_URL}/v1/embeddings\"\n",
    "\n",
    "print(\"Embedding URL: \", EMBEDDING_URL)\n",
    "print(\"Embedding Model Name: \", EMBEDDING_MODEL_NAME)\n",
    "\n",
    "retriever_target_config = {\n",
    " \"type\": \"retriever\",\n",
    " \"retriever\": {\n",
    "   \"pipeline\": {\n",
    "     \"query_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": EMBEDDING_URL,\n",
    "         \"model_id\": EMBEDDING_MODEL_NAME,\n",
    "         \"api_key\": \"\"\n",
    "       }\n",
    "     },\n",
    "     \"index_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": EMBEDDING_URL,\n",
    "         \"model_id\": EMBEDDING_MODEL_NAME,\n",
    "         \"api_key\": \"\"\n",
    "       }\n",
    "     },\n",
    "     \"top_k\": 10\n",
    "   }\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1243c",
   "metadata": {},
   "source": [
    "### Create an Evaluation Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c1a4c",
   "metadata": {},
   "source": [
    "NeMo Evaluator supports the `BEIR` format for evaluation, and can also run the `BEIR` benchmark itself, of which SciDocs is a part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331577d7",
   "metadata": {},
   "source": [
    "You may find more information about retriever model evaluation, including supported metrics, in the [documentation](https://docs.nvidia.com/nemo/microservices/latest/evaluate/evaluation-types/retriever.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_eval_config_scidocs = {\n",
    "    \"type\": \"retriever\",\n",
    "    \"namespace\": NMS_NAMESPACE,\n",
    "    \"tasks\": {\n",
    "        \"my-beir-task\": {\n",
    "            \"type\": \"beir\",\n",
    "            \"dataset\": {\n",
    "                \"files_url\": \"file://scidocs/\"\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"recall_5\": {\"type\": \"recall_5\"},\n",
    "                \"ndcg_cut_5\": {\"type\": \"ndcg_cut_5\"},\n",
    "                \"recall_10\": {\"type\": \"recall_10\"},\n",
    "                \"ndcg_cut_10\": {\"type\": \"ndcg_cut_10\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f0bb9",
   "metadata": {},
   "source": [
    "`NOTE`: Above we add a configuration to calculate the recall and NDCG metrics at cuttoffs (k) = 5 and 10.\n",
    "\n",
    "* `recall@k`:  This metric measures the fraction of all existing relevant items that are successfully found within the top k results returned by a system.\n",
    "\n",
    "* `NDCG@k`: This metric evaluates how well a system ranks items by relevance within the top k positions, assigning more value to placing highly relevant items at the very top of the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74181e60",
   "metadata": {},
   "source": [
    "### Create an Evaluation Job\n",
    "\n",
    "The following code cell creates an evaluation job using the target and eval configurations defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca284d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation job for the base model\n",
    "eval_job = nemo_client.evaluation.jobs.create(\n",
    "    config=retriever_eval_config_scidocs,\n",
    "    target=retriever_target_config\n",
    ")\n",
    "\n",
    "print(\"Evaluation job created: \", eval_job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80936290",
   "metadata": {},
   "source": [
    "### Monitor the Evaluation Job\n",
    "\n",
    "The following code cell defines a helper function to poll on the job status using the `nemo_client.evaluation.jobs.retrieve` method.\n",
    "\n",
    "**The evaluation will take about 10-12 minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046082a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_eval_job(nemo_client, job_id: str, polling_interval: int = 10, timeout: int = 6000):\n",
    "    \"\"\"Helper for waiting an eval job.\"\"\"\n",
    "    start_time = time()\n",
    "    job = nemo_client.evaluation.jobs.retrieve(job_id=job_id)\n",
    "    status = job.status\n",
    "\n",
    "    while (status in [\"pending\", \"created\", \"running\"]):\n",
    "        # Check for timeout\n",
    "        if time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Took more than {timeout} seconds.\")\n",
    "\n",
    "        # Sleep before polling again\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        # Fetch updated status and progress\n",
    "        job = nemo_client.evaluation.jobs.retrieve(job_id=job_id)\n",
    "        status = job.status\n",
    "\n",
    "        # Progress details (only fetch if status is \"running\")\n",
    "        progress = 0\n",
    "        if status == \"running\" and job.status_details:\n",
    "            progress = job.status_details.progress or 0\n",
    "        elif status == \"completed\":\n",
    "            progress = 100\n",
    "\n",
    "        print(f\"Job status: {status} after {time() - start_time:.2f} seconds. Progress: {progress}%\")\n",
    "\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac11757",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = wait_eval_job(nemo_client, eval_job.id, polling_interval=5, timeout=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c1edde",
   "metadata": {},
   "source": [
    "### Review the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nemo_client.evaluation.jobs.results(job_id=eval_job.id)\n",
    "print(results.model_dump_json(indent=2, exclude_unset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc76a48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "âœ… **Completed in this notebook:**\n",
    "- Configured evaluation target for the fine-tuned embedding model\n",
    "- Created evaluation configuration for the BEIR SciDocs benchmark\n",
    "- Ran evaluation job measuring recall@5, recall@10, NDCG@5, and NDCG@10 metrics\n",
    "- Analyzed results: recall@5 improved from approximately 0.159 (baseline `nvidia/llama-3_2-nv-embedqa-1b-v2`) to 0.176\n",
    "\n",
    "**What you've achieved:**\n",
    "\n",
    "Through this three-part tutorial series, you've completed the full embedding fine-tuning workflow: prepared domain-specific training data, fine-tuned `nvidia/llama-3.2-nv-embedqa-1b-v2` for improved scientific retrieval, deployed your custom model as a NIM, and evaluated performance on the challenging SciDocs zero-shot benchmark.\n",
    "\n",
    "**Next:**\n",
    "- Explore other [NeMo Microservices tutorials](../../../README.md) for LLM customization, RAG evaluation, and guardrails\n",
    "- Visit the [NeMo Microservices documentation](https://docs.nvidia.com/nemo/microservices/latest/about/index.html) to learn more about advanced features\n",
    "- Apply these techniques to your own domain-specific datasets for even better retrieval quality\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

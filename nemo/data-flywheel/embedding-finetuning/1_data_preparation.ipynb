{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c429ee79",
   "metadata": {},
   "source": [
    "# Part I: Preparing the Dataset\n",
    "\n",
    "This notebook showcases transforming a dataset for finetuning an embedding model with NeMo Microservices.\n",
    "\n",
    "\n",
    "It covers the following -\n",
    "1. Download the SPECTER dataset\n",
    "2. Prepare data for embedding fine-tuning.\n",
    "\n",
    "*Dataset Disclaimer: Each user is responsible for checking the content of datasets and the applicable licenses and determining if suitable for the intended use.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c70985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from config import HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b165f5",
   "metadata": {},
   "source": [
    "The following code cell sets a random seed for reproducibility, and sets data path.\n",
    "It also configures the fraction of training data to use for demonstration purposes as training with the whole [SPECTER](https://huggingface.co/datasets/embedding-data/SPECTER) dataset may take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "DATA_SAVE_PATH = \"./data\"\n",
    "\n",
    "# Configuration for data fraction\n",
    "USE_FRACTION = True  # Set to False to use full dataset\n",
    "FRACTION = 0.1  # Use 10% of the dataset (0.1 = 10%, 0.01 = 1%, etc.)\n",
    "\n",
    "# Configure Hugging Face access\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf17cc",
   "metadata": {},
   "source": [
    "## Step 1: Download the SPECTER Dataset\n",
    "\n",
    "The SPECTER dataset contains scientific paper triples for training embedding models. This step loads the dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset directly from Hugging Face\n",
    "dataset = load_dataset(\"embedding-data/SPECTER\")\n",
    "print(f\"Dataset info: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a3558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(f\"  Shape: {len(dataset['train']):,} rows × 1 column\")\n",
    "print(f\"  Column name: '{dataset['train'].column_names[0]}'\")\n",
    "print(f\"  Each row's value: A 3-element list [query_title, positive_title, negative_title]\")\n",
    "print(f\"\\n  Example of one row:\")\n",
    "example = dataset['train'][0]['set']\n",
    "print(f\"    dataset['train'][0]['set'] = [\")\n",
    "print(f\"      '{example[0][:60]}...',  # query\")\n",
    "print(f\"      '{example[1][:60]}...',  # positive\")\n",
    "print(f\"      '{example[2][:60]}...'   # negative\")\n",
    "print(f\"    ]\")\n",
    "\n",
    "# Display diverse examples from different parts of the dataset\n",
    "print(\"\\nDiverse examples from the dataset:\")\n",
    "sample_indices = [0, 100000, 200000]\n",
    "for idx, sample_idx in enumerate(sample_indices, 1):\n",
    "    example = dataset[\"train\"][sample_idx][\"set\"]\n",
    "    print(f\"\\nExample {idx} (row {sample_idx:,}):\")\n",
    "    print(f\"  Query (Anchor):       {example[0][:100]}{'...' if len(example[0]) > 100 else ''}\")\n",
    "    print(f\"  Positive (Related):   {example[1][:100]}{'...' if len(example[1]) > 100 else ''}\")\n",
    "    print(f\"  Negative (Unrelated): {example[2][:100]}{'...' if len(example[2]) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08700f5d",
   "metadata": {},
   "source": [
    "Each row in the dataset contains a triplet of scientific paper titles:\n",
    "- **Query (Anchor)**: The reference paper\n",
    "- **Positive (Related)**: A related or cited paper  \n",
    "- **Negative (Unrelated)**: An unrelated paper from a different field\n",
    "\n",
    "During training, contrastive learning is used to maximize the similarity between the query paper and related papers, while minimizing the similarity between the query paper and unrelated papers. This teaches the embedding model to understand semantic relationships between scientific documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba489337",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data for Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c498d7",
   "metadata": {},
   "source": [
    "For customizing embedding models, the NeMo Microservices platform leverages a JSONL format, where each row is:\n",
    "```\n",
    "{\n",
    "    \"query\": \"query text\",\n",
    "    \"pos_doc\": \"positive document text\",\n",
    "    \"neg_doc\": [\"negative document text 1\", \"negative document text 2\", ...]\n",
    "}\n",
    "```\n",
    "\n",
    "The following code cell converts the dataset to this format and splits it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96197d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data fraction if configured\n",
    "data = dataset['train'].shuffle(seed=SEED)\n",
    "if USE_FRACTION:\n",
    "    data = data.select(range(int(len(data) * FRACTION)))\n",
    "    print(f\"Using {len(data)}/{len(dataset['train'])} examples ({FRACTION*100:.1f}%)\")\n",
    "\n",
    "# Split: 90% train, 5% validation, 5% test\n",
    "train_val = data.train_test_split(test_size=0.10, seed=SEED)\n",
    "val_test = train_val['test'].train_test_split(test_size=0.50, seed=SEED)\n",
    "splits = {'train': train_val['train'], 'validation': val_test['train'], 'test': val_test['test']}\n",
    "\n",
    "print(f\"Train: {len(splits['train'])} | Validation: {len(splits['validation'])} | Test: {len(splits['test'])}\\n\")\n",
    "\n",
    "# Save to JSONL format\n",
    "folder_name = f\"specter_{int(FRACTION*100)}pct\" if USE_FRACTION else \"specter_full\"\n",
    "save_path = os.path.join(DATA_SAVE_PATH, folder_name)\n",
    "\n",
    "for split_name, split_data in [(\"training\", splits['train']), (\"validation\", splits['validation']), (\"testing\", splits['test'])]:\n",
    "    split_dir = os.path.join(save_path, split_name)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(split_dir, f\"{split_name}.jsonl\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for row in split_data:\n",
    "            example = {\"query\": row['set'][0], \"pos_doc\": row['set'][1], \"neg_doc\": [row['set'][2]]}\n",
    "            f.write(json.dumps(example) + \"\\n\")\n",
    "    print(f\"Saved {file_path}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample from training set:\")\n",
    "for i, row in enumerate(splits['train'].select(range(3))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Query:    {row['set'][0]}\")\n",
    "    print(f\"  Positive: {row['set'][1]}\")\n",
    "    print(f\"  Negative: {row['set'][2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47a9d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "✅ **Completed in this notebook:**\n",
    "- Loaded the SPECTER dataset containing scientific paper triplets\n",
    "- Inspected the dataset structure and sample examples\n",
    "- Converted the data to NeMo Microservices JSONL format\n",
    "- Split the data into training (90%), validation (5%), and test (5%) sets\n",
    "- Saved the processed data for fine-tuning\n",
    "\n",
    "**Continue to [2_finetuning_and_inference.ipynb](./2_finetuning_and_inference.ipynb)** to:\n",
    "- Upload the prepared dataset to NeMo Data Store\n",
    "- Configure and launch the embedding fine-tuning job\n",
    "- Deploy the fine-tuned model as a NIM\n",
    "- Run inference and test the customized model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c429ee79",
   "metadata": {},
   "source": [
    "# Part I: Preparing the Dataset\n",
    "\n",
    "This notebook showcases transforming a dataset for finetuning an embedding model with NeMo Microservices.\n",
    "\n",
    "\n",
    "It covers the following -\n",
    "1. Download the SPECTER dataset\n",
    "2. Prepare data for embedding fine-tuning.\n",
    "\n",
    "*Dataset Disclaimer: Each user is responsible for checking the content of datasets and the applicable licenses and determining if suitable for the intended use.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c70985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b165f5",
   "metadata": {},
   "source": [
    "The following code cell sets a random seed for reproducibility, and sets data path.\n",
    "It also configures the fraction of training data to use for demonstration purposes as training with the whole [SPECTER](https://huggingface.co/datasets/embedding-data/SPECTER) data may take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403c5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "DATA_SAVE_PATH = \"./data\"\n",
    "\n",
    "# Configuration for data fraction\n",
    "USE_FRACTION = True  # Set to False to use full dataset\n",
    "FRACTION = 0.1  # Use 10% of the dataset (0.1 = 10%, 0.01 = 1%, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf17cc",
   "metadata": {},
   "source": [
    "## Step 1: Download the SPECTER Dataset\n",
    "\n",
    "The SPECTER dataset contains scientific paper triples for training embedding models. This step loads the dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76c59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import HF_TOKEN\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d9650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['set'],\n",
      "        num_rows: 684100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset directly from Hugging Face\n",
    "dataset = load_dataset(\"embedding-data/SPECTER\")\n",
    "print(f\"Dataset info: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189a3558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Millimeter-wave CMOS digital controlled artificial dielectric differential mode transmission lines for reconfigurable ICs',\n",
       "  'CMP network-on-chip overlaid with multi-band RF-interconnect',\n",
       "  'Route packets, not wires: on-chip interconnection networks'],\n",
       " ['Millimeter-wave CMOS digital controlled artificial dielectric differential mode transmission lines for reconfigurable ICs',\n",
       "  'CMP network-on-chip overlaid with multi-band RF-interconnect',\n",
       "  'Entheses: tendon and ligament attachment sites'],\n",
       " ['Millimeter-wave CMOS digital controlled artificial dielectric differential mode transmission lines for reconfigurable ICs',\n",
       "  'CMP network-on-chip overlaid with multi-band RF-interconnect',\n",
       "  'Packet leashes: a defense against wormhole attacks in wireless networks']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first 3 rows\n",
    "dataset[\"train\"][:3][\"set\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08700f5d",
   "metadata": {},
   "source": [
    "Each row in the dataset contains three sentences (or triplets): query, positive passage, and negative passage, in order.\n",
    "\n",
    "During training of the embedding model, contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passage not useful to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba489337",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data for Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c498d7",
   "metadata": {},
   "source": [
    "For customizing embedding models, the NeMo Microservices platform leverages a JSONL format, where each row is:\n",
    "```\n",
    "{\n",
    "    \"query\": \"query text\",\n",
    "    \"pos_doc\": \"positive document text\",\n",
    "    \"neg_doc\": [\"negative document text 1\", \"negative document text 2\", ...]\n",
    "}\n",
    "```\n",
    "\n",
    "The following code cell -\n",
    "1. Defines a helper for data splitting\n",
    "2. Uses a fraction of the data, and converts each row to the required format\n",
    "3. Saves the data splits to jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96197d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples in dataset: 684100\n",
      "Using fraction of dataset: 68410/684100 examples (10.0%)\n",
      "Formatted 68410 examples\n",
      "\n",
      "Train set: 61569 examples\n",
      "Validation set: 3420 examples\n",
      "Test set: 3421 examples\n",
      "Saving data to: ./data/specter_10pct\n",
      "Saved 61569 examples to ./data/specter_10pct/training/training.jsonl\n",
      "Saved 3420 examples to ./data/specter_10pct/validation/validation.jsonl\n",
      "Saved 3421 examples to ./data/specter_10pct/testing/testing.jsonl\n",
      "\n",
      "First few examples from training set:\n",
      "Example 1:\n",
      "  Query: Rhythm, Metrics, and the Link to Phonology\n",
      "  Positive: Rhythm, Timing and the Timing of Rhythm\n",
      "  Negative: ['Social software and participatory learning: Pedagogical choices with technology affordances in the Web 2.0 era']\n",
      "\n",
      "Example 2:\n",
      "  Query: underwater image processing : state of the art of restoration and image enhancement methods .\n",
      "  Positive: Image quality assessment: from error visibility to structural similarity\n",
      "  Negative: ['An overview of home automation systems']\n",
      "\n",
      "Example 3:\n",
      "  Query: Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing.\n",
      "  Positive: Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks\n",
      "  Negative: ['Regularized multi--task learning']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1\"\n",
    "    \n",
    "    # Compute split indices\n",
    "    train_end = int(len(data) * train_ratio)\n",
    "    val_end = train_end + int(len(data) * val_ratio)\n",
    "    \n",
    "    # Split the data\n",
    "    train_set = data[:train_end]\n",
    "    val_set = data[train_end:val_end]\n",
    "    test_set = data[val_end:]\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "\n",
    "try:\n",
    "    # Get the raw data\n",
    "    raw_data = dataset['train']['set']\n",
    "    print(f\"Total examples in dataset: {len(raw_data)}\")\n",
    "    \n",
    "    # Shuffle the data once at the beginning\n",
    "    raw_data_list = list(raw_data)\n",
    "    random.shuffle(raw_data_list)\n",
    "    \n",
    "    # Apply fraction if specified (after shuffling)\n",
    "    if USE_FRACTION:\n",
    "        original_size = len(raw_data_list)\n",
    "        fraction_size = int(len(raw_data_list) * FRACTION)\n",
    "        raw_data_list = raw_data_list[:fraction_size]\n",
    "        print(f\"Using fraction of dataset: {len(raw_data_list)}/{original_size} examples ({FRACTION*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"Using full dataset: {len(raw_data_list)} examples\")\n",
    "    \n",
    "    # Format the data\n",
    "    data = []\n",
    "    for example in raw_data_list:\n",
    "        data.append({\n",
    "            \"query\": example[0],\n",
    "            \"pos_doc\": example[1], \n",
    "            \"neg_doc\": [example[2]]  # neg_doc as a list of strings\n",
    "        })\n",
    "    print(f\"Formatted {len(data)} examples\")\n",
    "    \n",
    "    # Split the data\n",
    "    train, val, test = split_data(data, train_ratio=0.90, val_ratio=0.05, test_ratio=0.05)\n",
    "    \n",
    "    print(f\"\\nTrain set: {len(train)} examples\")\n",
    "    print(f\"Validation set: {len(val)} examples\")\n",
    "    print(f\"Test set: {len(test)} examples\")\n",
    "    \n",
    "    # Generate save path with fraction suffix if using a fraction of the dataset\n",
    "    if USE_FRACTION:\n",
    "        # Convert fraction to percentage for folder name (e.g., 0.1 -> 10pct, 0.01 -> 1pct)\n",
    "        fraction_pct = int(FRACTION * 100)\n",
    "        folder_name = f\"specter_{fraction_pct}pct\"\n",
    "    else:\n",
    "        folder_name = \"specter_full\"\n",
    "    \n",
    "    save_path = os.path.join(DATA_SAVE_PATH, folder_name)\n",
    "    print(f\"Saving data to: {save_path}\")\n",
    "    \n",
    "    # Create directories for each split\n",
    "    for split_name in [\"training\", \"validation\", \"testing\"]:\n",
    "        split_dir = os.path.join(save_path, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "    \n",
    "    # Save to JSONL files in respective folders\n",
    "    for fname, ds, folder in ((\"training.jsonl\", train, \"training\"), \n",
    "                              (\"validation.jsonl\", val, \"validation\"), \n",
    "                              (\"testing.jsonl\", test, \"testing\")):\n",
    "        file_path = os.path.join(save_path, folder, fname)\n",
    "        with open(file_path, \"w\") as out:\n",
    "            for obj in ds:\n",
    "                out.write(json.dumps(obj) + \"\\n\")\n",
    "        print(f\"Saved {len(ds)} examples to {file_path}\")\n",
    "    \n",
    "    # Display first few examples from training set\n",
    "    print(\"\\nFirst few examples from training set:\")\n",
    "    for i, example in enumerate(train[:3]):\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Query: {example['query']}\")\n",
    "        print(f\"  Positive: {example['pos_doc']}\")\n",
    "        print(f\"  Negative: {example['neg_doc']}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

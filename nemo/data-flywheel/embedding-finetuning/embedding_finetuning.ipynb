{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embedding Fine-tuning with NeMo Microservices\n",
        "\n",
        "Fine-tune an embedding model and improve retrieval by 6-10% in ~1 hour.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**Hardware:** 1 NVIDIA GPU (used sequentially: baseline → training → deployment)\n",
        "\n",
        "**Setup Steps:**\n",
        "\n",
        "1. **Deploy NeMo Microservices 25.8.0+**: Follow the [Minikube setup guide](https://docs.nvidia.com/nemo/microservices/latest/get-started/setup/minikube/).\n",
        "\n",
        "2. **Register base model** (~2-3 minutes):\n",
        "   ```bash\n",
        "   helm upgrade nemo nmp/nemo-microservices-helm-chart --namespace default --reuse-values \\\n",
        "     --set customizer.customizationTargets.overrideExistingTargets=false \\\n",
        "     --set 'customizer.customizationTargets.targets.nvidia/llama-3\\.2-nv-embedqa-1b@v2.enabled=true' && \\\n",
        "   kubectl delete pod -n default -l app.kubernetes.io/name=nemo-customizer && \\\n",
        "   kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=nemo-customizer -n default --timeout=5m\n",
        "   ```\n",
        "\n",
        "3. **HuggingFace token**: https://huggingface.co/settings/tokens (read access). Set `HF_TOKEN` env var or enter when prompted.\n",
        "\n",
        "4. **Service URLs**: Run `cat /etc/hosts` to find hostnames (typically `http://nemo.test`, `http://data-store.test`, `http://nim.test`).\n",
        "\n",
        "> **Note:** This notebook uses 1 GPU sequentially - Step 0 deploys baseline model, then deletes it to free the GPU for training (Step 3), then deploys fine-tuned model (Step 4).\n",
        "\n",
        "> **Tip:** Cleanup cells at the end of the notebook can be uncommented to let you delete resources if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "**Use case:** Adapt a general embedding model to find related scientific papers.\n",
        "\n",
        "Fine-tuning an embedding model on your domain data improves retrieval accuracy. In a Retrieval-Augmented Generation (RAG) pipeline, this means the LLM receives more relevant context, producing better answers. For search applications, users find what they need more often.\n",
        "\n",
        "This notebook walks through the complete workflow: fine-tune a base embedding model on scientific paper data, deploy it as a production NVIDIA Inference Microservice (NIM), and measure the improvement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Test the baseline model on a retrieval task\n",
        "- Fine-tune [`nvidia/llama-3.2-nv-embedqa-1b-v2`](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2) on 65K scientific paper triplets from [SPECTER dataset](https://huggingface.co/datasets/embedding-data/SPECTER)\n",
        "- Deploy the fine-tuned model as a production-ready NIM inference service\n",
        "- Compare before/after retrieval rankings on your original task\n",
        "- Measure aggregate improvement on [SciDocs benchmark](https://huggingface.co/datasets/BeIR/scidocs): Recall@5 improves from 0.159 to ~0.17 (+6-10%)\n",
        "\n",
        "**Recall@5** measures the fraction of relevant documents that appear in the top 5 search results.\n",
        "\n",
        "**About the baseline:** The 0.159 baseline was measured by running the same SciDocs evaluation on the pretrained model. In Step 6, you can set `EVALUATE_BASELINE = True` to run this evaluation yourself as long as you ensure the base model from Step 0 is deployed.\n",
        "\n",
        "**Note**: Time estimates are approximate and depend on cluster configuration and GPU type(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install -q datasets huggingface_hub openai nemo-microservices ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json, requests, os\n",
        "import numpy as np\n",
        "from time import sleep, time\n",
        "from datasets import load_dataset\n",
        "from getpass import getpass\n",
        "from nemo_microservices import NeMoMicroservices\n",
        "from huggingface_hub import HfApi\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "NDS_URL = \"http://data-store.test\"      # Get from: cat /etc/hosts\n",
        "NEMO_URL = \"http://nemo.test\"           # Get from: cat /etc/hosts\n",
        "NIM_URL = \"http://nim.test\"             # Get from: cat /etc/hosts\n",
        "\n",
        "# Credentials - prompts if not set via env var\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\") or getpass(\"HuggingFace token (https://huggingface.co/settings/tokens): \")\n",
        "NAMESPACE = os.environ.get(\"NAMESPACE\") or input(\"Namespace (e.g. yourname_embedding): \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeMo client initialized\n"
          ]
        }
      ],
      "source": [
        "# Initialize NeMo client\n",
        "nemo = NeMoMicroservices(base_url=NEMO_URL, inference_base_url=NIM_URL)\n",
        "print(\"NeMo client initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Identify the Opportunity\n",
        "\n",
        "Let's start with a real-world scenario: searching scientific papers by meaning, not keywords.\n",
        "\n",
        "We'll deploy the base embedding model, run a test query, and see where it struggles. Then we'll fine-tune on scientific paper data and measure the improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deploying base model...\n"
          ]
        }
      ],
      "source": [
        "# Deploy base model (~2 mins), run baseline ranking, then clean up\n",
        "BASE_MODEL = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
        "BASE_DEPLOYMENT = f\"{NAMESPACE}-baseline\"\n",
        "NIM_IMAGE = \"nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2\" # NIM container from NGC (https://www.nvidia.com/en-us/gpu-cloud/)\n",
        "NIM_TAG = \"1.6.0\" # Update if using newer NIM release\n",
        "\n",
        "# Check if already deployed, otherwise create\n",
        "try:\n",
        "    existing = nemo.deployment.model_deployments.retrieve(deployment_name=BASE_DEPLOYMENT, namespace=NAMESPACE)\n",
        "    print(f\"Base model already deployed (status: {existing.status_details.status})\")\n",
        "except:\n",
        "    print(\"Deploying base model...\")\n",
        "    nemo.deployment.model_deployments.create(\n",
        "        name=BASE_DEPLOYMENT, namespace=NAMESPACE,\n",
        "        config={\"model\": BASE_MODEL, \"nim_deployment\": {\"image_name\": NIM_IMAGE, \"image_tag\": NIM_TAG, \"gpu\": 1, \"disable_lora_support\": True}})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending | 0m 0s"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending | 2m 30s\n",
            "Ready | 2m\n"
          ]
        }
      ],
      "source": [
        "# Wait for base model to deploy\n",
        "POLL_INTERVAL = 10\n",
        "INIT_WAIT = 30\n",
        "\n",
        "start = time()\n",
        "while True:\n",
        "    status = nemo.deployment.model_deployments.retrieve(deployment_name=BASE_DEPLOYMENT, namespace=NAMESPACE)\n",
        "    if status.status_details.status == 'ready':\n",
        "        break\n",
        "    print(f\"\\rStatus: {status.status_details.status} | {int(time()-start)//60}m {int(time()-start)%60}s\", end=\"\")\n",
        "    sleep(POLL_INTERVAL)\n",
        "print(f\"\\nReady | {int(time()-start)//60}m\")\n",
        "sleep(INIT_WAIT)  # Wait for model to initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: \"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data\"\n",
            "\n",
            "Base Model Ranking:\n",
            "-------------------------------------------------------\n",
            "  #1  [0.387]  CRF Tutorial <-- relevant\n",
            "  #2  [0.334]  BiLSTM-CRF <-- relevant\n",
            "  #3  [0.204]  Random Forest\n",
            "  #4  [0.189]  SVM\n",
            "  #5  [0.177]  NER-CRF <-- relevant\n"
          ]
        }
      ],
      "source": [
        "# Demo: searching scientific papers where keyword matching fails\n",
        "# \n",
        "# Query: \"Conditional Random Fields\" (CRFs) - a method for sequence labeling in NLP.\n",
        "# Trap: \"Random Forests\" shares the word \"random\" but is an unrelated tree-based algorithm.\n",
        "# Can the model tell the difference?\n",
        "\n",
        "DEMO_QUERY = \"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data\"\n",
        "\n",
        "DEMO_DOCS = [\n",
        "    \"Bidirectional LSTM-CRF Models for Sequence Tagging\",   # CRF-based paper\n",
        "    \"An Introduction to Conditional Random Fields\",         # CRF tutorial  \n",
        "    \"Random Forests\",                                       # Keyword trap! Unrelated.\n",
        "    \"Neural Architectures for Named Entity Recognition\",    # Related to sequence labeling; may use CRFs\n",
        "    \"Support Vector Machines for Classification\",           # Unrelated ML method\n",
        "]\n",
        "\n",
        "DEMO_LABELS = [\"BiLSTM-CRF\", \"CRF Tutorial\", \"Random Forest\", \"NER-CRF\", \"SVM\"]\n",
        "DEMO_RELEVANT = {0, 1, 3}  # Papers actually relevant to CRFs\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Run baseline ranking\n",
        "base_client = OpenAI(base_url=f\"{NIM_URL}/v1\", api_key=\"None\")\n",
        "query_emb = base_client.embeddings.create(input=[DEMO_QUERY], model=BASE_MODEL, extra_body={\"input_type\": \"query\"}).data[0].embedding\n",
        "doc_embs = [base_client.embeddings.create(input=[d], model=BASE_MODEL, extra_body={\"input_type\": \"passage\"}).data[0].embedding for d in DEMO_DOCS]\n",
        "scores = [(i, cosine_similarity(query_emb, doc_embs[i])) for i in range(len(DEMO_DOCS))]\n",
        "BASELINE_RANKING = sorted(scores, key=lambda x: -x[1])\n",
        "\n",
        "print(f\"Query: \\\"{DEMO_QUERY}\\\"\\n\")\n",
        "print(\"Base Model Ranking:\")\n",
        "print(\"-\" * 55)\n",
        "for rank, (idx, score) in enumerate(BASELINE_RANKING, 1):\n",
        "    marker = \" <-- relevant\" if idx in DEMO_RELEVANT else \"\"\n",
        "    print(f\"  #{rank}  [{score:.3f}]  {DEMO_LABELS[idx]}{marker}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting base model deployment...\n",
            "GPU freed. Now let's fine-tune and see if we can improve these rankings.\n"
          ]
        }
      ],
      "source": [
        "# Delete base model deployment to free GPU for training\n",
        "# (Skip this cell if you want to run baseline evaluation in Step 6)\n",
        "print(\"Deleting base model deployment...\")\n",
        "nemo.deployment.model_deployments.delete(deployment_name=BASE_DEPLOYMENT, namespace=NAMESPACE)\n",
        "print(\"GPU freed. Now let's fine-tune and see if we can improve these rankings.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Prepare Data\n",
        "\n",
        "Download 10% of the SPECTER dataset containing ~684K scientific paper triplets (query, positive, negative) and format for embedding fine-tuning.\n",
        "\n",
        "**Dataset format:** Each triplet teaches the model via contrastive learning to maximize similarity between query and positive document while minimizing similarity between query and negative document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading SPECTER dataset...\n",
            "Splitting into train/validation...\n",
            "Saving to JSONL...\n",
            "Prepared 64,980 training, 3,420 validation samples\n",
            "\n",
            "Example triplet:\n",
            "  Query:    affective news : the automated coding of sentiment in political texts lori young and stuart soroka ....\n",
            "  Positive: Using WordNet to Measure Semantic Orientations of Adjectives...\n",
            "  Negative: Development and application of a metric on semantic nets...\n"
          ]
        }
      ],
      "source": [
        "# Download and prepare training data\n",
        "DATASET_SIZE = 68400      # 10% of full dataset (684K triplets) - increase for better results\n",
        "VALIDATION_SPLIT = 0.05   # 5% held out for validation\n",
        "\n",
        "print(\"Downloading SPECTER dataset...\")\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "data = load_dataset(\"embedding-data/SPECTER\")['train'].shuffle(seed=42).select(range(DATASET_SIZE))\n",
        "\n",
        "print(\"Splitting into train/validation...\")\n",
        "splits = data.train_test_split(test_size=VALIDATION_SPLIT, seed=42)\n",
        "train_data = splits['train']\n",
        "validation_data = splits['test']\n",
        "\n",
        "# Save as JSONL (required format for Customizer)\n",
        "# More details: https://docs.nvidia.com/nemo/microservices/latest/fine-tune/tutorials/format-training-dataset.html\n",
        "print(\"Saving to JSONL...\")\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "for name, dataset in [(\"training\", train_data), (\"validation\", validation_data)]:\n",
        "    with open(f\"data/{name}.jsonl\", \"w\") as f:\n",
        "        for row in dataset:\n",
        "            f.write(json.dumps({\"query\": row['set'][0], \"pos_doc\": row['set'][1], \"neg_doc\": [row['set'][2]]}) + \"\\n\")\n",
        "\n",
        "print(f\"Prepared {len(train_data):,} training, {len(validation_data):,} validation samples\")\n",
        "print(f\"\\nExample triplet:\")\n",
        "print(f\"  Query:    {train_data[0]['set'][0][:100]}...\")\n",
        "print(f\"  Positive: {train_data[0]['set'][1][:100]}...\")\n",
        "print(f\"  Negative: {train_data[0]['set'][2][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload to NeMo Data Store\n",
        "\n",
        "**NeMo Data Store** holds datasets for training and evaluation. It exposes a HuggingFace-compatible API, so you can use familiar `huggingface_hub` methods - just pointed at a different endpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating namespace...\n",
            "Creating repository...\n",
            "Uploading files...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "179b20f69eca45de8297cbede84ceefd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training.jsonl:   0%|          | 0.00/16.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2752b00ec378424cadb20371d6cb8ec0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation.jsonl:   0%|          | 0.00/880k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registering dataset...\n",
            "Upload complete\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating namespace...\")\n",
        "nemo.namespaces.create(id=NAMESPACE)  # For job management\n",
        "requests.post(f\"{NDS_URL}/v1/datastore/namespaces\", data={\"namespace\": NAMESPACE})  # NeMo Data Store namespace (separate service)\n",
        "\n",
        "print(\"Creating repository...\")\n",
        "hf = HfApi(endpoint=f\"{NDS_URL}/v1/hf\", token=None)\n",
        "hf.create_repo(f\"{NAMESPACE}/data\", repo_type='dataset')\n",
        "\n",
        "print(\"Uploading files...\")\n",
        "hf.upload_file(path_or_fileobj=\"data/training.jsonl\", path_in_repo=\"training/training.jsonl\", repo_id=f\"{NAMESPACE}/data\", repo_type='dataset')\n",
        "hf.upload_file(path_or_fileobj=\"data/validation.jsonl\", path_in_repo=\"validation/validation.jsonl\", repo_id=f\"{NAMESPACE}/data\", repo_type='dataset')\n",
        "\n",
        "print(\"Registering dataset...\")\n",
        "nemo.datasets.create(name=\"data\", namespace=NAMESPACE, files_url=f\"hf://datasets/{NAMESPACE}/data\")\n",
        "print(\"Upload complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Model\n",
        "\n",
        "Fine-tune using supervised contrastive learning (model learns to pull query-positive pairs closer while pushing query-negative pairs apart).\n",
        "\n",
        "**Config vs Job:** A *config* defines the training template (base model, GPU settings). A *job* runs training with that config + dataset + hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating config...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CustomizationConfig(max_seq_length=2048, training_options=[CustomizationTrainingOption(finetuning_type='all_weights', micro_batch_size=8, num_gpus=1, training_type='sft', data_parallel_size=None, expert_model_parallel_size=None, num_nodes=1, pipeline_parallel_size=1, tensor_parallel_size=1, use_sequence_parallel=False)], chat_prompt_template=None, created_at=datetime.datetime(2026, 1, 5, 19, 21, 44, 633543), custom_fields={}, dataset_schemas=[{'$schema': 'https://json-schema.org/draft/2020-12/schema', '$id': 'https://nemo.nvidia.com/schema.json', 'title': 'Embedding Dataset - SFT Training Type - Newline-Delimited JSON File', 'description': '\\n                Newline-delimited JSON (application/jsonlines) file containing Embedding Dataset - SFT Training Type objects.\\n                This is represented as an array here, however data should be newline separated instead of a list.', '$defs': {}, 'type': 'array', 'items': {'description': 'Schema for Direct Preference Optimization (DPO) training data items.\\n\\nDefines the structure for training data used in DPO fine-tuning.', 'properties': {'query': {'description': 'The query to use as an anchor', 'title': 'Query', 'type': 'string'}, 'pos_doc': {'description': 'A document that should match positively with the anchor', 'title': 'Pos Doc', 'type': 'string'}, 'neg_doc': {'description': 'A document that should not match with the anchor', 'items': {'type': 'string'}, 'title': 'Neg Doc', 'type': 'array'}}, 'required': ['query', 'pos_doc', 'neg_doc'], 'title': 'EmbeddingDatasetItemSchema', 'type': 'object'}}], description=None, name='embedding-config@v1', namespace='rl-v1', ownership=None, pod_spec=None, project=None, prompt_template='{prompt} {completion}', target=CustomizationTarget(model_path='llama32_1b-embedding', num_parameters=1000000000, precision='bf16-mixed', id='cust-target-F5q7a99hadVokiPL8mbJp9', base_model='nvidia/llama-3.2-nv-embedqa-1b-v2', created_at=datetime.datetime(2026, 1, 5, 18, 57, 12, 557253), custom_fields=None, description=None, enabled=True, hf_endpoint=None, model_type='nemo', model_uri='ngc://nvidia/nemo/llama-3_2-1b-embedding-base:0.0.1', name='llama-3.2-nv-embedqa-1b@v2', namespace='nvidia', ownership=None, project=None, status='ready', tokenizer=None, updated_at=datetime.datetime(2026, 1, 5, 18, 57, 34, 653155), schema_version='1.0'), training_precision='bf16-mixed', updated_at=datetime.datetime(2026, 1, 5, 19, 21, 44, 633553))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BASE_MODEL = \"nvidia/llama-3.2-nv-embedqa-1b@v2\"\n",
        "NUM_GPUS = 1\n",
        "MICRO_BATCH_SIZE = 8\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "\n",
        "print(\"Creating config...\")\n",
        "nemo.customization.configs.create(\n",
        "    name=\"embedding-config@v1\", \n",
        "    namespace=NAMESPACE, \n",
        "    target=BASE_MODEL,\n",
        "    training_options=[{\n",
        "        \"training_type\": \"sft\",\n",
        "        \"finetuning_type\": \"all_weights\",\n",
        "        \"num_gpus\": NUM_GPUS,\n",
        "        \"micro_batch_size\": MICRO_BATCH_SIZE\n",
        "    }], \n",
        "    max_seq_length=MAX_SEQ_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job ID: cust-SzxQKSqacaN1WJaPVYkiMp\n"
          ]
        }
      ],
      "source": [
        "# Start training job (~25 min)\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 5e-6\n",
        "\n",
        "print(\"Starting training...\")\n",
        "training_job = nemo.customization.jobs.create(\n",
        "    name=\"embedding-training\", \n",
        "    config=f\"{NAMESPACE}/embedding-config@v1\", \n",
        "    dataset={\"namespace\": NAMESPACE, \"name\": \"data\"},\n",
        "    hyperparameters={\n",
        "        \"finetuning_type\": \"all_weights\",\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"learning_rate\": LEARNING_RATE\n",
        "    }, \n",
        "    output_model=f\"{NAMESPACE}/embedding-model\")\n",
        "\n",
        "print(f\"Job ID: {training_job.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pending... | 0m 0s"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running... | 14m 57s\n",
            "  3% | Step 9 | Loss: 0.2017 | 15m 7s\n",
            "  7% | Step 19 | Loss: 0.3870 | 15m 37s\n",
            " 11% | Step 29 | Loss: 0.3030 | 15m 58s\n",
            " 15% | Step 39 | Loss: 0.1749 | 16m 28s\n",
            " 19% | Step 49 | Loss: 0.2452 | 16m 58s\n",
            " 23% | Step 59 | Loss: 0.1882 | 17m 28s\n",
            " 27% | Step 69 | Loss: 0.1803 | 17m 49s\n",
            " 31% | Step 79 | Loss: 0.1540 | 18m 19s\n",
            " 35% | Step 89 | Loss: 0.1979 | 18m 49s\n",
            " 38% | Step 99 | Loss: 0.1483 | 19m 9s\n",
            " 42% | Step 109 | Loss: 0.1557 | 19m 39s\n",
            " 46% | Step 119 | Loss: 0.1624 | 20m 10s\n",
            " 50% | Step 129 | Loss: 0.1671 | 20m 40s\n",
            " 54% | Step 139 | Loss: 0.1435 | 21m 11s\n",
            " 58% | Step 149 | Loss: 0.1476 | 21m 31s\n",
            " 62% | Step 159 | Loss: 0.1620 | 22m 1s\n",
            " 66% | Step 169 | Loss: 0.1482 | 22m 32s\n",
            " 70% | Step 179 | Loss: 0.1594 | 23m 2s\n",
            " 74% | Step 189 | Loss: 0.1514 | 23m 22s\n",
            " 78% | Step 199 | Loss: 0.1325 | 23m 52s\n",
            " 82% | Step 209 | Loss: 0.1197 | 24m 23s\n",
            " 86% | Step 219 | Loss: 0.1503 | 24m 43s\n",
            " 90% | Step 229 | Loss: 0.1088 | 25m 13s\n",
            " 94% | Step 239 | Loss: 0.1350 | 25m 43s\n",
            " 98% | Step 249 | Loss: 0.1775 | 26m 13s\n",
            "Saving model... | 31m 6ss\n",
            "\n",
            "Training complete | 31m\n"
          ]
        }
      ],
      "source": [
        "POLL_INTERVAL = 10\n",
        "start = time()\n",
        "last_step = -1\n",
        "\n",
        "while True:\n",
        "    status = nemo.customization.jobs.retrieve(training_job.id)\n",
        "    if status.status not in [\"pending\", \"created\", \"running\"]:\n",
        "        break\n",
        "    \n",
        "    d = status.status_details\n",
        "    elapsed = int(time() - start)\n",
        "    elapsed_str = f\"{elapsed//60}m {elapsed%60}s\"\n",
        "    \n",
        "    if d.epochs_completed >= 1:\n",
        "        print(f\"\\rSaving model... | {elapsed_str}\", end=\"\")\n",
        "    elif d.metrics and d.metrics.metrics.train_loss:\n",
        "        step = d.metrics.metrics.train_loss[-1].step\n",
        "        if step != last_step:\n",
        "            if last_step == -1: print()\n",
        "            loss = d.metrics.metrics.train_loss[-1].value\n",
        "            pct = int(step / d.steps_per_epoch * 100) if d.steps_per_epoch else 0\n",
        "            print(f\"{pct:3d}% | Step {step} | Loss: {loss:.4f} | {elapsed_str}\")\n",
        "            last_step = step\n",
        "    else:\n",
        "        print(f\"\\r{status.status.capitalize()}... | {elapsed_str}\", end=\"\")\n",
        "    sleep(POLL_INTERVAL)\n",
        "\n",
        "print(f\"\\n\\nTraining complete | {int(time() - start)//60}m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Deploy Model\n",
        "\n",
        "**NeMo Deployment** serves your fine-tuned model as a NIM (NVIDIA Inference Microservice). Once deployed, you can query it via the standard OpenAI-compatible embeddings API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deploying model...\n",
            "Created, waiting...\n"
          ]
        }
      ],
      "source": [
        "# Deploy as NIM (~5 min)\n",
        "DEPLOYMENT_NAME = f\"{NAMESPACE}-embedding\"\n",
        "NIM_IMAGE = \"nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2\"  # NIM container from NGC (https://www.nvidia.com/en-us/gpu-cloud/)\n",
        "NIM_IMAGE_TAG = \"1.6.0\"  # Update if using newer NIM release\n",
        "DEPLOYMENT_GPUS = 1\n",
        "\n",
        "print(\"Deploying model...\")\n",
        "try:\n",
        "    existing = nemo.deployment.model_deployments.retrieve(deployment_name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
        "    print(f\"Deployment exists (status: {existing.status_details.status})\")\n",
        "except:\n",
        "    nemo.deployment.model_deployments.create(\n",
        "        name=DEPLOYMENT_NAME,\n",
        "        namespace=NAMESPACE,\n",
        "        config={\n",
        "            \"model\": f\"{NAMESPACE}/embedding-model@{training_job.id}\",\n",
        "            \"nim_deployment\": {\n",
        "                \"image_name\": NIM_IMAGE,\n",
        "                \"image_tag\": NIM_IMAGE_TAG,\n",
        "                \"gpu\": DEPLOYMENT_GPUS,\n",
        "                \"disable_lora_support\": True  # Optimizes NIM when not using LoRA adapters\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    print(\"Created, waiting...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending | 0m 0s"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending | 1m 40s\n",
            "Deployed | 1m\n"
          ]
        }
      ],
      "source": [
        "POLL_INTERVAL = 10\n",
        "\n",
        "start = time()\n",
        "while True:\n",
        "    deployment = nemo.deployment.model_deployments.retrieve(deployment_name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
        "    if deployment.status_details.status == 'ready':\n",
        "        break\n",
        "    elapsed = int(time() - start)\n",
        "    print(f\"\\rStatus: {deployment.status_details.status} | {elapsed//60}m {elapsed%60}s\", end=\"\")\n",
        "    sleep(POLL_INTERVAL)\n",
        "\n",
        "print(f\"\\nDeployed | {int(time() - start)//60}m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Health Check\n",
        "\n",
        "Verify the deployed model responds to requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference OK | Embedding dim: 2048\n"
          ]
        }
      ],
      "source": [
        "# Wait 30 seconds to ensure model fully initialized\n",
        "INIT_WAIT = 30\n",
        "sleep(INIT_WAIT)\n",
        "\n",
        "client = OpenAI(base_url=f\"{NIM_URL}/v1\", api_key=\"None\")\n",
        "response = client.embeddings.create(\n",
        "    input=[\"Deep learning for computer vision\"], \n",
        "    model=f\"{NAMESPACE}/embedding-model\", \n",
        "    extra_body={\"input_type\": \"query\"})\n",
        "\n",
        "print(f\"Inference OK | Embedding dim: {len(response.data[0].embedding)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: See the Improvement\n",
        "\n",
        "Now let's run the same query against your fine-tuned model and compare to the baseline we saw earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: \"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data\"\n",
            "\n",
            "Rank   Base Model                     Fine-tuned Model              \n",
            "------------------------------------------------------------------\n",
            "#1     CRF Tutorial [0.387] *         CRF Tutorial [0.627] *        \n",
            "#2     BiLSTM-CRF [0.334] *           BiLSTM-CRF [0.598] *          \n",
            "#3     Random Forest [0.204]          NER-CRF [0.556] *             \n",
            "#4     SVM [0.189]                    SVM [0.548]                   \n",
            "#5     NER-CRF [0.177] *              Random Forest [0.543]         \n",
            "\n",
            "* = relevant paper\n",
            "\n",
            "The fine-tuned model pushes 'Random Forest' down and ranks CRF papers higher.\n"
          ]
        }
      ],
      "source": [
        "# Compare: same query, base model vs fine-tuned\n",
        "query_emb = client.embeddings.create(input=[DEMO_QUERY], model=f\"{NAMESPACE}/embedding-model\", extra_body={\"input_type\": \"query\"}).data[0].embedding\n",
        "doc_embs = [client.embeddings.create(input=[d], model=f\"{NAMESPACE}/embedding-model\", extra_body={\"input_type\": \"passage\"}).data[0].embedding for d in DEMO_DOCS]\n",
        "scores = [(i, cosine_similarity(query_emb, doc_embs[i])) for i in range(len(DEMO_DOCS))]\n",
        "FINETUNED_RANKING = sorted(scores, key=lambda x: -x[1])\n",
        "\n",
        "print(f\"Query: \\\"{DEMO_QUERY}\\\"\\n\")\n",
        "print(f\"{'Rank':<6} {'Base Model':<30} {'Fine-tuned Model':<30}\")\n",
        "print(\"-\" * 66)\n",
        "\n",
        "for rank in range(len(DEMO_DOCS)):\n",
        "    b_idx, b_score = BASELINE_RANKING[rank]\n",
        "    f_idx, f_score = FINETUNED_RANKING[rank]\n",
        "    \n",
        "    b_label = f\"{DEMO_LABELS[b_idx]} [{b_score:.3f}]\" + (\" *\" if b_idx in DEMO_RELEVANT else \"\")\n",
        "    f_label = f\"{DEMO_LABELS[f_idx]} [{f_score:.3f}]\" + (\" *\" if f_idx in DEMO_RELEVANT else \"\")\n",
        "    \n",
        "    print(f\"#{rank+1:<5} {b_label:<30} {f_label:<30}\")\n",
        "\n",
        "print(\"\\n* = relevant paper\")\n",
        "print(\"\\nThe fine-tuned model pushes 'Random Forest' down and ranks CRF papers higher.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluate Performance\n",
        "\n",
        "**NeMo Evaluator** runs standardized benchmarks against your deployed model. Here we use SciDocs, a retrieval benchmark for scientific papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating: rl-v1/embedding-model\n",
            "Job ID: eval-HfL5znBqVCuLsvg1Q599Hz\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation on SciDocs (~10 min)\n",
        "EVALUATE_BASELINE = False  # Set to True to evaluate base model instead of fine-tuned\n",
        "TOP_K = 10  # Retrieve top 10, Recall@5 checks first 5\n",
        "\n",
        "model_to_eval = BASE_MODEL if EVALUATE_BASELINE else f\"{NAMESPACE}/embedding-model\"\n",
        "print(f\"Evaluating: {model_to_eval}\")\n",
        "\n",
        "# Config: what benchmark to run and what metrics to compute\n",
        "eval_config = {\n",
        "    \"type\": \"retriever\",  # Evaluating retrieval (vs generation, classification, etc.)\n",
        "    \"namespace\": NAMESPACE,\n",
        "    \"tasks\": {\n",
        "        \"scidocs\": {\n",
        "            \"type\": \"beir\",  # BEIR: standard benchmark format for retrieval\n",
        "            \"dataset\": {\"files_url\": \"file://scidocs/\"},  # Pre-loaded on cluster\n",
        "            \"metrics\": {\"recall_5\": {\"type\": \"recall_5\"}}}}}\n",
        "\n",
        "# Target: which model to evaluate and how to call it\n",
        "eval_target = {\n",
        "    \"type\": \"retriever\",\n",
        "    \"retriever\": {\n",
        "        \"pipeline\": {\n",
        "            # Same model encodes both queries and documents\n",
        "            \"query_embedding_model\": {\n",
        "                \"api_endpoint\": {\"url\": f\"{NIM_URL}/v1/embeddings\", \"model_id\": model_to_eval}},\n",
        "            \"index_embedding_model\": {\n",
        "                \"api_endpoint\": {\"url\": f\"{NIM_URL}/v1/embeddings\", \"model_id\": model_to_eval}},\n",
        "            \"top_k\": TOP_K}}}\n",
        "\n",
        "eval_job = nemo.evaluation.jobs.create(config=eval_config, target=eval_target)\n",
        "print(f\"Job ID: {eval_job.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: running | 0m 0s"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: running | 15m 42s\n",
            "Complete | 15m\n"
          ]
        }
      ],
      "source": [
        "POLL_INTERVAL = 10\n",
        "\n",
        "start = time()\n",
        "while True:\n",
        "    status = nemo.evaluation.jobs.retrieve(eval_job.id)\n",
        "    if status.status not in [\"pending\", \"created\", \"running\"]:\n",
        "        break\n",
        "    elapsed = int(time() - start)\n",
        "    print(f\"\\rStatus: {status.status} | {elapsed//60}m {elapsed%60}s\", end=\"\")\n",
        "    sleep(POLL_INTERVAL)\n",
        "\n",
        "print(f\"\\nComplete | {int(time() - start)//60}m\")\n",
        "results = nemo.evaluation.jobs.results(eval_job.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Results\n",
        "\n",
        "Compare your fine-tuned model against the pretrained baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESULTS: SciDocs Retrieval Benchmark\n",
            "============================================================\n",
            "Metric: Recall@5 (relevant docs found in top 5 results)\n",
            "\n",
            "Baseline (pretrained):  0.159\n",
            "Fine-tuned model:       0.171\n",
            "Improvement:           +7.5%\n",
            "============================================================\n",
            "\n",
            "Endpoint: http://nim.test/v1/embeddings\n",
            "Model: rl-v1/embedding-model\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "BASELINE_RECALL = 0.159  # Pretrained model on SciDocs\n",
        "finetuned_recall = results.tasks['scidocs'].metrics['retriever.recall_5'].scores['recall_5'].value\n",
        "improvement = ((finetuned_recall / BASELINE_RECALL) - 1) * 100\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RESULTS: SciDocs Retrieval Benchmark\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Metric: Recall@5 (relevant docs found in top 5 results)\")\n",
        "print()\n",
        "print(f\"Baseline (pretrained):  {BASELINE_RECALL:.3f}\")\n",
        "print(f\"Fine-tuned model:       {finetuned_recall:.3f}\")\n",
        "print(f\"Improvement:           +{improvement:.1f}%\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nEndpoint: {NIM_URL}/v1/embeddings\")\n",
        "print(f\"Model: {NAMESPACE}/embedding-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You fine-tuned NVIDIA's `llama-3.2-nv-embedqa-1b-v2` embedding model on 65K scientific paper triplets from SPECTER - a dataset where papers that cite each other are marked as \"related.\"\n",
        "\n",
        "The base model matched documents by keyword overlap. After fine-tuning, it learned scientific paper neighborhoods: which papers actually cite each other, regardless of surface-level word matches. The demo showed this - \"Random Forests\" dropped in ranking because it's unrelated to \"Conditional Random Fields,\" despite sharing the word \"random.\"\n",
        "\n",
        "SciDocs tests retrieval across thousands of scientific queries. Recall@5 asks: \"Of all relevant papers, how many appear in the top 5 results?\" Your model improved from 0.159 to ~0.17, meaning 6-10% more relevant papers now surface in the top 5.\n",
        "\n",
        "In a RAG pipeline, better retrieval means better context for the LLM and more accurate answers. Your model is deployed and ready to use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "**Scale Up:**\n",
        "- Train on full SPECTER dataset for additional improvement\n",
        "- Increase to 3 epochs for better convergence\n",
        "\n",
        "**Apply to Your Domain:**\n",
        "- [Format your data as query-positive-negative triplets](https://docs.nvidia.com/nemo/microservices/latest/fine-tune/models/embedding.html#data-preparation)\n",
        "- Replace SPECTER dataset with your domain data (legal, medical, product catalogs, etc.)\n",
        "- Evaluate on your own retrieval tasks\n",
        "\n",
        "**Learn More:**\n",
        "- [NeMo Microservices Documentation](https://docs.nvidia.com/nemo/microservices/latest/)\n",
        "- [Embedding Model Guide](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2)\n",
        "- [Other NeMo Tutorials](../../../README.md)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Uncomment cleanup cells as needed to delete resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete training jobs (PERMANENT)\n",
        "# print(\"Deleting training jobs...\")\n",
        "# for j in nemo.customization.jobs.list(filter={\"namespace\": NAMESPACE}).data:\n",
        "#     nemo.customization.jobs.delete(job_id=j.id)\n",
        "# print(\"Training jobs deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete deployment (frees GPU, keeps model for later redeployment)\n",
        "# print(\"Deleting deployment...\")\n",
        "# nemo.deployment.model_deployments.delete(deployment_name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
        "# print(\"Deployment deleted - GPU freed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete model (PERMANENT - must retrain to recover)\n",
        "# print(\"Deleting model...\")\n",
        "# for m in nemo.models.list(filter={\"namespace\": NAMESPACE}).data:\n",
        "#     nemo.models.delete(namespace=NAMESPACE, model_name=m.name.split('/')[-1])\n",
        "# print(\"Model deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete dataset (PERMANENT)\n",
        "# print(\"Deleting dataset...\")\n",
        "# nemo.datasets.delete(namespace=NAMESPACE, dataset_name=\"data\")\n",
        "# hf.delete_repo(f\"{NAMESPACE}/data\", repo_type='dataset')\n",
        "# print(\"Dataset deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete configs (PERMANENT)\n",
        "# print(\"Deleting configs...\")\n",
        "# nemo.customization.configs.delete(config_name=\"embedding-config@v1\", namespace=NAMESPACE)\n",
        "# for cfg in nemo.evaluation.configs.list(filter={\"namespace\": NAMESPACE}).data:\n",
        "#     nemo.evaluation.configs.delete(config_id=cfg.id)\n",
        "# print(\"Configs deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete eval jobs (PERMANENT)\n",
        "# print(\"Deleting eval jobs...\")\n",
        "# for j in nemo.evaluation.jobs.list(filter={\"namespace\": NAMESPACE}).data:\n",
        "#     nemo.evaluation.jobs.delete(job_id=j.id)\n",
        "# print(\"Eval jobs deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete namespace (PERMANENT - deletes everything in namespace)\n",
        "# print(\"Deleting namespace...\")\n",
        "# nemo.namespaces.delete(NAMESPACE)\n",
        "# requests.delete(f\"{NDS_URL}/v1/datastore/namespaces/{NAMESPACE}\")\n",
        "# print(f\"Namespace '{NAMESPACE}' deleted\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nemo_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

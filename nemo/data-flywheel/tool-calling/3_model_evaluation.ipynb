{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3051ba77-9d36-477c-9969-b07c4b776b6f",
   "metadata": {},
   "source": [
    "# Part III: Model Evaluation Using NeMo Evaluator\n",
    "\n",
    "This notebook covers the following:\n",
    "\n",
    "0. [Pre-requisites: Configurations and Health Checks](#step-0)\n",
    "1. [Establish a baseline accuracy benchmark](#step-1). This uses the off-the-shelf llama-3.2-1b-instruct model\n",
    "2. [Evaluate the LoRA customized model](#step-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5adb3981-0171-4080-96e8-245954b63dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from time import sleep, time\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4848b-8bdf-4ebf-9f82-2c2cd61cb097",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-0\"></a>\n",
    "## Prerequisites: Configurations and Health Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac797d-ea7b-497e-a6c4-357a2ca3367d",
   "metadata": {},
   "source": [
    "Before you proceed, make sure that you completed the previous notebooks on data preparation and model fine-tuning to obtain the assets required to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23824d49-22a8-4480-ba49-9745a0069f86",
   "metadata": {},
   "source": [
    "### Configure NeMo Microservices Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd69e09",
   "metadata": {},
   "source": [
    "The following code imports necessary configurations and prints the endpoints for the NeMo Data Store, Entity Store, Customizer, Evaluator, and NIM, as well as the namespace and base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5f5407-5495-42b7-b472-e1d859e63421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store endpoint: http://data-store.test\n",
      "Entity Store, Customizer, Evaluator endpoint: http://nemo.test\n",
      "NIM endpoint: http://nim.test\n",
      "Namespace: xlam-tutorial-ns\n",
      "Base Model: meta/llama-3.2-1b-instruct\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "\n",
    "print(f\"Data Store endpoint: {NDS_URL}\")\n",
    "print(f\"Entity Store, Customizer, Evaluator endpoint: {NEMO_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c6449-453b-4372-81fe-7baa9aea1b1a",
   "metadata": {},
   "source": [
    "### Check Available Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fe718",
   "metadata": {},
   "source": [
    "Specify the customized model name that you got from the previous notebook to the following variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07faee8b-cc39-4485-abf4-8ab884883d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMIZED_MODEL = \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c0ea9",
   "metadata": {},
   "source": [
    "The following code checks if the NIM endpoint hosts the model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b6072e-66a1-43f6-a19e-d506ad95281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(f\"{NIM_URL}/v1/models\")\n",
    "\n",
    "models = resp.json().get(\"data\", [])\n",
    "model_names = [model[\"id\"] for model in models]\n",
    "\n",
    "assert CUSTOMIZED_MODEL in model_names, \\\n",
    "    f\"Model {CUSTOMIZED_MODEL} not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35403c52-0448-49b4-aaf8-d80cf2a05226",
   "metadata": {},
   "source": [
    "### Verify the Availability of the Datasets\n",
    "\n",
    "In the previous notebook, we uploaded the test dataset along with the train and validation sets at `/v1/datasets/{NMS_NAMESPACE}/{DATASET_NAME}`. \n",
    "The following code performs a sanity check to validate the dataset by sending a GET request to the specified URL.\n",
    "It asserts that the status code of the response is either 200 or 201, indicating a successful fetch.\n",
    "If the assertion passes, it prints the URL of the files in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2267b33-43da-4623-832a-65ef9f6c2525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files URL: hf://datasets/xlam-tutorial-ns/xlam-ft-dataset\n"
     ]
    }
   ],
   "source": [
    "# Sanity check to validate dataset\n",
    "res = requests.get(url=f\"{NEMO_URL}/v1/datasets/{NMS_NAMESPACE}/{DATASET_NAME}\")\n",
    "assert res.status_code in (200, 201), f\"Status Code {res.status_code} Failed to fetch dataset {res.text}\"\n",
    "\n",
    "print(\"Files URL:\", res.json()[\"files_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f9282-abec-401d-9143-076a3977d9a9",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-1\"></a>\n",
    "## Step 1: Establish Baseline Accuracy Benchmark\n",
    "\n",
    "First, we’ll assess the accuracy of the 'off-the-shelf' base model—pristine, untouched, and blissfully unaware of the transformative magic that is fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147cf266-5e90-4b81-92a5-53b7981c47be",
   "metadata": {},
   "source": [
    "### 1.1: Create an Evaluation Config Object\n",
    "Create an evaluation configuration object for NeMo Evaluator. For more information on various parameters, refer to the [NeMo Evaluator configuration](https://docs.nvidia.com/nemo/microservices/latest/evaluate/evaluation-configs.html) in the NeMo microservices documentation.\n",
    "\n",
    "\n",
    "* The `tasks.custom-tool-calling.dataset.files_url` is used to indicate which test file to use. Note that it's required to upload this to the NeMo Data Store and register with Entity store before using.\n",
    "* The `tasks.dataset.limit` argument below specifies how big a subset of test data to run the evaluation on\n",
    "* The evaluation metric `tasks.metrics.tool-calling-accuracy` reports `function_name_accuracy` and `function_name_and_args_accuracy` numbers, which are as their names imply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e2e116-ed60-4e88-970b-f0d09d4a258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tool_calling_eval_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"tasks\": {\n",
    "        \"custom-tool-calling\": {\n",
    "            \"type\": \"chat-completion\",\n",
    "            \"dataset\": {\n",
    "                \"files_url\": f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}/testing/xlam-test-single.jsonl\",\n",
    "                \"limit\": 50\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"template\": {\n",
    "                    \"messages\": \"{{ item.messages | tojson}}\",\n",
    "                    \"tools\": \"{{ item.tools | tojson }}\",\n",
    "                    \"tool_choice\": \"auto\"\n",
    "                }\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"tool-calling-accuracy\": {\n",
    "                    \"type\": \"tool-calling\",\n",
    "                    \"params\": {\"tool_calls_ground_truth\": \"{{ item.tool_calls | tojson }}\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8007f17-2eb1-477f-b156-5ed1d17d894b",
   "metadata": {},
   "source": [
    "### 1.2: Launch Evaluation Job "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fbf88",
   "metadata": {},
   "source": [
    "The following code sends a POST request to the NeMo Evaluator API to launch an evaluation job. It uses the evaluation configuration defined in the previous cell and targets the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5ec90-a189-47d6-9c07-18162b95130d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = requests.post(\n",
    "    f\"{NEMO_URL}/v1/evaluation/jobs\",\n",
    "    json={\n",
    "        \"config\": simple_tool_calling_eval_config,\n",
    "        \"target\": {\"type\": \"model\", \"model\": BASE_MODEL}\n",
    "    }\n",
    ")\n",
    "\n",
    "base_eval_job_id = res.json()[\"id\"]\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5133e-076c-4560-815a-1ff71901af01",
   "metadata": {},
   "source": [
    "The following code defines a helper function to poll on job status until it finishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa0b694-a699-4eb0-903c-9bac8651f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_eval_job(job_url: str, polling_interval: int = 10, timeout: int = 6000):\n",
    "    \"\"\"Helper for waiting an eval job.\"\"\"\n",
    "    start_time = time()\n",
    "    res = requests.get(job_url)\n",
    "    status = res.json()[\"status\"]\n",
    "\n",
    "    while (status in [\"pending\", \"created\", \"running\"]):\n",
    "        # Check for timeout\n",
    "        if time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Took more than {timeout} seconds.\")\n",
    "\n",
    "        # Sleep before polling again\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        # Fetch updated status and progress\n",
    "        res = requests.get(job_url)\n",
    "        status = res.json()[\"status\"]\n",
    "\n",
    "        # Progress details (only fetch if status is \"running\")\n",
    "        if status == \"running\":\n",
    "            progress = res.json().get(\"status_details\", {}).get(\"progress\", 0)\n",
    "        elif status == \"completed\":\n",
    "            progress = 100\n",
    "\n",
    "        print(f\"Job status: {status} after {time() - start_time:.2f} seconds. Progress: {progress}%\")\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db71fc",
   "metadata": {},
   "source": [
    "Run the helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48322fdb-8920-435d-863d-9a75158843d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 5.03 seconds. Progress: 0.0%\n",
      "Job status: running after 10.05 seconds. Progress: 0.0%\n",
      "Job status: running after 15.06 seconds. Progress: 0.0%\n",
      "Job status: running after 20.08 seconds. Progress: 0.0%\n",
      "Job status: running after 25.09 seconds. Progress: 8.0%\n",
      "Job status: running after 30.11 seconds. Progress: 12.0%\n",
      "Job status: running after 35.12 seconds. Progress: 16.0%\n",
      "Job status: running after 40.13 seconds. Progress: 26.0%\n",
      "Job status: running after 45.15 seconds. Progress: 26.0%\n",
      "Job status: running after 50.16 seconds. Progress: 26.0%\n",
      "Job status: running after 55.18 seconds. Progress: 26.0%\n",
      "Job status: running after 60.20 seconds. Progress: 26.0%\n",
      "Job status: running after 65.21 seconds. Progress: 26.0%\n",
      "Job status: running after 70.23 seconds. Progress: 32.0%\n",
      "Job status: running after 75.24 seconds. Progress: 32.0%\n",
      "Job status: running after 80.26 seconds. Progress: 38.0%\n",
      "Job status: running after 85.28 seconds. Progress: 38.0%\n",
      "Job status: running after 90.29 seconds. Progress: 38.0%\n",
      "Job status: running after 95.31 seconds. Progress: 40.0%\n",
      "Job status: running after 100.32 seconds. Progress: 48.0%\n",
      "Job status: running after 105.34 seconds. Progress: 50.0%\n",
      "Job status: running after 110.35 seconds. Progress: 60.0%\n",
      "Job status: running after 115.37 seconds. Progress: 60.0%\n",
      "Job status: running after 120.38 seconds. Progress: 60.0%\n",
      "Job status: running after 125.40 seconds. Progress: 60.0%\n",
      "Job status: running after 130.42 seconds. Progress: 60.0%\n",
      "Job status: running after 135.43 seconds. Progress: 62.0%\n",
      "Job status: running after 140.45 seconds. Progress: 62.0%\n",
      "Job status: running after 145.46 seconds. Progress: 62.0%\n",
      "Job status: running after 150.48 seconds. Progress: 62.0%\n",
      "Job status: running after 155.50 seconds. Progress: 62.0%\n",
      "Job status: running after 160.51 seconds. Progress: 64.0%\n",
      "Job status: running after 165.53 seconds. Progress: 64.0%\n",
      "Job status: running after 170.55 seconds. Progress: 68.0%\n",
      "Job status: running after 175.56 seconds. Progress: 72.0%\n",
      "Job status: running after 180.58 seconds. Progress: 82.0%\n",
      "Job status: running after 185.60 seconds. Progress: 84.0%\n",
      "Job status: running after 190.61 seconds. Progress: 84.0%\n",
      "Job status: running after 195.63 seconds. Progress: 84.0%\n",
      "Job status: running after 200.65 seconds. Progress: 88.0%\n",
      "Job status: running after 205.67 seconds. Progress: 88.0%\n",
      "Job status: running after 210.68 seconds. Progress: 92.0%\n",
      "Job status: running after 215.70 seconds. Progress: 92.0%\n",
      "Job status: running after 220.71 seconds. Progress: 92.0%\n",
      "Job status: running after 225.73 seconds. Progress: 94.0%\n",
      "Job status: running after 230.75 seconds. Progress: 100.0%\n",
      "Job status: completed after 235.77 seconds. Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "# Poll\n",
    "res = wait_eval_job(f\"{NEMO_URL}/v1/evaluation/jobs/{base_eval_job_id}\", polling_interval=5, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de58a76-1775-4c93-8798-1ec89c8eeaff",
   "metadata": {},
   "source": [
    "### 1.3 Review Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb6220",
   "metadata": {},
   "source": [
    "The following code sends a GET request to retrieve the evaluation results for the base evaluation job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec04c15a-62a9-4271-8a7f-9a7e8ec7884d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': '2025-04-02T19:08:07.819322',\n",
       " 'updated_at': '2025-04-02T19:08:07.819325',\n",
       " 'id': 'evaluation_result-UgxzDz9JYWA6KMv5f9dWFR',\n",
       " 'job': 'eval-ddAsfX2u7htc8RjjrTV3G',\n",
       " 'tasks': {'custom-tool-calling': {'metrics': {'tool-calling-accuracy': {'scores': {'function_name_accuracy': {'value': 0.12,\n",
       "       'stats': {'count': 50, 'sum': 6.0, 'mean': 0.12}},\n",
       "      'function_name_and_args_accuracy': {'value': 0.08,\n",
       "       'stats': {'count': 50, 'sum': 4.0, 'mean': 0.08}}}}}}},\n",
       " 'groups': {},\n",
       " 'namespace': 'default',\n",
       " 'custom_fields': {}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get(f\"{NEMO_URL}/v1/evaluation/jobs/{base_eval_job_id}/results\")\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b28813",
   "metadata": {},
   "source": [
    "The following code extracts and prints the accuracy scores for the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2129c739-8980-4d28-ab72-331e00f0c2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: function_name_accuracy: 0.12\n",
      "Base model: function_name_and_args_accuracy: 0.08\n"
     ]
    }
   ],
   "source": [
    "# Extract function name accuracy score\n",
    "base_function_name_accuracy_score = res.json()[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_accuracy\"][\"value\"]\n",
    "base_function_name_and_args_accuracy = res.json()[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_and_args_accuracy\"][\"value\"]\n",
    "\n",
    "print(f\"Base model: function_name_accuracy: {base_function_name_accuracy_score}\")\n",
    "print(f\"Base model: function_name_and_args_accuracy: {base_function_name_and_args_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fcb78-0e23-45f6-988a-8be6e33c2cfc",
   "metadata": {},
   "source": [
    "Without any finetuning, the `meta/llama-3.2-1b-instruct` model should score in the ballpark of about 12% in `function_name_accuracy`, and 8% in `function_name_and_args_accuracy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c7ab4-bb3c-46c3-a51e-9e25509a7781",
   "metadata": {},
   "source": [
    "### (Optional) 1.4 Download and Inspect Results\n",
    "\n",
    "To take a deeper look into the model's generated outputs, you can download and review the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36bf02e0-2383-46d5-ad47-3edc1e1dfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_evaluation_results(eval_url, eval_job_id, output_file):\n",
    "    \"\"\"Downloads evaluation results for a given job ID from the NeMo server.\"\"\"\n",
    "    \n",
    "    download_response = requests.get(f\"{eval_url}/v1/evaluation/jobs/{eval_job_id}/download-results\")\n",
    "    \n",
    "    # Check the response status\n",
    "    if download_response.status_code == 200:\n",
    "        # Save the results to a file\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            file.write(download_response.content)\n",
    "        print(f\"Evaluation results for job {eval_job_id} downloaded successfully to {output_file}.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download evaluation results. Status code: {download_response.status_code}\")\n",
    "        print('Response:', download_response.text)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da8ec595-8edd-4d8c-a2a0-29f830710c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for job eval-ddAsfX2u7htc8RjjrTV3G downloaded successfully to eval-ddAsfX2u7htc8RjjrTV3G.json.\n"
     ]
    }
   ],
   "source": [
    "output_file = f\"{base_eval_job_id}.json\"\n",
    "\n",
    "# Assertion fails if download fails\n",
    "assert download_evaluation_results(eval_url=NEMO_URL, eval_job_id=base_eval_job_id, output_file=output_file) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f633d-5b20-4a28-8962-970a54b91f40",
   "metadata": {},
   "source": [
    "You can inspect the downloaded results file to observe places where the base model errors. Without any fine-tuning, some models not only return inaccurate function names and arguments, but they may not adhere to a consistent structured / predictable output schema. This makes it difficult to automatically parse these outputs, deterring integration with external systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79229ce6-4ce4-4f82-b0c9-15be351a2291",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-2\"></a>\n",
    "## Step 2: Evaluate the LoRA Customized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdeba8-6800-419f-b5c5-0dc34bf6e0f5",
   "metadata": {},
   "source": [
    "### 2.1 Launch Evaluation Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb900e7-2ec8-417a-9266-2ce682f655e5",
   "metadata": {},
   "source": [
    "Run another evaluation job with the same evaluation config but with the customized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5438608-e708-4421-bf45-da39fbe506ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': '2025-04-02T19:12:03.849375',\n",
       " 'updated_at': '2025-04-02T19:12:03.849376',\n",
       " 'id': 'eval-RMbUCrxKuzuE5cJdhwh3Uo',\n",
       " 'namespace': 'default',\n",
       " 'description': None,\n",
       " 'target': {'schema_version': '1.0',\n",
       "  'id': 'eval-target-DjryeDuurpvztuwb3MKpVT',\n",
       "  'description': None,\n",
       "  'type_prefix': 'eval-target',\n",
       "  'namespace': 'default',\n",
       "  'project': None,\n",
       "  'created_at': '2025-04-02T19:12:03.848747',\n",
       "  'updated_at': '2025-04-02T19:12:03.848748',\n",
       "  'custom_fields': {},\n",
       "  'ownership': None,\n",
       "  'name': 'eval-target-DjryeDuurpvztuwb3MKpVT',\n",
       "  'type': 'model',\n",
       "  'cached_outputs': None,\n",
       "  'model': 'xlam-tutorial-ns/llama-3.2-1b-xlam-run1@cust-4rZxaBqeqGtVUkZ3MdoMXT',\n",
       "  'retriever': None,\n",
       "  'rag': None},\n",
       " 'config': {'schema_version': '1.0',\n",
       "  'id': 'eval-config-Cp7srSQAmkGQ3QZcqwL4Jo',\n",
       "  'description': None,\n",
       "  'type_prefix': 'eval-config',\n",
       "  'namespace': 'default',\n",
       "  'project': None,\n",
       "  'created_at': '2025-04-02T19:12:03.848538',\n",
       "  'updated_at': '2025-04-02T19:12:03.848542',\n",
       "  'custom_fields': {},\n",
       "  'ownership': None,\n",
       "  'name': 'eval-config-Cp7srSQAmkGQ3QZcqwL4Jo',\n",
       "  'type': 'custom',\n",
       "  'params': None,\n",
       "  'tasks': {'custom-tool-calling': {'type': 'chat-completion',\n",
       "    'params': {'template': {'messages': '{{ item.messages | tojson}}',\n",
       "      'tools': '{{ item.tools | tojson }}',\n",
       "      'tool_choice': 'auto'}},\n",
       "    'metrics': {'tool-calling-accuracy': {'type': 'tool-calling',\n",
       "      'params': {'tool_calls_ground_truth': '{{ item.tool_calls | tojson }}'}}},\n",
       "    'dataset': {'schema_version': '1.0',\n",
       "     'id': 'dataset-TrhsB4bbA5S5ZgG1Kitq2e',\n",
       "     'description': None,\n",
       "     'type_prefix': None,\n",
       "     'namespace': 'default',\n",
       "     'project': None,\n",
       "     'created_at': '2025-04-02T19:12:03.848649',\n",
       "     'updated_at': '2025-04-02T19:12:03.848650',\n",
       "     'custom_fields': {},\n",
       "     'ownership': None,\n",
       "     'name': 'dataset-TrhsB4bbA5S5ZgG1Kitq2e',\n",
       "     'version_id': 'main',\n",
       "     'version_tags': [],\n",
       "     'format': None,\n",
       "     'files_url': 'hf://datasets/xlam-tutorial-ns/xlam-ft-dataset/testing/xlam-test-single.jsonl',\n",
       "     'hf_endpoint': None,\n",
       "     'split': None,\n",
       "     'limit': 50}}},\n",
       "  'groups': None},\n",
       " 'result': None,\n",
       " 'output_files_url': None,\n",
       " 'status_details': {'message': None, 'task_status': {}, 'progress': None},\n",
       " 'status': 'created',\n",
       " 'project': None,\n",
       " 'custom_fields': {},\n",
       " 'ownership': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.post(\n",
    "    f\"{NEMO_URL}/v1/evaluation/jobs\",\n",
    "    json={\n",
    "        \"config\": simple_tool_calling_eval_config,\n",
    "        \"target\": {\"type\": \"model\", \"model\": CUSTOMIZED_MODEL},\n",
    "    },\n",
    ")\n",
    "\n",
    "ft_eval_job_id = res.json()[\"id\"]\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7409f25-23c1-4574-9392-051f3da0498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 5.03 seconds. Progress: 0.0%\n",
      "Job status: running after 10.04 seconds. Progress: 0.0%\n",
      "Job status: running after 15.06 seconds. Progress: 0.0%\n",
      "Job status: running after 20.08 seconds. Progress: 0.0%\n",
      "Job status: running after 25.09 seconds. Progress: 8.0%\n",
      "Job status: running after 30.11 seconds. Progress: 12.0%\n",
      "Job status: running after 35.13 seconds. Progress: 18.0%\n",
      "Job status: running after 40.14 seconds. Progress: 26.0%\n",
      "Job status: running after 45.16 seconds. Progress: 26.0%\n",
      "Job status: running after 50.18 seconds. Progress: 26.0%\n",
      "Job status: running after 55.19 seconds. Progress: 26.0%\n",
      "Job status: running after 60.21 seconds. Progress: 26.0%\n",
      "Job status: running after 65.22 seconds. Progress: 28.0%\n",
      "Job status: running after 70.24 seconds. Progress: 32.0%\n",
      "Job status: running after 75.26 seconds. Progress: 32.0%\n",
      "Job status: running after 80.27 seconds. Progress: 38.0%\n",
      "Job status: running after 85.29 seconds. Progress: 38.0%\n",
      "Job status: running after 90.30 seconds. Progress: 38.0%\n",
      "Job status: running after 95.32 seconds. Progress: 40.0%\n",
      "Job status: running after 100.34 seconds. Progress: 50.0%\n",
      "Job status: running after 105.35 seconds. Progress: 50.0%\n",
      "Job status: running after 110.37 seconds. Progress: 60.0%\n",
      "Job status: running after 115.38 seconds. Progress: 60.0%\n",
      "Job status: running after 120.40 seconds. Progress: 60.0%\n",
      "Job status: running after 125.42 seconds. Progress: 60.0%\n",
      "Job status: running after 130.43 seconds. Progress: 60.0%\n",
      "Job status: running after 135.45 seconds. Progress: 62.0%\n",
      "Job status: running after 140.46 seconds. Progress: 62.0%\n",
      "Job status: running after 145.48 seconds. Progress: 62.0%\n",
      "Job status: running after 150.49 seconds. Progress: 62.0%\n",
      "Job status: running after 155.51 seconds. Progress: 62.0%\n",
      "Job status: running after 160.52 seconds. Progress: 64.0%\n",
      "Job status: running after 165.54 seconds. Progress: 64.0%\n",
      "Job status: running after 170.56 seconds. Progress: 68.0%\n",
      "Job status: running after 175.57 seconds. Progress: 74.0%\n",
      "Job status: running after 180.59 seconds. Progress: 84.0%\n",
      "Job status: running after 185.60 seconds. Progress: 84.0%\n",
      "Job status: running after 190.61 seconds. Progress: 84.0%\n",
      "Job status: running after 195.63 seconds. Progress: 84.0%\n",
      "Job status: running after 200.65 seconds. Progress: 88.0%\n",
      "Job status: running after 205.66 seconds. Progress: 88.0%\n",
      "Job status: running after 210.68 seconds. Progress: 92.0%\n",
      "Job status: running after 215.70 seconds. Progress: 92.0%\n",
      "Job status: running after 220.71 seconds. Progress: 92.0%\n",
      "Job status: running after 225.73 seconds. Progress: 94.0%\n",
      "Job status: completed after 230.75 seconds. Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "# Poll\n",
    "res = wait_eval_job(f\"{NEMO_URL}/v1/evaluation/jobs/{ft_eval_job_id}\", polling_interval=5, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706c060-8b37-40e2-b0e6-636d130260e7",
   "metadata": {},
   "source": [
    "### 2.2 Review Evaluation Metrics\n",
    "The following code sends a GET request to retrieve the evaluation results for the fine-tuned model evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d2ffdb8-8e8e-403e-a836-8bcd93778814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': '2025-04-02T19:12:03.887985',\n",
       " 'updated_at': '2025-04-02T19:12:03.887986',\n",
       " 'id': 'evaluation_result-RmJs94jfgu1J5ePZbm23qF',\n",
       " 'job': 'eval-RMbUCrxKuzuE5cJdhwh3Uo',\n",
       " 'tasks': {'custom-tool-calling': {'metrics': {'tool-calling-accuracy': {'scores': {'function_name_accuracy': {'value': 0.92,\n",
       "       'stats': {'count': 50, 'sum': 46.0, 'mean': 0.92}},\n",
       "      'function_name_and_args_accuracy': {'value': 0.72,\n",
       "       'stats': {'count': 50, 'sum': 36.0, 'mean': 0.72}}}}}}},\n",
       " 'groups': {},\n",
       " 'namespace': 'default',\n",
       " 'custom_fields': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get(f\"{NEMO_URL}/v1/evaluation/jobs/{ft_eval_job_id}/results\")\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adb03140-21ad-445b-b173-7aebdfb0b703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model: function_name_accuracy: 0.92\n",
      "Custom model: function_name_and_args_accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Extract function name accuracy score\n",
    "ft_function_name_accuracy_score = res.json()[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_accuracy\"][\"value\"]\n",
    "ft_function_name_and_args_accuracy = res.json()[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_and_args_accuracy\"][\"value\"]\n",
    "\n",
    "print(f\"Custom model: function_name_accuracy: {ft_function_name_accuracy_score}\")\n",
    "print(f\"Custom model: function_name_and_args_accuracy: {ft_function_name_and_args_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568a204-ad01-4a04-8cfa-602816b8937c",
   "metadata": {},
   "source": [
    "A successfully fine-tuned `meta/llama-3.2-1b-instruct` results in a significant increase in tool calling accuracy with \n",
    "\n",
    "In this case you should observe roughly the following improvements -\n",
    "* function_name_accuracy: 12% to 92%\n",
    "* function_name_and_args_accuracy: 8% to 72%\n",
    "\n",
    "Since this evaluation was on a limited number of samples for demonstration purposes, you may choose to increase `tasks.dataset.limit` in your evaluation config `simple_tool_calling_eval_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc53b6-677b-4b44-bf6e-bcdadef21a73",
   "metadata": {},
   "source": [
    "## (Optional) Next Steps\n",
    "\n",
    "\n",
    "\n",
    "* You may also run the same evaluation on a base `meta/llama-3.1-70B` model for comparison.\n",
    "For this, first you will need to deploy the corresponding NIM using instructions [here](https://build.nvidia.com/meta/llama-3_1-70b-instruct/deploy). After your NIM is deployed, set that endpoint as your evaluation target like so -\n",
    "\n",
    "``` python\n",
    "# Create an evaluation target\n",
    "NIM_URL = \"http://0.0.0.0:8000\"\n",
    "EVAL_TARGET = {\n",
    "    \"type\": \"model\", \n",
    "    \"model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": f\"{NIM_URL}/v1/completions\",\n",
    "         \"model_id\": \"meta/llama-3.1-70b-instruct\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Start eval job\n",
    "res = requests.post(\n",
    "    f\"{NEMO_URL}/v1/evaluation/jobs\",\n",
    "    json={\n",
    "        \"config\": simple_tool_calling_eval_config,\n",
    "        \"target\": EVAL_TARGET\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "Running evaluation using the default config in this notebook, you should observe `meta/llama-3.1-70B` performance similar to -\n",
    "* function_name_accuracy: 98%\n",
    "* function_name_and_args_accuracy: 66%\n",
    "\n",
    "Remarkably, a LoRA-tuned `meta/llama-3.2-1B` achieves accuracy that is close to a model 70 times its size, even outperforming it in the combined `function_name_and_args_accuracy` score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406f95b-4fc5-462a-ad6c-43196b70896d",
   "metadata": {},
   "source": [
    "You can now proceed with the same processes to fine-tune other NIM for LLMs and evaluate the accuracies between the base model and the fine-tuned model. By doing so, you can produce more accurate models for your use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3051ba77-9d36-477c-9969-b07c4b776b6f",
   "metadata": {},
   "source": [
    "# Part III: Model Evaluation Using NeMo Evaluator\n",
    "\n",
    "This notebook covers the following:\n",
    "\n",
    "0. [Pre-requisites: Configurations and Health Checks](#step-0)\n",
    "1. [Establish a baseline accuracy benchmark](#step-1). This uses the off-the-shelf llama-3.2-1b-instruct model\n",
    "2. [Evaluate the LoRA customized model](#step-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5adb3981-0171-4080-96e8-245954b63dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep, time\n",
    "from nemo_microservices import NeMoMicroservices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4848b-8bdf-4ebf-9f82-2c2cd61cb097",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-0\"></a>\n",
    "## Prerequisites: Configurations and Health Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac797d-ea7b-497e-a6c4-357a2ca3367d",
   "metadata": {},
   "source": [
    "Before you proceed, make sure that you completed the previous notebooks on data preparation and model fine-tuning to obtain the assets required to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23824d49-22a8-4480-ba49-9745a0069f86",
   "metadata": {},
   "source": [
    "### Configure NeMo Microservices Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd69e09",
   "metadata": {},
   "source": [
    "The following code imports necessary configurations and prints the endpoints for the NeMo Data Store, Entity Store, Customizer, Evaluator, and NIM, as well as the namespace and base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf007c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "# Initialize NeMo Microservices SDK client\n",
    "nemo_client = NeMoMicroservices(\n",
    "    base_url=NEMO_URL,\n",
    "    inference_base_url=NIM_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a5f5407-5495-42b7-b472-e1d859e63421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store endpoint: http://data-store.test\n",
      "Entity Store, Customizer, Evaluator endpoint: http://nemo.test\n",
      "NIM endpoint: http://nim.test\n",
      "Namespace: xlam-tutorial-ns\n",
      "Base Model: meta/llama-3.2-1b-instruct\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Store endpoint: {NDS_URL}\")\n",
    "print(f\"Entity Store, Customizer, Evaluator endpoint: {NEMO_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c6449-453b-4372-81fe-7baa9aea1b1a",
   "metadata": {},
   "source": [
    "### Check Available Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fe718",
   "metadata": {},
   "source": [
    "Specify the customized model name that you got from the previous notebook to the following variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07faee8b-cc39-4485-abf4-8ab884883d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMIZED_MODEL = CUSTOM_MODEL # paste from the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c0ea9",
   "metadata": {},
   "source": [
    "The following code checks if the NIM endpoint hosts the model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b6072e-66a1-43f6-a19e-d506ad95281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the custom LoRA model is hosted by NVIDIA NIM\n",
    "models = nemo_client.inference.models.list()\n",
    "model_names = [model.id for model in models.data]\n",
    "\n",
    "assert CUSTOMIZED_MODEL in model_names, \\\n",
    "    f\"Model {CUSTOMIZED_MODEL} not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35403c52-0448-49b4-aaf8-d80cf2a05226",
   "metadata": {},
   "source": [
    "### Verify the Availability of the Datasets\n",
    "\n",
    "In the previous notebook, we uploaded the test dataset along with the train and validation sets using `nemo_client.datasets.create(name=DATASET_NAME, namespace=NMS_NAMESPACE, ...)`.\n",
    "The following code performs a sanity check to validate the dataset by retrieving it and printing the URL of the files in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2267b33-43da-4623-832a-65ef9f6c2525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files URL: hf://datasets/xlam-tutorial-ns/xlam-ft-dataset\n"
     ]
    }
   ],
   "source": [
    "# Sanity check to validate dataset\n",
    "dataset = nemo_client.datasets.retrieve(namespace=NMS_NAMESPACE, dataset_name=DATASET_NAME)\n",
    "print(\"Files URL:\", dataset.files_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f9282-abec-401d-9143-076a3977d9a9",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-1\"></a>\n",
    "## Step 1: Establish Baseline Accuracy Benchmark\n",
    "\n",
    "First, we’ll assess the accuracy of the 'off-the-shelf' base model—pristine, untouched, and blissfully unaware of the transformative magic that is fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147cf266-5e90-4b81-92a5-53b7981c47be",
   "metadata": {},
   "source": [
    "### 1.1: Create an Evaluation Config Object\n",
    "Create an evaluation configuration object for NeMo Evaluator. For more information on various parameters, refer to the [NeMo Evaluator configuration](https://docs.nvidia.com/nemo/microservices/latest/evaluate/evaluation-configs.html) in the NeMo microservices documentation.\n",
    "\n",
    "\n",
    "* The `tasks.custom-tool-calling.dataset.files_url` is used to indicate which test file to use. Note that it's required to upload this to the NeMo Data Store and register with Entity store before using.\n",
    "* The `tasks.dataset.limit` argument below specifies how big a subset of test data to run the evaluation on\n",
    "* The evaluation metric `tasks.metrics.tool-calling-accuracy` reports `function_name_accuracy` and `function_name_and_args_accuracy` numbers, which are as their names imply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e2e116-ed60-4e88-970b-f0d09d4a258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tool_calling_eval_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"tasks\": {\n",
    "        \"custom-tool-calling\": {\n",
    "            \"type\": \"chat-completion\",\n",
    "            \"dataset\": {\n",
    "                \"files_url\": f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}/testing/xlam-test-single.jsonl\",\n",
    "                \"limit\": 50\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"template\": {\n",
    "                    \"messages\": \"{{ item.messages | tojson}}\",\n",
    "                    \"tools\": \"{{ item.tools | tojson }}\",\n",
    "                    \"tool_choice\": \"auto\"\n",
    "                }\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"tool-calling-accuracy\": {\n",
    "                    \"type\": \"tool-calling\",\n",
    "                    \"params\": {\"tool_calls_ground_truth\": \"{{ item.tool_calls | tojson }}\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8007f17-2eb1-477f-b156-5ed1d17d894b",
   "metadata": {},
   "source": [
    "### 1.2: Launch Evaluation Job "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fbf88",
   "metadata": {},
   "source": [
    "The following code calls the `nemo_client.evaluation.jobs.create()` method to launch an evaluation job in the NeMo Evaluator.\n",
    "It uses the evaluation configuration defined in the previous cell and targets the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c5ec90-a189-47d6-9c07-18162b95130d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation job: eval-6QisJKdfmRquYXdFBNBAD7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationJob(config=Config(type='custom', id='eval-config-UEphFyZQvSb5rFAkPuRYuu', created_at=datetime.datetime(2025, 6, 20, 16, 36, 27, 493465), custom_fields={}, description=None, groups=None, name='eval-config-UEphFyZQvSb5rFAkPuRYuu', namespace='default', ownership=None, params=None, project=None, schema_version='1.0', tasks={'custom-tool-calling': ConfigTasks(type='chat-completion', dataset=ConfigTasksDatasetDatasetEv(files_url='hf://datasets/xlam-tutorial-ns/xlam-ft-dataset/testing/xlam-test-single.jsonl', id='dataset-UAEo4C8C4p6uEqTFTn2v8B', created_at=datetime.datetime(2025, 6, 20, 16, 36, 27, 493606), custom_fields={}, description=None, format=None, hf_endpoint=None, limit=50, name='dataset-UAEo4C8C4p6uEqTFTn2v8B', namespace='default', ownership=None, project=None, schema_version='1.0', split=None, type_prefix=None, updated_at=datetime.datetime(2025, 6, 20, 16, 36, 27, 493607), version_id='main', version_tags=[]), metrics={'tool-calling-accuracy': ConfigTasksMetrics(type='tool-calling', params={'tool_calls_ground_truth': '{{ item.tool_calls | tojson }}'})}, params={'template': {'messages': '{{ item.messages | tojson}}', 'tools': '{{ item.tools | tojson }}', 'tool_choice': 'auto'}})}, type_prefix='eval-config', updated_at=datetime.datetime(2025, 6, 20, 16, 36, 27, 493467)), target=Target(type='model', id='eval-target-N8uf4rSaNEAwJ1Kj7YWNve', cached_outputs=None, created_at=datetime.datetime(2025, 6, 20, 16, 36, 27, 493687), custom_fields={}, dataset=None, description=None, model='meta/llama-3.2-1b-instruct', name='eval-target-N8uf4rSaNEAwJ1Kj7YWNve', namespace='default', ownership=None, project=None, rag=None, retriever=None, rows=None, schema_version='1.0', type_prefix='eval-target', updated_at=datetime.datetime(2025, 6, 20, 16, 36, 27, 493688)), id='eval-6QisJKdfmRquYXdFBNBAD7', created_at=datetime.datetime(2025, 6, 20, 16, 36, 27, 493989), custom_fields={}, description=None, namespace='default', output_files_url=None, ownership=None, project=None, result=None, status='created', status_details={'message': None, 'task_status': {}, 'progress': None}, updated_at=datetime.datetime(2025, 6, 20, 16, 36, 27, 493990))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluation job for the base model\n",
    "eval_job = nemo_client.evaluation.jobs.create(\n",
    "    config=simple_tool_calling_eval_config,\n",
    "    target={\"type\": \"model\", \"model\": BASE_MODEL}\n",
    ")\n",
    "\n",
    "base_eval_job_id = eval_job.id\n",
    "print(f\"Created evaluation job: {base_eval_job_id}\")\n",
    "eval_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5133e-076c-4560-815a-1ff71901af01",
   "metadata": {},
   "source": [
    "The following code defines a helper function to poll on job status until it finishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa0b694-a699-4eb0-903c-9bac8651f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_eval_job(nemo_client, job_id: str, polling_interval: int = 10, timeout: int = 6000):\n",
    "    \"\"\"Helper for waiting an eval job.\"\"\"\n",
    "    start_time = time()\n",
    "    job = nemo_client.evaluation.jobs.retrieve(job_id=job_id)\n",
    "    status = job.status\n",
    "\n",
    "    while (status in [\"pending\", \"created\", \"running\"]):\n",
    "        # Check for timeout\n",
    "        if time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Took more than {timeout} seconds.\")\n",
    "\n",
    "        # Sleep before polling again\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        # Fetch updated status and progress\n",
    "        job = nemo_client.evaluation.jobs.retrieve(job_id=job_id)\n",
    "        status = job.status\n",
    "\n",
    "        # Progress details (only fetch if status is \"running\")\n",
    "        progress = 0\n",
    "        if status == \"running\" and job.status_details:\n",
    "            progress = job.status_details.progress or 0\n",
    "        elif status == \"completed\":\n",
    "            progress = 100\n",
    "\n",
    "        print(f\"Job status: {status} after {time() - start_time:.2f} seconds. Progress: {progress}%\")\n",
    "\n",
    "    return job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db71fc",
   "metadata": {},
   "source": [
    "Run the helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48322fdb-8920-435d-863d-9a75158843d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 5.03 seconds. Progress: 0.0%\n",
      "Job status: running after 10.05 seconds. Progress: 0.0%\n",
      "Job status: running after 15.06 seconds. Progress: 0.0%\n",
      "Job status: running after 20.08 seconds. Progress: 0.0%\n",
      "Job status: running after 25.09 seconds. Progress: 8.0%\n",
      "Job status: running after 30.11 seconds. Progress: 12.0%\n",
      "Job status: running after 35.12 seconds. Progress: 16.0%\n",
      "Job status: running after 40.13 seconds. Progress: 26.0%\n",
      "Job status: running after 45.15 seconds. Progress: 26.0%\n",
      "Job status: running after 50.16 seconds. Progress: 26.0%\n",
      "Job status: running after 55.18 seconds. Progress: 26.0%\n",
      "Job status: running after 60.20 seconds. Progress: 26.0%\n",
      "Job status: running after 65.21 seconds. Progress: 26.0%\n",
      "Job status: running after 70.23 seconds. Progress: 32.0%\n",
      "Job status: running after 75.24 seconds. Progress: 32.0%\n",
      "Job status: running after 80.26 seconds. Progress: 38.0%\n",
      "Job status: running after 85.28 seconds. Progress: 38.0%\n",
      "Job status: running after 90.29 seconds. Progress: 38.0%\n",
      "Job status: running after 95.31 seconds. Progress: 40.0%\n",
      "Job status: running after 100.32 seconds. Progress: 48.0%\n",
      "Job status: running after 105.34 seconds. Progress: 50.0%\n",
      "Job status: running after 110.35 seconds. Progress: 60.0%\n",
      "Job status: running after 115.37 seconds. Progress: 60.0%\n",
      "Job status: running after 120.38 seconds. Progress: 60.0%\n",
      "Job status: running after 125.40 seconds. Progress: 60.0%\n",
      "Job status: running after 130.42 seconds. Progress: 60.0%\n",
      "Job status: running after 135.43 seconds. Progress: 62.0%\n",
      "Job status: running after 140.45 seconds. Progress: 62.0%\n",
      "Job status: running after 145.46 seconds. Progress: 62.0%\n",
      "Job status: running after 150.48 seconds. Progress: 62.0%\n",
      "Job status: running after 155.50 seconds. Progress: 62.0%\n",
      "Job status: running after 160.51 seconds. Progress: 64.0%\n",
      "Job status: running after 165.53 seconds. Progress: 64.0%\n",
      "Job status: running after 170.55 seconds. Progress: 68.0%\n",
      "Job status: running after 175.56 seconds. Progress: 72.0%\n",
      "Job status: running after 180.58 seconds. Progress: 82.0%\n",
      "Job status: running after 185.60 seconds. Progress: 84.0%\n",
      "Job status: running after 190.61 seconds. Progress: 84.0%\n",
      "Job status: running after 195.63 seconds. Progress: 84.0%\n",
      "Job status: running after 200.65 seconds. Progress: 88.0%\n",
      "Job status: running after 205.67 seconds. Progress: 88.0%\n",
      "Job status: running after 210.68 seconds. Progress: 92.0%\n",
      "Job status: running after 215.70 seconds. Progress: 92.0%\n",
      "Job status: running after 220.71 seconds. Progress: 92.0%\n",
      "Job status: running after 225.73 seconds. Progress: 94.0%\n",
      "Job status: running after 230.75 seconds. Progress: 100.0%\n",
      "Job status: completed after 235.77 seconds. Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "# Poll\n",
    "job = wait_eval_job(nemo_client, base_eval_job_id, polling_interval=5, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de58a76-1775-4c93-8798-1ec89c8eeaff",
   "metadata": {},
   "source": [
    "### 1.3 Review Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb6220",
   "metadata": {},
   "source": [
    "The following code retrieves the evaluation results for the base evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec04c15a-62a9-4271-8a7f-9a7e8ec7884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"job\": \"eval-6QisJKdfmRquYXdFBNBAD7\",\n",
      "  \"id\": \"evaluation_result-B8PqbKeE9KrzMUxGRCcpF2\",\n",
      "  \"created_at\": \"2025-06-20T16:36:27.527337\",\n",
      "  \"custom_fields\": {},\n",
      "  \"groups\": {},\n",
      "  \"namespace\": \"default\",\n",
      "  \"tasks\": {\n",
      "    \"custom-tool-calling\": {\n",
      "      \"metrics\": {\n",
      "        \"tool-calling-accuracy\": {\n",
      "          \"scores\": {\n",
      "            \"function_name_accuracy\": {\n",
      "              \"value\": 0.12,\n",
      "              \"stats\": {\n",
      "                \"count\": 50,\n",
      "                \"mean\": 0.16,\n",
      "                \"sum\": 8.0\n",
      "              }\n",
      "            },\n",
      "            \"function_name_and_args_accuracy\": {\n",
      "              \"value\": 0.08,\n",
      "              \"stats\": {\n",
      "                \"count\": 50,\n",
      "                \"mean\": 0.12,\n",
      "                \"sum\": 6.0\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"updated_at\": \"2025-06-20T16:36:27.527338\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "results = nemo_client.evaluation.jobs.results(job_id=base_eval_job_id)\n",
    "print(results.model_dump_json(indent=2, exclude_unset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b28813",
   "metadata": {},
   "source": [
    "The following code extracts and prints the accuracy scores for the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2129c739-8980-4d28-ab72-331e00f0c2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: function_name_accuracy: 0.12\n",
      "Base model: function_name_and_args_accuracy: 0.08\n"
     ]
    }
   ],
   "source": [
    "# Extract function name accuracy score\n",
    "base_scores = results.tasks[\"custom-tool-calling\"].metrics[\"tool-calling-accuracy\"].scores\n",
    "base_function_name_accuracy_score = base_scores[\"function_name_accuracy\"].value\n",
    "base_function_name_and_args_accuracy = base_scores[\"function_name_and_args_accuracy\"].value\n",
    "\n",
    "print(f\"Base model: function_name_accuracy: {base_function_name_accuracy_score}\")\n",
    "print(f\"Base model: function_name_and_args_accuracy: {base_function_name_and_args_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fcb78-0e23-45f6-988a-8be6e33c2cfc",
   "metadata": {},
   "source": [
    "Without any finetuning, the `meta/llama-3.2-1b-instruct` model should score in the ballpark of about 12% in `function_name_accuracy`, and 8% in `function_name_and_args_accuracy` (note that scores will vary by about +/-4% due to non-determinism of LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c7ab4-bb3c-46c3-a51e-9e25509a7781",
   "metadata": {},
   "source": [
    "### (Optional) 1.4 Download and Inspect Results\n",
    "\n",
    "To take a deeper look into the model's generated outputs, you can download and review the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36bf02e0-2383-46d5-ad47-3edc1e1dfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_evaluation_results(nemo_client, eval_job_id, output_file):\n",
    "    \"\"\"Downloads evaluation results for a given job ID.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get download results\n",
    "        results = nemo_client.evaluation.jobs.download_results(job_id=eval_job_id)\n",
    "        \n",
    "        # Save the results to a file\n",
    "        results.write_to_file(output_file)\n",
    "\n",
    "        print(f\"Evaluation results for job {eval_job_id} downloaded successfully to {output_file}.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download evaluation results: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da8ec595-8edd-4d8c-a2a0-29f830710c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for job eval-6QisJKdfmRquYXdFBNBAD7 downloaded successfully to eval-6QisJKdfmRquYXdFBNBAD7.zip.\n"
     ]
    }
   ],
   "source": [
    "output_file = f\"{base_eval_job_id}.zip\"\n",
    "\n",
    "# Assertion fails if download fails\n",
    "assert download_evaluation_results(nemo_client=nemo_client, eval_job_id=base_eval_job_id, output_file=output_file) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f633d-5b20-4a28-8962-970a54b91f40",
   "metadata": {},
   "source": [
    "You can inspect the downloaded results file to observe places where the base model errors. Without any fine-tuning, some models not only return inaccurate function names and arguments, but they may not adhere to a consistent structured / predictable output schema. This makes it difficult to automatically parse these outputs, deterring integration with external systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79229ce6-4ce4-4f82-b0c9-15be351a2291",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-2\"></a>\n",
    "## Step 2: Evaluate the LoRA Customized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdeba8-6800-419f-b5c5-0dc34bf6e0f5",
   "metadata": {},
   "source": [
    "### 2.1 Launch Evaluation Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb900e7-2ec8-417a-9266-2ce682f655e5",
   "metadata": {},
   "source": [
    "Run another evaluation job with the same evaluation config but with the customized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5438608-e708-4421-bf45-da39fbe506ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation job for customized model: eval-QbibneF16tyDiCJWc9oxgZ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationJob(config=Config(type='custom', id='eval-config-QZNjbAHSWjNr5FvAVhuzD', created_at=datetime.datetime(2025, 6, 20, 16, 55, 38, 581764), custom_fields={}, description=None, groups=None, name='eval-config-QZNjbAHSWjNr5FvAVhuzD', namespace='default', ownership=None, params=None, project=None, schema_version='1.0', tasks={'custom-tool-calling': ConfigTasks(type='chat-completion', dataset=ConfigTasksDatasetDatasetEv(files_url='hf://datasets/xlam-tutorial-ns/xlam-ft-dataset/testing/xlam-test-single.jsonl', id='dataset-DRno5c1nwQnfmKJQAd5KQA', created_at=datetime.datetime(2025, 6, 20, 16, 55, 38, 581838), custom_fields={}, description=None, format=None, hf_endpoint=None, limit=50, name='dataset-DRno5c1nwQnfmKJQAd5KQA', namespace='default', ownership=None, project=None, schema_version='1.0', split=None, type_prefix=None, updated_at=datetime.datetime(2025, 6, 20, 16, 55, 38, 581839), version_id='main', version_tags=[]), metrics={'tool-calling-accuracy': ConfigTasksMetrics(type='tool-calling', params={'tool_calls_ground_truth': '{{ item.tool_calls | tojson }}'})}, params={'template': {'messages': '{{ item.messages | tojson}}', 'tools': '{{ item.tools | tojson }}', 'tool_choice': 'auto'}})}, type_prefix='eval-config', updated_at=datetime.datetime(2025, 6, 20, 16, 55, 38, 581766)), target=Target(type='model', id='eval-target-PgDfKhiTDkXtx5BsQVAzTQ', cached_outputs=None, created_at=datetime.datetime(2025, 6, 20, 16, 55, 38, 581900), custom_fields={}, dataset=None, description=None, model='xlam-tutorial-ns/llama-3.2-1b-xlam-run1@cust-FarcM8gwhL1XFDXQ57qGLL', name='eval-target-PgDfKhiTDkXtx5BsQVAzTQ', namespace='default', ownership=None, project=None, rag=None, retriever=None, rows=None, schema_version='1.0', type_prefix='eval-target', updated_at=datetime.datetime(2025, 6, 20, 16, 55, 38, 581900)), id='eval-QbibneF16tyDiCJWc9oxgZ', created_at=datetime.datetime(2025, 6, 20, 16, 55, 38, 582286), custom_fields={}, description=None, namespace='default', output_files_url=None, ownership=None, project=None, result=None, status='created', status_details={'message': None, 'task_status': {}, 'progress': None}, updated_at=datetime.datetime(2025, 6, 20, 16, 55, 38, 582287))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluation job for customized model\n",
    "ft_eval_job = nemo_client.evaluation.jobs.create(\n",
    "    config=simple_tool_calling_eval_config,\n",
    "    target={\"type\": \"model\", \"model\": CUSTOMIZED_MODEL}\n",
    ")\n",
    "\n",
    "ft_eval_job_id = ft_eval_job.id\n",
    "print(f\"Created evaluation job for customized model: {ft_eval_job_id}\")\n",
    "ft_eval_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7409f25-23c1-4574-9392-051f3da0498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 5.03 seconds. Progress: 0.0%\n",
      "Job status: running after 10.04 seconds. Progress: 0.0%\n",
      "Job status: running after 15.06 seconds. Progress: 0.0%\n",
      "Job status: running after 20.08 seconds. Progress: 0.0%\n",
      "Job status: running after 25.09 seconds. Progress: 8.0%\n",
      "Job status: running after 30.11 seconds. Progress: 12.0%\n",
      "Job status: running after 35.13 seconds. Progress: 18.0%\n",
      "Job status: running after 40.14 seconds. Progress: 26.0%\n",
      "Job status: running after 45.16 seconds. Progress: 26.0%\n",
      "Job status: running after 50.18 seconds. Progress: 26.0%\n",
      "Job status: running after 55.19 seconds. Progress: 26.0%\n",
      "Job status: running after 60.21 seconds. Progress: 26.0%\n",
      "Job status: running after 65.22 seconds. Progress: 28.0%\n",
      "Job status: running after 70.24 seconds. Progress: 32.0%\n",
      "Job status: running after 75.26 seconds. Progress: 32.0%\n",
      "Job status: running after 80.27 seconds. Progress: 38.0%\n",
      "Job status: running after 85.29 seconds. Progress: 38.0%\n",
      "Job status: running after 90.30 seconds. Progress: 38.0%\n",
      "Job status: running after 95.32 seconds. Progress: 40.0%\n",
      "Job status: running after 100.34 seconds. Progress: 50.0%\n",
      "Job status: running after 105.35 seconds. Progress: 50.0%\n",
      "Job status: running after 110.37 seconds. Progress: 60.0%\n",
      "Job status: running after 115.38 seconds. Progress: 60.0%\n",
      "Job status: running after 120.40 seconds. Progress: 60.0%\n",
      "Job status: running after 125.42 seconds. Progress: 60.0%\n",
      "Job status: running after 130.43 seconds. Progress: 60.0%\n",
      "Job status: running after 135.45 seconds. Progress: 62.0%\n",
      "Job status: running after 140.46 seconds. Progress: 62.0%\n",
      "Job status: running after 145.48 seconds. Progress: 62.0%\n",
      "Job status: running after 150.49 seconds. Progress: 62.0%\n",
      "Job status: running after 155.51 seconds. Progress: 62.0%\n",
      "Job status: running after 160.52 seconds. Progress: 64.0%\n",
      "Job status: running after 165.54 seconds. Progress: 64.0%\n",
      "Job status: running after 170.56 seconds. Progress: 68.0%\n",
      "Job status: running after 175.57 seconds. Progress: 74.0%\n",
      "Job status: running after 180.59 seconds. Progress: 84.0%\n",
      "Job status: running after 185.60 seconds. Progress: 84.0%\n",
      "Job status: running after 190.61 seconds. Progress: 84.0%\n",
      "Job status: running after 195.63 seconds. Progress: 84.0%\n",
      "Job status: running after 200.65 seconds. Progress: 88.0%\n",
      "Job status: running after 205.66 seconds. Progress: 88.0%\n",
      "Job status: running after 210.68 seconds. Progress: 92.0%\n",
      "Job status: running after 215.70 seconds. Progress: 92.0%\n",
      "Job status: running after 220.71 seconds. Progress: 92.0%\n",
      "Job status: running after 225.73 seconds. Progress: 94.0%\n",
      "Job status: completed after 230.75 seconds. Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "# Poll\n",
    "job = wait_eval_job(nemo_client, ft_eval_job_id, polling_interval=5, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706c060-8b37-40e2-b0e6-636d130260e7",
   "metadata": {},
   "source": [
    "### 2.2 Review Evaluation Metrics\n",
    "The following code retrieves the evaluation results for the fine-tuned model evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d2ffdb8-8e8e-403e-a836-8bcd93778814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"job\": \"eval-QbibneF16tyDiCJWc9oxgZ\",\n",
      "  \"id\": \"evaluation_result-B2uHJZANeXGz92ME8McJj5\",\n",
      "  \"created_at\": \"2025-06-20T16:55:38.609088\",\n",
      "  \"custom_fields\": {},\n",
      "  \"description\": null,\n",
      "  \"files_url\": null,\n",
      "  \"groups\": {},\n",
      "  \"namespace\": \"default\",\n",
      "  \"ownership\": null,\n",
      "  \"project\": null,\n",
      "  \"tasks\": {\n",
      "    \"custom-tool-calling\": {\n",
      "      \"metrics\": {\n",
      "        \"tool-calling-accuracy\": {\n",
      "          \"scores\": {\n",
      "            \"function_name_accuracy\": {\n",
      "              \"value\": 0.92,\n",
      "              \"stats\": {\n",
      "                \"count\": 50,\n",
      "                \"max\": null,\n",
      "                \"mean\": 0.92,\n",
      "                \"min\": null,\n",
      "                \"stddev\": null,\n",
      "                \"stderr\": null,\n",
      "                \"sum\": 46.0,\n",
      "                \"sum_squared\": null,\n",
      "                \"variance\": null\n",
      "              }\n",
      "            },\n",
      "            \"function_name_and_args_accuracy\": {\n",
      "              \"value\": 0.72,\n",
      "              \"stats\": {\n",
      "                \"count\": 50,\n",
      "                \"max\": null,\n",
      "                \"mean\": 0.72,\n",
      "                \"min\": null,\n",
      "                \"stddev\": null,\n",
      "                \"stderr\": null,\n",
      "                \"sum\": 36.0,\n",
      "                \"sum_squared\": null,\n",
      "                \"variance\": null\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"updated_at\": \"2025-06-20T16:55:38.609089\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation results for customized model\n",
    "ft_results = nemo_client.evaluation.jobs.results(job_id=ft_eval_job_id)\n",
    "print(ft_results.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adb03140-21ad-445b-b173-7aebdfb0b703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model: function_name_accuracy: 0.92\n",
      "Custom model: function_name_and_args_accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Extract function name accuracy score\n",
    "ft_scores = ft_results.tasks[\"custom-tool-calling\"].metrics[\"tool-calling-accuracy\"].scores\n",
    "ft_function_name_accuracy_score = ft_scores[\"function_name_accuracy\"].value\n",
    "ft_function_name_and_args_accuracy = ft_scores[\"function_name_and_args_accuracy\"].value\n",
    "\n",
    "print(f\"Custom model: function_name_accuracy: {ft_function_name_accuracy_score}\")\n",
    "print(f\"Custom model: function_name_and_args_accuracy: {ft_function_name_and_args_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568a204-ad01-4a04-8cfa-602816b8937c",
   "metadata": {},
   "source": [
    "A successfully fine-tuned `meta/llama-3.2-1b-instruct` results in a significant increase in tool calling accuracy with \n",
    "\n",
    "In this case you should observe roughly the following improvements -\n",
    "* function_name_accuracy: 12% to 92%\n",
    "* function_name_and_args_accuracy: 8% to 72%\n",
    "\n",
    "Since this evaluation was on a limited number of samples for demonstration purposes, you may choose to increase `tasks.dataset.limit` in your evaluation config `simple_tool_calling_eval_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc53b6-677b-4b44-bf6e-bcdadef21a73",
   "metadata": {},
   "source": [
    "## (Optional) Next Steps\n",
    "\n",
    "\n",
    "\n",
    "* You may also run the same evaluation on a base `meta/llama-3.1-70b-instruct` model for comparison.\n",
    "For this, first you will need to deploy the corresponding NIM using instructions [here](https://build.nvidia.com/meta/llama-3_1-70b-instruct/deploy). After your NIM is deployed, set that endpoint as your evaluation target like so -\n",
    "\n",
    "``` python\n",
    "# Create an evaluation target\n",
    "NIM_URL = \"http://0.0.0.0:8000\"\n",
    "EVAL_TARGET = {\n",
    "    \"type\": \"model\", \n",
    "    \"model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": f\"{NIM_URL}/v1/completions\",\n",
    "         \"model_id\": \"meta/llama-3.1-70b-instruct\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Start eval job\n",
    "eval_job = nemo_client.evaluation.jobs.create(\n",
    "    config=simple_tool_calling_eval_config,\n",
    "    target=EVAL_TARGET\n",
    ")\n",
    "```\n",
    "\n",
    "Running evaluation using the default config in this notebook, you should observe `meta/llama-3.1-70b-instruct` performance similar to -\n",
    "* function_name_accuracy: 98%\n",
    "* function_name_and_args_accuracy: 66%\n",
    "\n",
    "Remarkably, a LoRA-tuned `meta/llama-3.2-1b-instruct` achieves accuracy that is close to a model 70 times its size, even outperforming it in the combined `function_name_and_args_accuracy` score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406f95b-4fc5-462a-ad6c-43196b70896d",
   "metadata": {},
   "source": [
    "You can now proceed with the same processes to fine-tune other NIM for LLMs and evaluate the accuracies between the base model and the fine-tuned model. By doing so, you can produce more accurate models for your use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

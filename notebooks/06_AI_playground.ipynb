{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d4e77f-cc3d-402d-987b-a9c361ce99e1",
   "metadata": {},
   "source": [
    "# Notebook 6: RAG with NVIDIA AI Playground and Langchain\n",
    "\n",
    "**NVIDIA AI Playground** on NGC allows developers to experience state of the art LLMs accelerated on NVIDIA DGX Cloud with NVIDIA TensorRT nd Triton Inference Server. Developers get **free credits for 10K requests** to any of the available models. Sign up process is easy. Follow the instructions <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RetrievalAugmentedGeneration/docs/aiplayground.md\"> here </a>\n",
    "\n",
    "This notebook demonstrates how to use LangChain and NVIDIA AI Playground to build a chatbot that references a custom knowledge-base. \n",
    "\n",
    "Suppose you have some text documents (PDF, blog, Notion pages, etc.) and want to ask questions related to the contents of those documents. LLMs, given their proficiency in understanding text, are a great tool for this. \n",
    "\n",
    "### [LangChain](https://python.langchain.com/docs/get_started/introduction)\n",
    "[**LangChain**](https://python.langchain.com/docs/get_started/introduction) provides a simple framework for connecting LLMs to your own data sources. Since LLMs are both only trained up to a fixed point in time and do not contain knowledge that is proprietary to an Enterprise, they can't answer questions about new or proprietary knowledge. LangChain solves this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23110dc7-df11-4413-b8d3-9db2f3e1187a",
   "metadata": {},
   "source": [
    "![data_connection](./imgs/data_connection_langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6806b29-cbc1-4dc8-b378-5c6333eb7229",
   "metadata": {},
   "source": [
    "### Step 1: Load Documents [*(Retrieval)*](https://python.langchain.com/docs/modules/data_connection/)\n",
    "LangChain provides a variety of [document loaders](https://python.langchain.com/docs/integrations/document_loaders) that load various types of documents (HTML, PDF, code) from many different sources and locations (private s3 buckets, public websites).\n",
    "\n",
    "Document loaders load data from a source as **Documents**. A **Document** is a piece of text (the page_content) and associated metadata. Document loaders provide a ``load`` method for loading data as documents from a configured source. \n",
    "\n",
    "In this example, we use a LangChain [`UnstructuredFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file) to load a research paper about Llama2 from Meta.\n",
    "\n",
    "[Here](https://python.langchain.com/docs/integrations/document_loaders) are some of the other document loaders available from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09ede8-58cc-44d5-b46c-b1060dd578c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O \"llama2_paper.pdf\" -nc --user-agent=\"Mozilla\" https://arxiv.org/pdf/2307.09288.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43793ae9-f25f-4e9d-a7a5-13b64f02b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "loader = UnstructuredFileLoader(\"llama2_paper.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8568d-c20e-4d79-a828-c2bcd2b6cd20",
   "metadata": {},
   "source": [
    "### Step 2: Transform Documents [*(Retrieval)*](https://python.langchain.com/docs/modules/data_connection/)\n",
    "Once documents have been loaded, they are often transformed. One method of transformation is known as **chunking**, which breaks down large pieces of text, for example, a long document, into smaller segments. This technique is valuable because it helps [optimize the relevance of the content returned from the vector database](https://www.pinecone.io/learn/chunking-strategies/). \n",
    "\n",
    "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/), such as text splitters. In this example, we use a [``SentenceTransformersTokenTextSplitter``](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.SentenceTransformersTokenTextSplitter.html#langchain.text_splitter.SentenceTransformersTokenTextSplitter). The ``SentenceTransformersTokenTextSplitter`` is a specialized text splitter for use with the sentence-transformer models. The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use. This sentence transformer model is used to generate the embeddings from documents. \n",
    "\n",
    "There are some nuanced complexities to text splitting since semantically related text, in theory, should be kept together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea998e-c776-4763-b811-b42b6372fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "TEXT_SPLITTER_MODEL = \"intfloat/e5-large-v2\"\n",
    "TEXT_SPLITTER_CHUNCK_SIZE = 510\n",
    "TEXT_SPLITTER_CHUNCK_OVERLAP = 200\n",
    "\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=TEXT_SPLITTER_MODEL,\n",
    "    chunk_size=TEXT_SPLITTER_CHUNCK_SIZE,\n",
    "    chunk_overlap=TEXT_SPLITTER_CHUNCK_OVERLAP,\n",
    ")\n",
    "start_time = time.time()\n",
    "documents = text_splitter.split_documents(data)\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a07ff-119a-468c-ab68-f8f6e9f7c6fc",
   "metadata": {},
   "source": [
    "### Step 3: Generate Embeddings and Store Embeddings in the Vector Store [*(Retrieval)*](https://python.langchain.com/docs/modules/data_connection/)\n",
    "\n",
    "#### a) Generate Embeddings\n",
    "[Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) for documents are created by vectorizing the document text; this vectorization captures the semantic meaning of the text. This allows you to quickly and efficiently find other pieces of text that are similar. The embedding model used below is [intfloat/e5-large-v2](https://huggingface.co/intfloat/e5-large-v2).\n",
    "\n",
    "LangChain provides a wide variety of [embedding models](https://python.langchain.com/docs/integrations/text_embedding) from many providers and makes it simple to swap out the models. \n",
    "\n",
    "When a user sends in their query, the query is also embedded using the same embedding model that was used to embed the documents. As explained earlier, this allows to find similar (relevant) documents to the user's query. \n",
    "\n",
    "#### b) Store Document Embeddings in the Vector Store\n",
    "Once the document embeddings are generated, they are stored in a vector store so that at query time we can:\n",
    "1) Embed the user query and\n",
    "2) Retrieve the embedding vectors that are most similar to the embedding query.\n",
    "\n",
    "A vector store takes care of storing the embedded data and performing a vector search.\n",
    "\n",
    "LangChain provides support for a [great selection of vector stores](https://python.langchain.com/docs/integrations/vectorstores/). \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "⚠️ For this workflow, [Milvus](https://milvus.io/) vector database is running as a microservice. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3be33-5f78-4937-9e07-a7cc93d5d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Milvus\n",
    "import torch\n",
    "import time\n",
    "\n",
    "#Running the model on CPU as we want to conserve gpu memory.\n",
    "#In the production deployment (API server shown as part of the 5th notebook we run the model on GPU)\n",
    "model_name = \"intfloat/e5-large-v2\"\n",
    "model_kwargs = {\"device\": \"cuda:0\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "start_time = time.time()\n",
    "vectorstore = Milvus.from_documents(documents=documents, embedding=hf_embeddings, connection_args={\"host\": \"milvus\", \"port\": \"19530\"})\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71d0c9-aeab-4a26-959f-5493ca1f2f02",
   "metadata": {},
   "source": [
    "### Step 4: Sign up to NVIDIA AI Playground \n",
    "\n",
    "**NVIDIA AI Playground** on NGC allows developers to experience state of the art LLMs accelerated on NVIDIA DGX Cloud with NVIDIA TensorRT nd Triton Inference Server. Developers get **free credits for 10K requests** to any of the available models. Sign up process is easy. Follow the instructions <a href=\"https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RetrievalAugmentedGeneration/docs/aiplayground.md\"> here </a> and replace the API key below.  For this example we will be using the llama2 13B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddceb0-1f58-48c4-aba4-cc1962ed8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nv_aiplay import GeneralLLM\n",
    "from langchain.callbacks import streaming_stdout\n",
    "\n",
    "os.environ['NVAPI_KEY'] = \"REPLACE_WITH_API_KEY\"\n",
    "\n",
    "callbacks = [streaming_stdout.StreamingStdOutCallbackHandler()]\n",
    "\n",
    "llm = GeneralLLM(\n",
    "    temperature=0.2,\n",
    "    max_tokens=300,\n",
    "    streaming=True,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25e8af-9aff-460a-a214-87a3e285469d",
   "metadata": {},
   "source": [
    "### Step 5: Ask a question without context\n",
    "\n",
    "Send request to the llm without any context from the vector DB. The answer is generic and irrelvant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fced303-d825-4797-9e4a-9a8c6e5f82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can you talk about the safety features of llama2 chat?\"\n",
    "answer = llm(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225fc935-4bc9-4957-a476-6d23d147e664",
   "metadata": {},
   "source": [
    "### Step 6: Compose a streamed answer using a Chain\n",
    "We have already integrated the Llama2 TRT LLM into LangChain with a custom wrapper, loaded and transformed documents, and generated and stored document embeddings in a vector database. To finish the pipeline, we need to add a few more LangChain components and combine all the components together with a [chain](https://python.langchain.com/docs/modules/chains/).\n",
    "\n",
    "A [LangChain chain](https://python.langchain.com/docs/modules/chains/) combines components together. In this case, we use a [RetrievalQA chain](https://js.langchain.com/docs/modules/chains/popular/vector_db_qa/), which is a chain type for question-answering against a vector index. It combines a *Retriever* and a *question answering (QA) chain*.\n",
    "\n",
    "We pass it 3 of our LangChain components:\n",
    "- Our instance of the LLM (from step 1).\n",
    "- A [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/), which is an interface that returns documents given an unstructured query. In this case, we use our vector store as the retriever.\n",
    "- Our prompt template constructed from the prompt format for Llama2 (from step 2)\n",
    "\n",
    "```\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": LLAMA_PROMPT}\n",
    ")\n",
    "```\n",
    "\n",
    "Lastly, we pass a user query to the chain and stream the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7bac7-fda6-4584-b469-43f9eba75468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "result = qa_chain({\"query\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

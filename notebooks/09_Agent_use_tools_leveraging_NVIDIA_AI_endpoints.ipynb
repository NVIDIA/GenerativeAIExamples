{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f2afe4",
   "metadata": {},
   "source": [
    "# Multimodal Models from NVIDIA AI Endpoints with LangChain Agent \n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To run this notebook, you need the following:\n",
    "\n",
    "1. Performed the [setup](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints#setup) and generated an API key.\n",
    "\n",
    "2. Installed Python dependencies from [requirements.txt](https://github.com/NVIDIA/GenerativeAIExamples/blob/main/notebooks/requirements.txt).\n",
    "\n",
    "3. Installed additional packages for this notebook: \n",
    "\n",
    "        pip install gradio matplotlib scikit-image\n",
    "\n",
    "\n",
    "This notebook covers the following custom plug-in components:\n",
    "\n",
    "- LLM using NVIDIA AI Endpoint mixtral_8x7b\n",
    "    \n",
    "- A NVIDIA AI endpoint **Deplot** as one of the tool\n",
    "\n",
    "- A NVIDIA AI endpoint **NeVa** as one of the tool\n",
    "    \n",
    "- Gradio as the simply User Interface where we will upload a few images\n",
    "\n",
    "At the end of the day, as below illustrated, we would like to have a UI which allow user to upload image of their choice and have the agent choose tools to do visual reasoning. \n",
    "\n",
    "![interactive UI](./imgs/visual_reasoning.png)    \n",
    "Note: As one can see, since we are using NVIDIA AI endpoints as an API, there is no further requirement in the prerequisites about GPUs as compute hardware\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565acc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the below to install additional python packages.\n",
    "#!pip install unstructured\n",
    "#!pip install matplotlib scikit-image\n",
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62d8fe",
   "metadata": {},
   "source": [
    "### Step 1  - Export the NVIDIA_API_KEY\n",
    "You can supply the NVIDIA_API_KEY directly in this notebook when you run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5578093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "## API Key can be found by going to NVIDIA NGC -> AI Foundation Models -> (some model) -> Get API Code or similar.\n",
    "## 10K free queries to any endpoint (which is a lot actually).\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "global nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a6dba",
   "metadata": {},
   "source": [
    "### Step 2 - wrap the NeVa API call into a function and verify by supplying an image to get a respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d055c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, httpx, sys\n",
    "\n",
    "import base64, io\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def img2base64_string(img_path):\n",
    "    image = Image.open(img_path)\n",
    "    if image.width > 800 or image.height > 800:\n",
    "        image.thumbnail((800, 800))\n",
    "    buffered = io.BytesIO()\n",
    "    image.convert(\"RGB\").save(buffered, format=\"JPEG\", quality=85)\n",
    "    image_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "    return image_base64\n",
    "\n",
    "def nv_api_response(prompt, img_path):\n",
    "    base = \"https://api.nvcf.nvidia.com\"\n",
    "    url = \"/v2/nvcf/pexec/functions/8bf70738-59b9-4e5f-bc87-7ab4203be7a0\"\n",
    "\n",
    "    # Get your key at: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/neva-22b/api\n",
    "    # click on the \"Generate Key\" button\n",
    "\n",
    "    def hook(request):\n",
    "        request.url = httpx.URL(request.url, path=url)\n",
    "        request.headers['Accept'] = 'text/event-stream'\n",
    "\n",
    "    client = openai.OpenAI(\n",
    "        base_url=base,\n",
    "        api_key=nvapi_key,\n",
    "        http_client=httpx.Client(event_hooks={'request': [hook]})\n",
    "    )\n",
    "    base64_str=img2base64_string(img_path)\n",
    "\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"neva-22b\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{base64_str}\"}         # or image/jpeg\n",
    "                ]\n",
    "            },\n",
    "\n",
    "            # {\"role\": \"assistant\", \"labels\": {'creativity': 0}}   # Uncomment to get less verbose response\n",
    "        ],\n",
    "        max_tokens=512,     # Minimum 32, maximum 512. This is a bug.\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        stream=True         # Use streaming mode for responses longer than 32 tokens.\n",
    "    )\n",
    "\n",
    "    for chunk in result:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6d798",
   "metadata": {},
   "source": [
    "fetch a test image of a pair of white sneakers and verify the function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18260b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://docs.google.com/uc?export=download&id=12ZpBBFkYu-jzz1iz356U5kMikn4uN9ww\" -O ./toy_data/jordan.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"./toy_data/jordan.png\"\n",
    "prompt=\"describe the image\"\n",
    "out=nv_api_response(prompt,img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36891baa",
   "metadata": {},
   "source": [
    "### Step 3 - we are gonna use mixtral_8x7b model as our main LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c47713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run and see that you can genreate a respond successfully\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "llm = ChatNVIDIA(model=\"mixtral_8x7b\", nvidia_api_key=nvapi_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c93087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Prerequisites for Image Captioning App User Interface\n",
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64\n",
    "import requests\n",
    "import gradio as gr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cbd029",
   "metadata": {},
   "source": [
    "### Step 4- wrap Deplot and Neva as tools for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f084601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Prerequisites for Image Captioning App User Interface\n",
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64\n",
    "import requests\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import torch\n",
    "#\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "class ImageCaptionTool(BaseTool):\n",
    "    name = \"Image captioner from NeVa\"\n",
    "    description = \"Use this tool when given the path to an image that you would like to be described. \" \\\n",
    "                  \"It will return a simple caption describing the image.\"\n",
    "\n",
    "    # generate api key via https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/neva-22b/api\n",
    "    def img2base64_string(self,img_path):\n",
    "        print(img_path)\n",
    "        image = Image.open(img_path)\n",
    "        if image.width > 800 or image.height > 800:\n",
    "            image.thumbnail((800, 800))\n",
    "        buffered = io.BytesIO()\n",
    "        image.convert(\"RGB\").save(buffered, format=\"JPEG\", quality=85)\n",
    "        image_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        return image_base64\n",
    "\n",
    "    def _run(self, img_path):\n",
    "        invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/8bf70738-59b9-4e5f-bc87-7ab4203be7a0\"\n",
    "        fetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\"\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {nvapi_key}\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "        base64_str = self.img2base64_string(img_path)\n",
    "        prompt = \"\"\"\\\n",
    "        can you summarize what is in the image\\\n",
    "        and return the answer \\\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "          \"messages\":[\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{base64_str}\"}         # or image/jpeg\n",
    "                    ]\n",
    "                },\n",
    "            {\n",
    "          \"labels\": {\n",
    "            \"creativity\": 6,\n",
    "            \"helpfulness\": 6,\n",
    "            \"humor\": 0,\n",
    "            \"quality\": 6\n",
    "          },\n",
    "          \"role\": \"assistant\"\n",
    "            } ],\n",
    "          \"temperature\": 0.2,\n",
    "          \"top_p\": 0.7,\n",
    "          \"max_tokens\": 512,\n",
    "          \"stream\": False\n",
    "        }\n",
    "\n",
    "        # re-use connections\n",
    "        session = requests.Session()\n",
    "\n",
    "        response = session.post(invoke_url, headers=headers, json=payload)\n",
    "        print(response)\n",
    "        while response.status_code == 202:\n",
    "            request_id = response.headers.get(\"NVCF-REQID\")\n",
    "            fetch_url = fetch_url_format + request_id\n",
    "            response = session.get(fetch_url, headers=headers)\n",
    "\n",
    "        response.raise_for_status()\n",
    "        response_body = response.json()\n",
    "        print(response_body)\n",
    "        return response_body['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "\n",
    "class TabularPlotTool(BaseTool):\n",
    "    name = \"Tabular Plot reasoning tool\"\n",
    "    description = \"Use this tool when given the path to an image that contain bar, pie chart objects. \" \\\n",
    "                  \"It will extract and return the tabular data \"\n",
    "\n",
    "    def img2base64_string(self, img_path):\n",
    "        print(img_path)\n",
    "        image = Image.open(img_path)\n",
    "        if image.width > 800 or image.height > 800:\n",
    "            image.thumbnail((800, 800))\n",
    "        buffered = io.BytesIO()\n",
    "        image.convert(\"RGB\").save(buffered, format=\"JPEG\", quality=85)\n",
    "        image_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        return image_base64\n",
    "\n",
    "    def _run(self, img_path):\n",
    "        # using DePlot from NVIDIA AI Endpoint playground, generate your key via :https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/deplot/api\n",
    "        invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/3bc390c7-eeec-40f7-a64d-0c6a719985f7\"\n",
    "        fetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\"\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {nvapi_key}\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        base64_str = self.img2base64_string(img_path)\n",
    "        prompt = \"\"\"\\\n",
    "        can you summarize what is in the image\\\n",
    "        and return the answer \\\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "          \"messages\":[\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{base64_str}\"}         # or image/jpeg\n",
    "                    ]\n",
    "                },\n",
    "            ],\n",
    "          \"temperature\": 0.2,\n",
    "          \"top_p\": 0.7,\n",
    "          \"max_tokens\": 512,\n",
    "          \"stream\": False\n",
    "        }\n",
    "\n",
    "        # re-use connections\n",
    "        session = requests.Session()\n",
    "\n",
    "        response = session.post(invoke_url, headers=headers, json=payload)\n",
    "\n",
    "        while response.status_code == 202:\n",
    "            request_id = response.headers.get(\"NVCF-REQID\")\n",
    "            fetch_url = fetch_url_format + request_id\n",
    "            response = session.get(fetch_url, headers=headers)\n",
    "\n",
    "        response.raise_for_status()\n",
    "        response_body = response.json()\n",
    "        print(response_body)\n",
    "        return response_body['choices'][0]['message']['content']\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59c38a",
   "metadata": {},
   "source": [
    "### Step 5 - initaite the agent with tools we previously defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92047c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the gent\n",
    "tools = [ImageCaptionTool(),TabularPlotTool()]\n",
    "\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    max_iterations=5,\n",
    "    verbose=True,\n",
    "    memory=conversational_memory,\n",
    "    handle_parsing_errors=True,\n",
    "    early_stopping_method='generate'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07766583",
   "metadata": {},
   "source": [
    "### Step 6 - verify the agent can indeed use the tools with the supplied image and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05adfb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is in this image?\"\n",
    "img_path=\"./toy_data/jordan.png\"\n",
    "response = agent.run(f'{user_question}, this is the image path: {img_path}')\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5568dcb",
   "metadata": {},
   "source": [
    "### Step 7 - wrap the agent into a simple gradio UI so we can interactively upload arbitrary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "ImageCaptionApp = gr.Interface(fn=agent,\n",
    "                    inputs=[gr.Image(label=\"Upload image\", type=\"filepath\")],\n",
    "                    outputs=[gr.Textbox(label=\"Caption\")],\n",
    "                    title=\"Image Captioning with langchain agent\",\n",
    "                    description=\"combine langchain agent using tools for image reasoning\",\n",
    "                    allow_flagging=\"never\")\n",
    "\n",
    "ImageCaptionApp.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

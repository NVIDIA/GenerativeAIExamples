{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccac457e",
   "metadata": {},
   "source": [
    "### Retriever API Usage\n",
    "\n",
    "This notebook showcases how to use the NVIDIA RAG retriever APIs to fetch relevant document passages based on user queries and also generate responses using end-to-end RAG APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cfe48-22f4-4526-91f3-b555c45b406d",
   "metadata": {},
   "source": [
    "\n",
    "- Ensure the rag-server container is running before executing the notebook by [following steps in the readme](../docs/quickstart.md#start-the-containers-for-rag-microservices).\n",
    "- Please run the [ingestion notebook](./ingestion_api_usage.ipynb) as a prerequisite to using this notebook.\n",
    "- Replace `IP_ADDRESS` with the actual server URL if the API is hosted on another system.\n",
    "\n",
    "You can now execute each cell in sequence to test the API.\n",
    "#### 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28c582-c0ac-4f04-8c97-433e79be8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiohttp\n",
    "import aiohttp\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541f87b-c9b7-46dc-8d1e-7fb6eb885374",
   "metadata": {},
   "source": [
    "#### 2. Setup Base Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcab5bd-60d5-4354-a3f7-bf8039d78389",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPADDRESS = \"localhost\" #Replace this with the correct IP address\n",
    "RAG_SERVER_PORT = \"8081\"\n",
    "BASE_URL = f\"http://{IPADDRESS}:{RAG_SERVER_PORT}\"  # Replace with your server URL\n",
    "\n",
    "async def print_response(response):\n",
    "    \"\"\"Helper to print API response.\"\"\"\n",
    "    try:\n",
    "        response_json = await response.json()\n",
    "        print(json.dumps(response_json, indent=2))\n",
    "    except aiohttp.ClientResponseError:\n",
    "        print(await response.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451651b-23b6-4854-a734-656c3fac253f",
   "metadata": {},
   "source": [
    "#### 3. Health Check Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint performs a health check on the server. It returns a 200 status code if the server is operational. It also returns the status of the dependent services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753b52b-2728-4201-bf0e-0fbbf4d115bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_health_status():\n",
    "    \"\"\"Fetch health status asynchronously.\"\"\"\n",
    "    url = f\"{BASE_URL}/v1/health\"\n",
    "    params = {\"check_dependencies\": \"True\"} # Check health of dependencies as well\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url, params=params) as response:\n",
    "            await print_response(response)\n",
    "\n",
    "# Run the async function\n",
    "await fetch_health_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf4e4b-78cd-4a6f-a97a-971c18dafcaf",
   "metadata": {},
   "source": [
    "#### 4. Generate Answer Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint generates a streaming AI response to a given user message. The system message is specified in the [prompts.yaml](src/prompt.yaml) file. This API retrieves the relevant chunks related to the query from knowledge base, adds them as part of the LLM prompt and returns a streaming response. It supports parameters like temperature, top_p, knowledge base usage, and also generates based on the specified vector collection. \n",
    "\n",
    "The API endpoint also returns multimodal base64 encoded data if the cited source is an image as part of the returned document chunks. The citations field is always populated as part of the first chunk returned in the streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fdcfd-c845-4cbf-b1ae-d53b2d631b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"{BASE_URL}/v1/generate\"\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"How does the price of bluetooth speaker compare with hammer?\"\n",
    "    }\n",
    "  ],\n",
    "  \"use_knowledge_base\": True,\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"reranker_top_k\": 2,\n",
    "  \"vdb_top_k\": 10,\n",
    "  \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "  \"collection_name\": \"multimodal_data\",\n",
    "  \"enable_query_rewriting\": True,\n",
    "  \"enable_reranker\": True,\n",
    "  \"enable_citations\": True,\n",
    "  \"model\": \"meta/llama-3.1-70b-instruct\",\n",
    "  \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "  \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "  # Provide url of the model endpoints if deployed elsewhere\n",
    "  # \"llm_endpoint\": \"\",\n",
    "  #\"embedding_endpoint\": \"\",\n",
    "  #\"reranker_endpoint\": \"\",\n",
    "  \"stop\": []\n",
    "}\n",
    "\n",
    "async def generate_answer(payload):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=url, json=payload) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "await generate_answer(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc614b-846f-4a91-a09a-061d8e66e21c",
   "metadata": {},
   "source": [
    "#### 5. Document Search Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint searches for the most relevant documents in the vector store based on a query. You can specify the maximum number of documents to retrieve using `reranker_top_k`.  \n",
    "\n",
    "The `content` of the document is returned as well, in case of images representing charts or table, in a base64 represention. Developers can use these base64 representations for rendering multimodal citations to end users. The textual representation of this content is available under `description` field of `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e2709-9de9-49ca-aa43-66182fe31d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"{BASE_URL}/v1/search\"\n",
    "payload={\n",
    "  \"query\": \"Tell me about robert frost's poems\",\n",
    "  \"reranker_top_k\": 2,\n",
    "  \"vdb_top_k\": 10,\n",
    "  \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "  \"collection_name\": \"multimodal_data\",\n",
    "  \"messages\": [],\n",
    "  \"enable_query_rewriting\": True,\n",
    "  \"enable_reranker\": True,\n",
    "  \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "  # Provide url of the model endpoints if deployed elsewhere\n",
    "  #\"embedding_endpoint\": \"\",\n",
    "  #\"reranker_endpoint\": \"\",\n",
    "  \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "\n",
    "}\n",
    "\n",
    "async def document_seach(payload):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=url, json=payload) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "await document_seach(payload)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

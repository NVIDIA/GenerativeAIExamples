{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2941e94f-db20-44a5-ab87-2cab499825f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# An Introduction to Developing Agents with NVIDIA Morpheus\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Generative AI (GenAI)** and **Large Language Models (LLMs)** are becoming essential tools in **cybersecurity** in part due to their ability to enhance the efficiency of cyber threat detection and response by **accelerating analyst workflows**. However, prototyping and moving these accelerated workflows into production can be daunting.\n",
    "\n",
    "Cybersecurity remains among the top three challenges impacting every industry—from the public sector to financial services, telecommunications, retail, automotive, and more. Most CEOs believe organizations with the most **advanced generative AI capabilities** will have a competitive advantage and are looking for ways to incorporate this into their business. While adversaries are already leveraging generative AI in their attacks, there is significant potential to harness this power for **cyber defense**.\n",
    "\n",
    "This hands-on tutorial will focus on **accelerating an exploitability analysis workflow** to increase analyst productivity and enhance cybersecurity defenses.\n",
    "\n",
    "### Problem Statement: Common Vulnerabilities and Exposures (CVE) Impact Analysis\n",
    "\n",
    "Determining the impact of a documented **CVE** on a specific project or container is a labor-intensive and manual task. This intricate process involves the collection, comprehension, and synthesis of various pieces of information to ascertain whether immediate remediation, such as patching, is necessary upon the identification of a new CVE.\n",
    "\n",
    "#### Challenges\n",
    "\n",
    "- **Information Collection:** The process involves significant manual labor to collect and synthesize relevant information.\n",
    "- **Decision Complexity:** Decisions on whether to update a library impacted by a CVE often hinge on various considerations, including:\n",
    "  - **Scan False Positives:** Occasionally, vulnerability scans may incorrectly flag a library as vulnerable, leading to a false alarm.\n",
    "  - **Mitigating Factors:** In some cases, existing safeguards within the environment may reduce or negate the risk posed by a CVE.\n",
    "  - **Lack of Required Environments or Dependencies:** For an exploit to succeed, specific conditions must be met. The absence of these necessary elements can render a vulnerability irrelevant.\n",
    "- **Manual Documentation:** Once an analyst has determined the library is not affected, a **Vulnerability Exploitability eXchange (VEX)** document must be created to standardize and distribute the results.\n",
    "\n",
    "The efficiency of this process can be significantly enhanced through the deployment of an **event-driven LLM agent pipeline**.\n",
    "\n",
    "### Tutorial Goals\n",
    "\n",
    "Our team developed a **cybersecurity vulnerability analysis tool** to aid in assessing the exploitability of CVEs in specific projects and containers. This tutorial will guide you step-by-step through the process of using **LLMs, Retrieval-Augmented Generation (RAG), and agents** to create both a toy version and a microservice running **LLM-powered CVE exploitability analysis**.\n",
    "\n",
    "You'll have the chance to experiment with various modules, boosting your skills and understanding of these technologies. This experience will prepare you to later expand your use case by exploring new functionalities, enhancing the current setup, or even creating your own tailored solutions to meet specific needs or address new challenges.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [0 - Environment Setup](#0---Environment-Setup)\n",
    "- [1 - Intro to Interacting with LLMs](#1---Intro-to-Interacting-with-LLMs)\n",
    "  - [1.1 - Python Calls to LLM API](#1.1---Python-Calls-to-LLM-API)\n",
    "  - [1.2 - Prompt Engineering](#1.2---Prompt-Engineering)\n",
    "  - [1.3 - Prompt Templating](#1.3---Prompt-Templating)\n",
    "  - [1.4 - One-Shot Learning](#1.4---One-Shot-Learning)\n",
    "  - [1.5 - Few-Shot Learning](#1.5---Few-Shot-Learning)\n",
    "  - [1.6 - Evaluation Strategies](#1.6---Evaluation-Strategies)\n",
    "- [2 - Prototyping](#2---Prototyping)\n",
    "  - [2.1 - Overview](#21---overview)\n",
    "  - [2.2 - Building the Vector Database](#22---building-the-vector-database)\n",
    "  - [2.3 - Running a RAG pipeline with Morpheus](#23---running-a-rag-pipeline-with-morpheus)\n",
    "  - [2.4 - Running the CVE Pipeline with Morpheus](#24---running-the-cve-pipeline-with-morpheus)\n",
    "- [3 - Beyond Prototyping](#3---Beyond-Prototyping)\n",
    "  - [3.1 - Improving the Model](#31---improving-the-model)\n",
    "  - [3.2 - Scaling the Pipeline](#32---scaling-the-pipeline)\n",
    "  - [3.3 - Event Driven Pipeline: Creating a Microservice](#33---event-driven-pipeline-creating-a-microservice) \n",
    "- [4 - Conclusion](#4---conclusion)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please continue running the notebook up to Part 1 during the introduction presentation to ensure your environment is set up correctly.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1392801d-3325-4297-974a-e430f5248170",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff405752-d264-49b4-946f-ccb693ccb732",
   "metadata": {},
   "source": [
    "## 0 - Environment Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700a3a1",
   "metadata": {},
   "source": [
    "The following code blocks are used to setup environment variables and imports for the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a60d2a30",
   "metadata": {
    "tags": []
   },
   "source": [
    "%load_ext autoreload\n",
    "%aimport -logging\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "if (\"MORPHEUS_ROOT\" not in os.environ):\n",
    "    os.environ[\"MORPHEUS_ROOT\"] = os.path.abspath(\"../\")\n",
    "\n",
    "llm_dir = os.path.abspath(os.path.join(os.getenv(\"MORPHEUS_ROOT\", \"../\")))\n",
    "\n",
    "if (llm_dir not in sys.path):\n",
    "    sys.path.append(llm_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3523bc32",
   "metadata": {},
   "source": [
    "Ensure the necessary environment variables are set. As a last resort, try to load them from a `.env` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef295094",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ensure that the current environment is set up with API keys\n",
    "required_env_vars = [\"MORPHEUS_ROOT\", \"OPENAI_API_KEY\", \"OPENAI_BASE_URL\"]\n",
    "\n",
    "if (not all([var in os.environ for var in required_env_vars])):\n",
    "\n",
    "    # Try loading an .env file if it exists\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # Check again\n",
    "    if (not all([var in os.environ for var in required_env_vars])):\n",
    "        raise ValueError(f\"Please set the following environment variables: {required_env_vars}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86203ec2",
   "metadata": {},
   "source": [
    "Import some common libraries to allow them to be used later in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2aa2691",
   "metadata": {
    "tags": []
   },
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "import cudf\n",
    "\n",
    "# Finally, ensure Morpheus is installed correctly\n",
    "import morpheus._lib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7932f728",
   "metadata": {},
   "source": [
    "Configure logging to allow Morpheus messages to appear in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9ab76ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configure logging\n",
    "import cyber_dev_day\n",
    "\n",
    "# Create a logger for this module. Use the cyber_dev_day module name because the notebook will just be __main__\n",
    "logger = logging.getLogger(cyber_dev_day.__name__)\n",
    "\n",
    "# Configure the parent logger log level\n",
    "logger.parent.setLevel(logging.INFO)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1cfc3ce8",
   "metadata": {},
   "source": [
    "Finally, test out the logger to ensure that it is working correctly. You should see a message printed to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "78a1fcfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "logger.info(\"Successfully configured logging!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b911e7b9-18bd-4972-a7e1-ff1c976ba66b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Please wait here until instructed to continue with running Part 1 of the notebook.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb62b83",
   "metadata": {},
   "source": [
    "## 1 - Intro to Interacting with LLMs\n",
    "\n",
    "This section will go over how to integrate LLMs into code with Python based examples. We will highlight some of the basic techniques for using and improving calls to LLMs for cybersecurity use cases.\n",
    "\n",
    "- [1.1 - Python Calls to LLM API](#1.1---Python-Calls-to-LLM-API)\n",
    "- [1.2 - Prompt Engineering](#1.2---Prompt-Engineering)\n",
    "- [1.3 - Prompt Templating](#1.3---Prompt-Templating)\n",
    "- [1.4 - One-Shot Learning](#1.4---One-Shot-Learning)\n",
    "- [1.5 - Few-Shot Learning](#1.5---Few-Shot-Learning)\n",
    "- [1.6 - Evaluation Strategies](#1.6---Evaluation-Strategies)\n",
    "\n",
    "In this lab, we will be using [NVIDIA Inference Microservices (NIM)](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/) (previously named NeMo) as our generative AI platform. NIM is a cloud-native framework for building, customizing and deploying generative AI models with a familiar ChatGPT-like interface. Utilizing NIM (or any other generative AI service) in our pipelines allows us to offload the heavy lifting of language model inference to a dedicated service, freeing up our own resources for other tasks. All requests to the NIM service are made via an HTTP API, which allows us to easily integrate it into our existing codebase.\n",
    "\n",
    "To simplify the process of interacting with the LLM, we will use a Python client library ([`openai`](https://pypi.org/project/openai/0.26.5/)) that wraps the HTTP API. This library provides a simple interface for making requests to the LLM, and handles the details of making HTTP requests and parsing the responses. This allows us to focus on the high-level logic of our application, rather than the low-level details of making HTTP requests.\n",
    "\n",
    "Before sending requests to the LLM, we need to set up a connection object, `llm_client` which is shown below. We will use the `completions` endpoint of the connection object to send requests to the LLM for the remainder of this section.\n",
    "\n",
    "It's important to note here that although we store the NGC API Key under the OPENAI_API_KEY variable, we will be interacting with NVIDIA hosted LLMs and not OpenAI LLMs.\n",
    "\n",
    "NVIDIA NIMs are OpenAI API compliant to maximize usability, so we will be using the openai with package as a wrapped to make API calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "67c8c1d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create the connection object. The API key and organization ID are read from the environment variables NGC_API_KEY and\n",
    "# NGC_ORG_ID respectively\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "llm_client = OpenAI(\n",
    "  base_url = base_url,\n",
    "  api_key = api_key\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Connected to LLM hosted at: {base_url}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4639ba36-02ac-4a94-9815-182bbda12c38",
   "metadata": {},
   "source": [
    "### 1.1 - Python Calls to LLM API\n",
    "\n",
    "This section demonstrates executing a call to the LLM API for cybersecurity knowledge support. This could stand alone as a potential use case where we have a cyber knowledge assistant to aid junior cyber analysts.\n",
    "\n",
    "<dt><b>Query</b></dt>\n",
    "<dd><code>How can one determine if a CVE is vulnerable in a specific environment?</code></dd>\n",
    "</dl>\n",
    "\n",
    "The code snippet below utilizes the `chat.completions.create()` method of the connection object (`llm_client`) to query the LLM, detailing the potential **model parameters** that can be provided:\n",
    "\n",
    "- **Temperature**: Controls the creativity of the model. Higher values enable the model to generate more creative outputs.\n",
    "- **Top P**: Controls the creativity of the model. Higher values enable the model to generate more creative outputs, suitable for tasks such as creative writing. This determines the minimum number of highest-probability tokens whose probabilities sum to or exceed the Top P value, from which the next token will be selected at random during text generation.\n",
    "- **Seed**: Affects the generation of random results by the model. It is possible to reproduce results by fixing the random seed (assuming all other hyperparameters are also fixed).\n",
    "- **Presence Penalty**: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "- **Frequency Penalty**: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
    "- **Stream**:If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "739e8a57-d198-4f6b-911b-0500f97a983d",
   "metadata": {
    "tags": []
   },
   "source": [
    "completion = llm_client.chat.completions.create(\n",
    "  model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "  messages=[{\"role\":\"user\",\"content\":\"How can one determine if a CVE is vulnerable in a specific environment?\"}], #Prompt goes here\n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    "    \n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a21d4222-9f6d-46cf-b037-b765889825cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1.1 - Explore On Your Own: Different Models\n",
    "\n",
    "Try another model such as from https://build.nvidia.com/explore/reasoning below\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "70ef49a4-df05-4f68-8cd8-7eb053e31397",
   "metadata": {
    "tags": []
   },
   "source": [
    "# # UNCOMMENT to try different models\n",
    "# completion = llm_client.chat.completions.create(\n",
    "#   model=\"<MODEL_GOES_HERE>\",\n",
    "#   messages=[{\"role\":\"user\",\"content\":\"How can one determine if a CVE is vulnerable in a specific environment?\"}], #Prompt goes here\n",
    "#   temperature=0.5,\n",
    "#   top_p=1,\n",
    "#   max_tokens=1024,\n",
    "#   stream=True\n",
    "    \n",
    "# )\n",
    "\n",
    "# for chunk in completion:\n",
    "#     if chunk.choices[0].delta.content is not None:\n",
    "#         print(chunk.choices[0].delta.content, end=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ffff783-1e41-46cb-a819-e9fb96252d31",
   "metadata": {},
   "source": [
    "#### 1.1.2 - Explore On Your Own: Model Parameters\n",
    "\n",
    "- How do the different models compare? Can you change the parameters (like `temperature` or `presence_penalty`) to help the smaller models improve?\n",
    "\n",
    "- What are some other cybersecurity questions you could ask an LLM to upskill a junior analyst?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec13bd8-e835-4260-8b09-f419513610e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Try a few `temperature` values such as `[0.0, 0.5, 0.7, 0.9]`.\n",
    "\n",
    "What happens to the model's output with higher creativity?"
   ]
  },
  {
   "cell_type": "code",
   "id": "fec55c66-1b44-44a2-a69d-c899c08f4ccf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# # UNCOMMENT to try different parameters\n",
    "# # Analyze output of the model for different value of temperature and top_k\n",
    "# for temp in [0.0, 0.5, 0.7, 0.9]:\n",
    "#     completion = llm_client.chat.completions.create(\n",
    "#       model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "#       messages=[{\"role\":\"user\",\"content\":\"How can one determine if a CVE is vulnerable in a specific environment?\"}], #Prompt goes here\n",
    "#       temperature=temp,\n",
    "#       top_p=1,\n",
    "#       max_tokens=1024,\n",
    "#       stream=True\n",
    "\n",
    "#     )\n",
    "\n",
    "#     for chunk in completion:\n",
    "#         if chunk.choices[0].delta.content is not None:\n",
    "#             print(chunk.choices[0].delta.content, end=\"\")\n",
    "            \n",
    "#     print(\"\\n-----\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a03af5d2-0931-4e0c-a27a-d280c9f77fcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### 1.2 - Prompt Engineering\n",
    "\n",
    "Sometimes, a simple prompt might not deliver the results we're aiming for. That's where prompt engineering steps in.<br>\n",
    "Prompt engineering is an iterative process that focuses on crafting prompts to clearly communicate our intentions to the model, guiding it to generate the most relevant and accurate responses. This approach helps optimize the model's performance, especially in specialized fields.<br>(Some tips for improving performance using prompt engineering can be found here https://www.promptingguide.ai/introduction/tips.)\n",
    "\n",
    "<u>**Implementing Personas in Prompts**</u>\n",
    "\n",
    "An interesting approach within prompt engineering involves assigning a **persona** to the model. By doing this, we can guide the model to produce responses that align with a specific character, making the interaction more tailored, in-depth, and relevant to our needs. The example below demonstrates a way to achieve persona prompting.\n",
    "\n",
    "<dt><b>Persona</b></dt>\n",
    "<dd><code>You are a highly experienced and knowledgeable cybersecurity expert with a deep understanding of cyber threats, network defense strategies, and the latest in cybersecurity technology. Your communication is clear, concise, and authoritative, aiming to educate and inform on best practices for digital security.</code></dd>\n",
    "<dt><b>Query</b></dt>\n",
    "<dd><code>How can one determine if a CVE is vulnerable in a specific environment?</code></dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "95010654-9f3e-4cb1-bb31-d4ceb288c9da",
   "metadata": {
    "tags": []
   },
   "source": [
    "security_expert_persona = (\n",
    "    \"You are a highly experienced and knowledgeable cybersecurity expert with a deep understanding of cyber threats, \"\n",
    "    \"network defense strategies, and the latest in cybersecurity technology. Your communication is clear, concise, and authoritative, \"\n",
    "    \"aiming to educate and inform on best practices for digital security.\")\n",
    "formatted_prompt = \"{persona} {query}\".format(\n",
    "    persona=security_expert_persona, query=\"How can one determine if a CVE is vulnerable in a specific environment?\")\n",
    "\n",
    "completion = llm_client.chat.completions.create(\n",
    "  model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "  messages=[{\"role\":\"user\",\"content\":formatted_prompt}], \n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    "    \n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1035ba69-8a51-4d92-950f-d1b4e420cd70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2.1 - Explore on your own: Different Personas\n",
    "\n",
    "Does the persona improve performance? What happens if you change the persona or attributes such as communication style?\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4f6ebc5-a1ff-4e27-8083-8f9757fcb6ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# # UNCOMMENT to try a different persona\n",
    "# # Here is an example:\n",
    "# persona = \"You are an elementary school teacher who teaches digital security.\",\n",
    "# formatted_prompt = \"{persona} {query}\".format(\n",
    "#     persona=persona, query=\"How can one determine if a CVE is vulnerable in a specific environment?\")\n",
    "\n",
    "# completion = llm_client.chat.completions.create(\n",
    "#   model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "#   messages=[{\"role\":\"user\",\"content\":formatted_prompt}], \n",
    "#   temperature=0.5,\n",
    "#   top_p=1,\n",
    "#   max_tokens=1024,\n",
    "#   stream=True\n",
    "    \n",
    "# )\n",
    "\n",
    "# for chunk in completion:\n",
    "#     if chunk.choices[0].delta.content is not None:\n",
    "#         print(chunk.choices[0].delta.content, end=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b265febf-5117-4010-a879-dc41ab957c5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### 1.3 - Prompt Templating\n",
    "\n",
    "While cyber knowledge assistants are valuable, there are occasions when we need more **detailed information** or support on particular subjects, such as specific **malware** or a **security vulnerability** we're examining.<br>\n",
    "For instance, if we're assessing whether a known vulnerability can be exploited in our systems, how can we leverage LLM to guide us through the process?\n",
    "Can the LLM provide us with clear instructions on what steps to take?\n",
    "\n",
    "<dl>\n",
    "<dt><b>Use Case</b></dt>\n",
    "<dd>Utilizing LLMs to Evaluate System Vulnerabilities to Specific CVEs</dd>\n",
    "<dt><b>Query</b></dt>\n",
    "<dd><code>How can I determine if my specific environment is affected by CVE-2023-47248?</code></dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "402c784d-6ae2-4468-b121-b820879aa36f",
   "metadata": {
    "tags": []
   },
   "source": [
    "formatted_prompt = \"{persona} {query}\".format(\n",
    "    persona=\"You are helpful cybersecurity expert with an IQ of 140.\",\n",
    "    query=\"How can I determine if my specific environment is affected by CVE-2023-47248?\")\n",
    "\n",
    "completion = llm_client.chat.completions.create(\n",
    "  model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "  messages=[{\"role\":\"user\",\"content\":formatted_prompt}], \n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    "    \n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3bf4e69-e720-4d49-b203-0f4a38d97f13",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "**<u>Observation</u>**<br>\n",
    "Although the LLM offered general cybersecurity guidance, it couldn't give specific details about the CVE due to the limited capability caused by its offline nature.\n",
    "\n",
    "**<u>Potential Solution</u>**<br>\n",
    "To enhance the model's effectiveness, we can <b>directly include specific details about the CVE in our prompts</b>. This approach leverages the model's ability to analyze information and compensates for its inability to access real-time data.<br>\n",
    "By doing so, the model can provide more precise and helpful recommendations concerning particular issues.\n",
    "\n",
    "**<u>CVE Intel Examples</u>**<br>\n",
    "Here are two example CVEs that we'll be using as recurring examples throughout the notebook (the information is sourced  from the internet): "
   ]
  },
  {
   "cell_type": "code",
   "id": "70451746",
   "metadata": {
    "tags": []
   },
   "source": [
    "PYARROW_CVE_INTEL = dict(\n",
    "    cve=\"CVE-2023-47248\",\n",
    "    cve_description=\n",
    "    \"Deserialization of untrusted data in IPC and Parquet readers in PyArrow before version 14.0.0 allows arbitrary code execution. It is recommended \\\n",
    "that users of PyArrow upgrade to 14.0.1. Similarly, it is recommended that downstream libraries upgrade their dependency requirements to PyArrow 14.0.1 or later. PyPI \\\n",
    "packages are already available, and we hope that conda-forge packages will be available soon. If it is not possible to upgrade, we provide a separate package `pyarrow-hotfix` \\\n",
    "for you to import to your codebase. This fix disables the vulnerability on older PyArrow versions. See https://pypi.org/project/pyarrow-hotfix/ for importing instructions.\",\n",
    "    vuln_package=\"PyArrow\",\n",
    "    vuln_package_version=\"before 14.0.1\",\n",
    "    cvss3=\"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\",\n",
    ")\n",
    "\n",
    "LOG4J_CVE_INTEL = dict(\n",
    "    cve=\"CVE-2021-44228\",\n",
    "    cve_description=\n",
    "    \"Apache Log4j2 2.0-beta9 through 2.15.0 (excluding security releases 2.12.2, 2.12.3, and 2.3.1) JNDI features used in \"\n",
    "    \"configuration, log messages, and parameters do not protect against attacker controlled LDAP and other JNDI related endpoints. An attacker \"\n",
    "    \"who can control log messages or log message parameters can execute arbitrary code loaded from LDAP servers when message lookup substitution \"\n",
    "    \"is enabled. From log4j 2.15.0, this behavior has been disabled by default. From version 2.16.0 (along with 2.12.2, 2.12.3, and 2.3.1), this \"\n",
    "    \"functionality has been completely removed. Note that this vulnerability is specific to log4j-core and does not affect log4net, log4cxx, or \"\n",
    "    \"other Apache Logging Services projects.\",\n",
    "    vuln_package=\"log4j\",\n",
    "    vuln_package_version=\n",
    "    \"from 2.0.1 up to (excluding) 2.3.1, from 2.4.0 up to (excluding) 2.12.2, from 2.13.0 up to (excluding) 2.15.0\",\n",
    "    cvss3=\"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3ad0c8e",
   "metadata": {},
   "source": [
    "Below is an example illustrating how to create a prompt template that allows us to easily insert information of any given CVE:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f799d938-0761-40f1-9007-05d900609bef",
   "metadata": {
    "tags": []
   },
   "source": [
    "prompt_template = \"\"\"Generate a checklist for a security analyst to use when assessing the exploitability of a specific CVE within a containerized environment. \\\n",
    "For each checklist item, start with an action verb, making it clear and actionable. Provide the checklist as a Python list of strings. \\\n",
    "Utilize the provided CVE details below to tailor the checklist items specifically for this CVE.\n",
    "CVE Details:\n",
    "- CVE ID: {cve}\n",
    "- Description: {cve_description}\n",
    "- Vulnerable Package Name: {vuln_package}\n",
    "- Vulnerable Package Version: {vuln_package_version}\n",
    "- CVSS3 Vector String: {cvss3}\"\"\"\n",
    "\n",
    "formatted_prompt = prompt_template.format(**PYARROW_CVE_INTEL)\n",
    "\n",
    "completion = llm_client.chat.completions.create(\n",
    "  model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "  messages=[{\"role\":\"user\",\"content\":formatted_prompt}],\n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    "  stream=False\n",
    "    \n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f374c4a4-67b1-4dad-92c8-ca74995af35c",
   "metadata": {},
   "source": [
    "#### 1.3.1 - Reflective Questions\n",
    "\n",
    "<dl>\n",
    "<dt><b>1. Assessing the Model's Output</b></dt>\n",
    "<dd>⦁ Review the checklist provided by the model. Do the steps outlined seem practical and relevant to the CVE?</dd>\n",
    "<dd>⦁ How does the generated checklist aligns with your expectations?</dd><br>\n",
    "<dt><b>2. Checking for Format Compliance</b></dt>\n",
    "<dd>⦁ Did the model generate the output in the format we requested, specifically as a Python list of strings?</dd>\n",
    "<dd>⦁ Consider the importance of format in data pipelines and how it affects the usability of the model's output.</dd><br>\n",
    "<dt><b>3. Measuring Accuracy</b></dt>\n",
    "<dd>⦁ How can we determine the accuracy of a language model's output?</dd>\n",
    "<dd>⦁ Think about the criteria you would use to evaluate whether the checklist is accurate and relevant to the CVE details provided.</dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24f441-2997-45b4-a6d7-4de574716623",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.3.2 - Explore On Your Own: Different Models\n",
    "\n",
    "How do alternative models perform? Do any adhere more closely to the formatting instructions?"
   ]
  },
  {
   "cell_type": "code",
   "id": "bcbbb288-8aa6-41ea-ace5-292c8d288d47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# # UNCOMMENT to try different models\n",
    "# models_to_try = [\"meta/llama3-70b-instruct\", \"meta/llama3-8b-instruct\"] #Other models go here\n",
    "\n",
    "# for model in models_to_try:\n",
    "#     print(f\"\\n----\\nModel: {model}\")\n",
    "#     completion = llm_client.chat.completions.create(\n",
    "#       model=model,\n",
    "#       messages=[{\"role\":\"user\",\"content\":formatted_prompt}],\n",
    "#       temperature=0.5,\n",
    "#       top_p=1,\n",
    "#       max_tokens=1024,\n",
    "#       stream=True\n",
    "\n",
    "#     )\n",
    "\n",
    "#     for chunk in completion:\n",
    "#         if chunk.choices[0].delta.content is not None:\n",
    "#             print(chunk.choices[0].delta.content, end=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "945a3b2b-d397-4f85-8c79-894565ecad7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.3.3 - Evaluating Model Performance through Formatting Checks\n",
    "\n",
    "One way to to assess the model's performance is by checking its ability to adhere to our formatting instructions to output a python list.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "857dfe77-bc9c-4ca7-b5d2-ece4cf18bd09",
   "metadata": {
    "tags": []
   },
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "# we can evaluate if the checklist is properly formatted using this function\n",
    "def is_properly_formatted_list(checklist):\n",
    "    try:\n",
    "        # Attempt to evaluate checklist as a Python literal\n",
    "        evaluated_checklist = ast.literal_eval(checklist)\n",
    "\n",
    "        # Check if the evaluated object is a list\n",
    "        if isinstance(evaluated_checklist, list):\n",
    "            print(\"Checklist is properly formatted.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Checklist is not a list.\")\n",
    "            return False\n",
    "    except ValueError as e:\n",
    "        # Handle the case where checklist cannot be evaluated as a Python literal\n",
    "        print(f\"Checklist is not properly formatted: {e}\")\n",
    "        return False\n",
    "    except SyntaxError as e:\n",
    "        # Handle syntax errors in the checklist string\n",
    "        print(f\"Checklist has a syntax error: {e}\")\n",
    "        return False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cfb6596f-a607-4aff-a44c-90acb147dabf",
   "metadata": {
    "tags": []
   },
   "source": [
    "is_properly_formatted_list(\n",
    "    llm_client.chat.completions.create(\n",
    "      model=\"meta/llama3-70b-instruct\",\n",
    "      messages=[{\"role\":\"user\",\"content\":formatted_prompt}], #Prompt goes here\n",
    "      temperature=0.5,\n",
    "      top_p=1,\n",
    "      max_tokens=1024,\n",
    "      stream=False\n",
    ").choices[0].message.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0933b63-13bb-4e57-8cf1-cb0aab3f43da",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "If the model's output doesn't meet our expectations, what are our next steps?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3686239-80aa-49a1-91f7-5df78fb72d25",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr>\n",
    "\n",
    "### 1.4 - One-Shot Learning\n",
    "\n",
    "Our prevoius examples illustrated zero-shot learning or direct prompting, where the LLM was simply given instructions and asked to follow them. The process of adding one example to the prompt or **one-shot learning** can often greatly improve performance as it is more difficult to describe the desired output than it is to show it. It's as straight forward as it sounds, add a single example of the desired output to the prompt. Let's try it.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b58c9b36-b074-41e9-8177-a243f75a9ec1",
   "metadata": {
    "tags": []
   },
   "source": [
    "unparsable_list = \"\"\"- Check if the vulnerable package, PyArrow, is installed in the container.\n",
    "- If the vulnerable package is installed, check the version of the package. If it is before 14.0.1, the vulnerability is present.\n",
    "- Check if the container has any exposed IPC or Parquet readers.\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60d44721-9d7c-4c5c-9c16-05f11a8cf8ee",
   "metadata": {},
   "source": [
    "Zero-Shot Example\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c1a8f41-5e33-49bc-aeec-7d32471f46b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "zero_shot_template = \"\"\"Parse the following checklist's contents into a python list.\n",
    "Checklist:\n",
    "{checklist}\n",
    "\n",
    "Only provide the list as a resonse.\"\"\"\n",
    "\n",
    "formatted_prompt = zero_shot_template.format(checklist=unparsable_list)\n",
    "\n",
    "model_output = llm_client.chat.completions.create(\n",
    "      model=\"meta/llama3-70b-instruct\",\n",
    "      messages=[{\"role\":\"user\",\"content\":formatted_prompt}], \n",
    "      temperature=0.5,\n",
    "      top_p=1,\n",
    "      max_tokens=1024,\n",
    "      stream=False\n",
    ").choices[0].message.content\n",
    "\n",
    "print(model_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3872ee0-cbb1-42b7-8194-c457ee606ff5",
   "metadata": {
    "tags": []
   },
   "source": [
    "is_properly_formatted_list(model_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5d2c0ce8-6a2c-4f23-bad4-5d55731da199",
   "metadata": {},
   "source": [
    "One-shot Example\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5eda499-e858-4114-9900-01d086ccef55",
   "metadata": {
    "tags": []
   },
   "source": [
    "one_shot_template = \"\"\"Using the example as a guide, parse the checklist's contents into a python list.\n",
    "Example Checklist:\n",
    "- Check for notable vulnerable software vendors\n",
    "- Consider the network exposure of your Docker container\n",
    "\n",
    "Example Output:\n",
    "[\"Check for notable vulnerable software vendors\", \"Consider the network exposure of your Docker container\"]\n",
    "\n",
    "Given Checklist:\n",
    "{checklist}\n",
    "\n",
    "Output Python List. Only provide the list as a response: \"\"\"\n",
    "\n",
    "formatted_prompt = one_shot_template.format(checklist=unparsable_list)\n",
    "\n",
    "model_output = llm_client.chat.completions.create(\n",
    "      model=\"meta/llama3-70b-instruct\",\n",
    "      messages=[{\"role\":\"user\",\"content\":formatted_prompt}],\n",
    "      temperature=0.5,\n",
    "      top_p=1,\n",
    "      max_tokens=1024,\n",
    "      stream=False\n",
    ").choices[0].message.content\n",
    "\n",
    "print(model_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6299d06-e278-4372-8100-7d5f111c59cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "is_properly_formatted_list(model_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1fa97813-b21c-4445-8f8a-b99a03e03267",
   "metadata": {},
   "source": [
    "#### 1.4.1 - Explore on your own: Robustness\n",
    "\n",
    "- How robust is the one-shot example?\n",
    "\n",
    "- Is it effective when the unparsable list is enumerated or bulleted instead of using dashes?\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "570d2da8-b30b-47e7-895e-0f9fde2acb08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# # UNCOMMENT to try with an enumerated list\n",
    "# enumerated_list = \"\"\"1. Check if the vulnerable package, PyArrow, is installed in the container.\n",
    "# 2. If the vulnerable package is installed, check the version of the package. If it is before 14.0.1, the vulnerability is present.\n",
    "# 3. Check if the container has any exposed IPC or Parquet readers.\"\"\"\n",
    "\n",
    "# formatted_prompt = one_shot_template.format(checklist=unparsable_list)\n",
    "\n",
    "# model_output = llm_client.chat.completions.create(\n",
    "#       model=\"meta/llama3-70b-instruct\",\n",
    "#       messages=[{\"role\":\"user\",\"content\":formatted_prompt}], #Prompt goes here\n",
    "#       temperature=0.5,\n",
    "#       top_p=1,\n",
    "#       max_tokens=1024,\n",
    "#       stream=False\n",
    "# ).choices[0].message.content\n",
    "# print(model_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bca366ae-4557-4b9f-be6c-1aacb5e0c52f",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08bba90-3201-4c95-b8c4-6f4d2eb0f3a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.5 - Few-Shot Learning and Detailed Prompts\n",
    "\n",
    "Prompts can be extended to be quite large, descriptive and include many examples. Below is an example of very detailed prompt that contains all the elements discussed above and more. What extra elements do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "62834ec0-ef09-4462-88e1-851066dd45cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "few_shot_prompt_template = \"\"\"You are an expert security analyst. Your objective is to add a \"Checklist\" section containing steps to use when assessing the exploitability of a specific CVE within a containerized environment. \\\n",
    "For each checklist item, start with an action verb, making it clear and actionable\n",
    "\n",
    "**Context**:\n",
    "Not all CVEs are exploitable in a given container. By making a checklist specific to the information available for a given CVE analysts can execute the checklist to determine exploitability.\n",
    "\n",
    "**Example Format**:\n",
    "Below is a format for examples that illustrate transforming CVE information into an exploitability assessment checklist.\n",
    "\n",
    "Example 1 CVE Details:\n",
    "- CVE ID: CVE-2022-2309\n",
    "- Description: NULL Pointer Dereference allows attackers to cause a denial of service (or application crash). This only applies when lxml up to version 4.9.1 \\\n",
    "is used together with libxml2 2.9.10 through 2.9.14. libxml2 2.9.9 and earlier are not affected. It allows triggering crashes through forged input data, given a \\\n",
    "vulnerable code sequence in the application. The vulnerability is caused by the iterwalk function (also used by the canonicalize function). Such code shouldn't be \\\n",
    "in wide-spread use, given that parsing + iterwalk would usually be replaced with the more efficient iterparse function. However, an XML converter that serialises to \\\n",
    "C14N would also be vulnerable, for example, and there are legitimate use cases for this code sequence. If untrusted input is received (also remotely) and processed via \\\n",
    "iterwalk function, a crash can be triggered.\n",
    "- Vulnerable Package Name: lxml, libxml2\n",
    "- Vulnerable Package Version: lxml: up to 4.9.1, libxml2: 2.91.0 through 2.9.14\n",
    "- CVSS3 Vector String: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H\n",
    "\n",
    "Example 1 Exploitability Assessment Checklist:\n",
    "[\n",
    "\"Check for lxml: Verify if your project uses the lxml library, which is the affected package. If lxml is not a dependency in your project, then your code is not vulnerable to this CVE.\",\n",
    "\"Review Affected Versions: If lxml is used, checked the version that your project depends on. According to the vulnerability details, versions 4.9.0 and earlier are vulnerable.\",\n",
    "\"Review Versions of Connected Dependencies: The package is only vulnerable if libxml 2.9.10 through 2.9.14 is also present. Check the version of libxml in the project.\",\n",
    "\"Check for use of vulnerable functions: The library is vulnerable through its `iterwalk` function, which is also utilized by the `canonicalize` function. Check if either of these functions are used in your code base.\"\n",
    "]\n",
    "\n",
    "Example 2 CVE Details:\n",
    "- CVE ID: CVE-2024-23334\n",
    "- Description: aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. When using aiohttp as a web server and configuring static routes, \\\n",
    "it is necessary to specify the root path for static files. Additionally, the option 'follow_symlinks' can be used to determine whether to follow symbolic links \\\n",
    "outside the static root directory. When 'follow_symlinks' is set to True, there is no validation to check if reading a file is within the root directory. This can \\\n",
    "lead to directory traversal vulnerabilities, resulting in unauthorized access to arbitrary files on the system, even when symlinks are not present. \\\n",
    "Disabling `follow_symlinks` by setting `follow_symlinks = False` and using a reverse proxy are encouraged mitigations. Version 3.9.2 fixes this issue.\n",
    "- Vulnerable Package Name: aiohttp\n",
    "- Vulnerable Package Version: from 1.0.5 up to (excluding) 3.9.2\n",
    "- CVSS3 Vector String: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N\n",
    "\n",
    "Example 2 Exploitability Assessment Checklist:\n",
    "[\n",
    "    \"Check for aiohttp: Verify if your project uses the aiohttp library, which is the affected package. If aiohttp is not a dependency in your project, then your code is not vulnerable to this CVE.\",\n",
    "    \"Review Affected Versions: If aiohttp is used, check the version that your project depends on. According to the vulnerability details, versions from 1.0.5 up to (excluding) 3.9.2 are affected by this vulnerability.\",\n",
    "    \"Review Code To Check for Vulnerability Mitigation: Check if the 'follow_symlinks' option is set to False to mitigate the risk of directory traversal vulnerabilities.\"\n",
    "]\n",
    "\n",
    "**Criteria**:\n",
    "- Exploitability assessment checklists must relate to the information in the specific CVE Details.\n",
    "- Exploitability assessment checklists must include checks for mitigating conditions when present in the CVE Details.\n",
    "\n",
    "**Procedure**:\n",
    "[\n",
    "\"Understand the CVE Details, description, and CVSS3 attack vector string.\",\n",
    "\"Produce a CVE exploitability assessment checklist.\",\n",
    "\"Format the checklist as comma separated list surrounded by square braces.\",\n",
    "\"Output the checklist.\"\n",
    "]\n",
    "\n",
    "**CVE Details:**\n",
    "- CVE ID: {cve}\n",
    "- Description: {cve_description}\n",
    "- Vulnerable Package Name: {vuln_package}\n",
    "- Vulnerable Package Version: {vuln_package_version}\n",
    "- CVSS3 Vector String: {cvss3}\n",
    "\n",
    "**Checklist**: \n",
    "\n",
    "Please only provide the list as output.\"\"\"\n",
    "\n",
    "formatted_prompt = few_shot_prompt_template.format(**PYARROW_CVE_INTEL)\n",
    "\n",
    "model_output = llm_client.chat.completions.create(\n",
    "      model=\"meta/llama3-70b-instruct\",\n",
    "      messages=[{\"role\":\"user\",\"content\":formatted_prompt}],\n",
    "      temperature=0.5,\n",
    "      top_p=1,\n",
    "      max_tokens=1024,\n",
    "      stream=False\n",
    ").choices[0].message.content\n",
    "\n",
    "print(model_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b14a3924-eb83-4ce4-91d3-e9fb7f9648b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "is_properly_formatted_list(model_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6d50410-fe09-4569-8aab-e72e4a808bc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.5.1 - Explore on your own\n",
    "- What feedback could an expert cyber analyst give you about this output?\n",
    "\n",
    "\n",
    "- What happens when you take a checklist item from what the model generated above and ask the model about it?"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8fcbfb2-e3ca-424c-9575-db61be58bb72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# # UNCOMMENT to try it out\n",
    "# example_checklist_item = (\n",
    "#     \"Check for PyArrow: Verify if your project uses the PyArrow library, which is the affected package. \"\n",
    "#     \"If PyArrow is not a dependency in your project, then your code is not vulnerable to this CVE.\"\n",
    "# )\n",
    "# print(llm_client.chat.completions.create(\n",
    "#       model=\"meta/llama3-70b-instruct\",\n",
    "#       messages=[{\"role\":\"user\",\"content\":example_checklist_item}], #Prompt goes here\n",
    "#       temperature=0.5,\n",
    "#       top_p=1,\n",
    "#       max_tokens=1024,\n",
    "#       stream=False\n",
    "# ).choices[0].message.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14d4d912-a7b0-4def-b823-0512beb770c7",
   "metadata": {},
   "source": [
    "- How many examples do you think could fit in a prompt before the context is too large for the model?"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f0b6274-95da-47d4-b1f9-fa30a87bd339",
   "metadata": {
    "tags": []
   },
   "source": [
    "# # UNCOMMENT to try it out\n",
    "# # Let's try repeating the first example 14 times\n",
    "# example_one_start_index = 626\n",
    "# example_one_end_index = 2527\n",
    "\n",
    "# def repeat_substring(main_str, substring_start_index, substring_end_index, n):\n",
    "#     # Divide the main_str into before and after parts\n",
    "#     sub_str = main_str[substring_start_index:substring_end_index]\n",
    "#     before_part = main_str[:substring_start_index]\n",
    "#     after_part = main_str[substring_end_index:]\n",
    "\n",
    "#     # Repeat sub_str n times\n",
    "#     repeated_sub_str = sub_str * n\n",
    "\n",
    "#     # Concatenate before part, repeated sub_str, and after part\n",
    "#     result = before_part + repeated_sub_str + after_part\n",
    "#     return result\n",
    "\n",
    "# updated_prompt = repeat_substring(few_shot_prompt_template, example_one_start_index, example_one_end_index, 14)\n",
    "\n",
    "# model_output = llm_client.chat.completions.create(\n",
    "#       model=\"meta/llama3-8b-instruct\",\n",
    "#       messages=[{\"role\":\"user\",\"content\":updated_prompt}], #Prompt goes here\n",
    "#       temperature=0.5,\n",
    "#       top_p=1,\n",
    "#       max_tokens=1024,\n",
    "#       stream=False\n",
    "# ).choices[0].message.content\n",
    "# print(model_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86b93294",
   "metadata": {},
   "source": [
    "Do you see an error indicating that the context is too long for the model to handle?<br>\n",
    "Try different values of `n` to see where the boundary of context length lies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228cdd9-efd8-4d59-8320-c462a4a6552d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### 1.6 - Evaluation Strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48198ef0-ec03-43af-a050-bfc9fa4f0ff0",
   "metadata": {},
   "source": [
    "**A Note On Evaluation Strategies**\n",
    "\n",
    "Evaluating model performance on desired metrics such as **creates a properly formatted list** is relatively straightforward and traditional accuracy measurements (ie. properly formatted outputs/total outputs) can be used.\n",
    "\n",
    "For evaluating more subjective outcomes such as **completeness of the checklist** there are other strategies that can be explored for task-specific LLMs.\n",
    "\n",
    "During this initial experimental stage, it makes sense to have expert humans review outputs to determine the model's performance. A common pattern that emerges when developing and evaluating cybersecurity use cases around LLMs is as follows:\n",
    "\n",
    "1. Experiment using a few golden examples to determine feasibility, and evaluate candidate models and prompts by hand\n",
    "2. Collect feedback on initial model outputs from experts and use this feedback to create a larger dataset\n",
    "3. Use the newly created larger dataset from experts to create use-case-specific training and benchmark datasets\n",
    "\n",
    "Since getting these initial results into the hands of experts for evaluation is oftentimes a crucial component for obtaining a larger benchmark dataset, we will focus on quickly and easily building out the end-to-end pipeline for this use case example.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afdc87a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Please wait here until instructed to continue with running the notebook.\n",
    "</div>\n",
    "\n",
    "## 2 - Prototyping\n",
    "\n",
    "Now that we have the task generation for this workflow ready, how can we automate getting the answers for our checklist items?\n",
    "\n",
    "### 2.1 - Overview\n",
    "\n",
    "It is possible to build a language model-based system that accesses external knowledge sources to complete tasks. In [Section 1.3](#1.3---Prompt-Templating), we added additional CVE details into the prompt by hand. While this strategy can be effective for adding additional context for very specific items like CVE Details, it requires a priori knowledge of what details to include (like those from NVD). When you would like to help your LLM with its query by adding more context in real-time, you're ready for RAG (Retrieval Augmented Generation).\n",
    "\n",
    "When a query or checklist item is posed to an LLM equipped with RAG, the model first consults the vector database to find relevant information related to the query. This retrieved data is then combined with the original question and fed back into the LLM. With this enriched context, the LLM can generate a more accurate and informed response, potentially including evidence or reasoning based on the newly incorporated data. This approach not only improves the quality of the LLM's outputs but also gives our tool access to project- and container-specific information to determine CVE exploitability.\n",
    "\n",
    "### 2.2 - Building the Vector Database\n",
    "\n",
    "In addition to having a query and LLM, RAG requires additional information to be stored in a vector database. One mechanism of finding the proper information from the database is to first embed the query into the same vector space and retrieve the top most similar items via a distance metric. The additional information is then presented in the prompt of the LLM. The neighboring vectors in the database are said to be \"semantically similar\" to the query and likely relevant.\n",
    "\n",
    "For our demonstration purposes, we would like our LLM to be able to access the code repository of the project we're interested in checking for exploitable CVEs. The first step is transforming the specific repo into a vector database. Before that, lets pull a shallow clone of the `Morpheus 24.03` branch from GitHub and use that as the codebase for this example. We'll also set up a logging directory for the Morpheus LLM Client logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "45ec2c86-8de5-497e-9fb9-7ddfd27f0d13",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%capture\n",
    "\n",
    "! git clone --depth 1 -b branch-24.03 https://github.com/nv-morpheus/Morpheus.git\n",
    "! mkdir /root/.cache/morpheus \n",
    "! mkdir /root/.cache/morpheus/log"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0c5175d",
   "metadata": {
    "tags": []
   },
   "source": [
    "from cyber_dev_day.embeddings import create_code_embedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Create the embedding object that will be used to generate the embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "                                   model_kwargs={\"device\": \"cuda\"},\n",
    "                                   show_progress=True)\n",
    "\n",
    "# Create a vector database of the code using the supplied embedding function. The returned value will be a\n",
    "# FaissVectorDatabase object.\n",
    "# NOTE: This may take a few minutes to run.\n",
    "faiss_vdb = create_code_embedding(code_dir=os.path.join(os.getenv(\"MORPHEUS_ROOT\"), \"notebooks\", \"Morpheus\"),\n",
    "                                  embedding=embeddings,\n",
    "                                  include_notebooks=False,\n",
    "                                  exclude=[\".cache/**/*.py\", \"build*/**/*.py\"])\n",
    "\n",
    "# Save the vector database to disk\n",
    "code_faiss_dir = os.path.join(os.getenv(\"MORPHEUS_ROOT\"), \".tmp\", \"morpheus_code_faiss\")\n",
    "\n",
    "faiss_vdb.save_local(code_faiss_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb0cd628",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Please wait here until instructed to continue with running the notebook.\n",
    "</div>\n",
    "\n",
    "### 2.3 - Running a RAG Pipeline with Morpheus\n",
    "\n",
    "Now that we have built a vector database to provide external knowledge for the LLM, we need to make a tool that can query the vector database, add the information to the prompt, and execute the LLM query. There are many tools out there that can perform this task, but in this lab, we will be using NVIDIA Morpheus.\n",
    "\n",
    "#### 2.3.1 - Morpheus Overview\n",
    "\n",
    "NVIDIA Morpheus is an open AI application framework that aids cybersecurity experts in building high-performance pipelines for cybersecurity workflows. Morpheus is well suited for building a RAG pipeline due to its LLM Engine, which is specifically designed to aid in integrating LLMs into high throughput and low latency pipelines. A complete guide covering Morpheus is beyond the scope of this notebook but more information on Morpheus can be found at the following locations:\n",
    "\n",
    "- Documentation: https://docs.nvidia.com/morpheus/index.html\n",
    "- Github Repo: https://github.com/nv-morpheus/Morpheus\n",
    "- Morpheus Examples: https://github.com/nv-morpheus/Morpheus/tree/branch-24.03/examples\n",
    "\n",
    "To start building a Morpheus pipeline, the first step is always to create a configuration object. The configuration object controls global options for the pipeline such as batch size, number of threads, logging, and more. For our needs, we can use the default values and only need to create the object which we will be passing to each pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec9cbc52",
   "metadata": {
    "tags": []
   },
   "source": [
    "from morpheus.config import Config\n",
    "from morpheus.config import PipelineModes\n",
    "\n",
    "# Create the pipeline config\n",
    "pipeline_config = Config()\n",
    "pipeline_config.mode = PipelineModes.OTHER"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7829100",
   "metadata": {},
   "source": [
    "#### 2.3.2 - Building a Morpheus RAG Pipeline\n",
    "\n",
    "Below, we will build a pipeline that uses Morpheus to answer questions about the code in the repository that we created a vector database for. This works by using the `LLMEngine` in Morpheus with a `RAGNode`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3833deda",
   "metadata": {
    "tags": []
   },
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "from cyber_dev_day.config import EngineConfig\n",
    "from cyber_dev_day.config import LLMModelConfig\n",
    "from cyber_dev_day.config import NVFoundationLLMModelConfig\n",
    "from cyber_dev_day.nim_llm_service import NIMLLMService\n",
    "from cyber_dev_day import config\n",
    "from cyber_dev_day.llm_service import LLMService \n",
    "from cyber_dev_day.faiss_vdb_service import FaissVectorDBService\n",
    "\n",
    "from morpheus._lib.llm import LLMEngine\n",
    "from morpheus.llm.nodes.extracter_node import ExtracterNode\n",
    "from morpheus.llm.nodes.rag_node import RAGNode\n",
    "from morpheus.llm.task_handlers.simple_task_handler import SimpleTaskHandler\n",
    "from morpheus.messages import ControlMessage\n",
    "from morpheus.pipeline.linear_pipeline import LinearPipeline\n",
    "from morpheus.stages.input.in_memory_source_stage import InMemorySourceStage\n",
    "from morpheus.stages.llm.llm_engine_stage import LLMEngineStage\n",
    "from morpheus.stages.output.in_memory_sink_stage import InMemorySinkStage\n",
    "from morpheus.stages.preprocess.deserialize_stage import DeserializeStage\n",
    "from morpheus.utils.concat_df import concat_dataframes\n",
    "\n",
    "\n",
    "def _build_rag_llm_engine(model_config: LLMModelConfig):\n",
    "\n",
    "    engine = LLMEngine()\n",
    "\n",
    "    # Create an extracter node to pull the specified input from the DataFrame\n",
    "    engine.add_node(\"extracter\", node=ExtracterNode())\n",
    "\n",
    "    prompt = dedent(\"\"\"\n",
    "    You are a helpful assistant. Given the following background information:\n",
    "    {% for c in contexts -%}\n",
    "    Source File: {{ c.metadata.source }}\n",
    "    Source File Language: {{ c.metadata.language }}\n",
    "    Source Content:\n",
    "    ```\n",
    "    {{ c.page_content }}\n",
    "    ```\n",
    "    {% endfor %}\n",
    "\n",
    "    Please answer the following question:\n",
    "    {{ query }}\n",
    "    \"\"\").strip(\"\\n\")\n",
    "\n",
    "    # Create a service to query the vector database we created from the python source code\n",
    "    vector_service = FaissVectorDBService(code_faiss_dir, embeddings=embeddings)\n",
    "    vdb_resource = vector_service.load_resource()\n",
    "\n",
    "    # Create an LLM service using the model configuration options in the LLM Model Config\n",
    "    llm_service = LLMService.create(model_config.service.type, **model_config.service.model_dump(exclude={\"type\"}))\n",
    "    llm_client = llm_service.get_client(**model_config.model_dump(exclude={\"service\"}))\n",
    "\n",
    "    # Async wrapper around embeddings\n",
    "    async def calc_embeddings(texts: list[str]) -> list[list[float]]:\n",
    "        return embeddings.embed_documents(texts)\n",
    "\n",
    "    # Add a RAG Node to the engine which will use the prompt, vector database, emebddings and LLM\n",
    "    engine.add_node(\"rag\",\n",
    "                    inputs=[\"/extracter\"],\n",
    "                    node=RAGNode(prompt=prompt,\n",
    "                                 vdb_service=vdb_resource,\n",
    "                                 embedding=calc_embeddings,\n",
    "                                 llm_client=llm_client))\n",
    "\n",
    "    # Final step of every LLM Engine is to turn the output data back into a Control Message for the rest of the pipeline\n",
    "    engine.add_task_handler(inputs=[\"/rag\"], handler=SimpleTaskHandler())\n",
    "\n",
    "    return engine\n",
    "\n",
    "\n",
    "# Define a function that will build and run the pipeline given a configuration and question input\n",
    "async def run_rag_pipeline(p_config: Config, model_config: LLMModelConfig, question: str):\n",
    "    source_dfs = [\n",
    "        cudf.DataFrame({\"questions\": [question]}),\n",
    "    ]\n",
    "\n",
    "    # Create a completion task to be used by the DeserializeStage. This indicates which columns to use from the\n",
    "    # dataframe\n",
    "    completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"questions\"], }}\n",
    "\n",
    "    pipe = LinearPipeline(p_config)\n",
    "\n",
    "    # Create a source object which will emit our dataframe into the pipeline\n",
    "    pipe.set_source(InMemorySourceStage(p_config, dataframes=source_dfs))\n",
    "\n",
    "    # The deserialize stage will take the dataframe and convert it into a ControlMessage object\n",
    "    pipe.add_stage(\n",
    "        DeserializeStage(p_config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=completion_task))\n",
    "\n",
    "    # Add the LLM Engine stage to the pipeline. This executes our RAG query and runs the LLM model\n",
    "    pipe.add_stage(LLMEngineStage(p_config, engine=_build_rag_llm_engine(model_config)))\n",
    "\n",
    "    # Add a sink to the pipeline to capture the output of the pipeline\n",
    "    sink = pipe.add_stage(InMemorySinkStage(p_config))\n",
    "\n",
    "    # Run the pipeline. This will complete once all messages have been processed\n",
    "    await pipe.run_async()\n",
    "\n",
    "    messages = sink.get_messages()\n",
    "    responses = concat_dataframes(messages)\n",
    "\n",
    "    # The responses are quite long, when debug is enabled disable the truncation that pandas and cudf normally\n",
    "    # perform on the output\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "    logger.info(\"Response:\\n%s\" % (responses[\"response\"].iloc[0], ))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd9f1092",
   "metadata": {
    "tags": []
   },
   "source": [
    "model_config = config.NIMModelConfig.model_validate({\n",
    "    \"service\": {\n",
    "        \"type\": \"NIM\", \"api_key\": None\n",
    "    },\n",
    "    \"base_url\": \"https://integrate.api.nvidia.com/v1\",\n",
    "    \"model_name\": \"meta/llama3-70b-instruct\",\n",
    "    \"temperature\": 0.0\n",
    "}) #API key here is an environment variable, so we don't need to specify it explicitly\n",
    "\n",
    "# Run the Pipeline\n",
    "await run_rag_pipeline(pipeline_config, model_config, \"Does the code repo import the `pyarrow_hotfix` package from the `morpheus` root package?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0380153a",
   "metadata": {},
   "source": [
    "#### 2.3.3 - RAG Limitations\n",
    "\n",
    "Using the pipeline we built, we can now ask questions about the code in the repository and the LLM will be able to use the vector database to answer them. However, what happens if we need to ask questions about code that is not in the vector database? For example, what if we needed to ask questions about the dependencies that the code uses? Would the LLM be able to answer these questions? Let's try it out by re-running our RAG pipeline with a more complex question:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "506b5fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the Pipeline\n",
    "await run_rag_pipeline(pipeline_config, model_config, \"Does the code repo use `langchain` functions which are deprecated?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4abd8482",
   "metadata": {},
   "source": [
    "It's likely that the model was not able to determine the answer to this question because it would need additional information. Depending on the model used, you might see output similar to:\n",
    "\n",
    "```\n",
    "Without further information about the `langchain` package or its documentation, it's difficult to determine if any specific functions or methods used in the code are deprecated.\n",
    "```\n",
    "\n",
    "How would we go about solving this problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb24c7",
   "metadata": {},
   "source": [
    "### 2.4 - Running the CVE Pipeline with Morpheus\n",
    "\n",
    "#### 2.4.1 - Answering Complex Questions with RAG + LLM Agents\n",
    "\n",
    "To answer a question about the existence of deprecated `langchain` functions, the model needs to look up versions of the packages in our container or project. We can add an additional knowledge source such as a Software Bill of Materials (SBOM). With multiple tools/knowledge sources- `SBOM Package Checker` and `Docker Container Code QA System` we need a new framework to allow our LLM to choose what tools it needs to use and synthesize the responses. One method we can use is [LangChain agents](https://python.langchain.com/docs/modules/agents/). \n",
    "\n",
    "An agent in this sense is an LLM that has \"agency\" to determine what sources of information it needs to retrieve to answer questions. This can be achieved through prompting. The most simplistic prompt to use to turn an LLM into an agent with tool usage might look like this:\n",
    "\n",
    "```\n",
    "You are a helpful assistant. Help the user answer any questions.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "In order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags.\n",
    "You will then get back a response in the form <observation></observation>\n",
    "When you are done, respond with a final answer between <final_answer></final_answer>. \n",
    "\n",
    "Question: {input}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896fa18",
   "metadata": {},
   "source": [
    "Ideally, with just one round of query-> tool-> observation-> final answer, the LLM will get the information it needs to answer simple queries such as `What version of PyArrow is in the repo?`\n",
    "\n",
    "But what about more complex queries such as `Does the code repo use langchain functions which are deprecated?` \n",
    "This query would require the LLM to first find what functions are deprecated before searching the code base for them. We would prompt the LLM to use a series of steps (repeated N times): Thought, Action, and Observation. This process loop of reasoning and acting is called a [ReAct Agent](https://react-lm.github.io/). In practice, it could be like this:\n",
    "\n",
    "```\n",
    "query: Does the morpheus code repo use langchain functions which are deprecated?\n",
    "> Entering new AgentExecutor chain...\n",
    "I need to check the langchain version in the container's SBOM and the deprecated source code functions.\n",
    "Action: SBOM Package Checker\n",
    "Action Input: langchain\n",
    "Observation: 0.1.12\n",
    "Thought: The langchain version in the container is 0.1.12.\n",
    "Thought: I need to check the langchain source code for deprecated functions.\n",
    "Action: Docker Container Code QA System\n",
    "Action Input: Does the repo use the format_tool_to_openai_function or __call__ from langchain?\n",
    "Observation: No, the repo does not use format_tool_to_openai_function or __call__ from langchain.\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: The morpheus code repo does not use langchain functions which are deprecated.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f48d3",
   "metadata": {},
   "source": [
    "How can we incorporate these powerful ReAct agents and their tools into an end-to-end pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdcfb0",
   "metadata": {},
   "source": [
    "#### 2.4.2 - The Morpheus CVE Pipeline\n",
    "\n",
    "To convert our RAG pipeline into a CVE pipeline, all we need to do is update the LLM engine to run the CVE steps instead of a single RAG node as before."
   ]
  },
  {
   "cell_type": "code",
   "id": "a16bf979",
   "metadata": {
    "tags": []
   },
   "source": [
    "from cyber_dev_day.pipeline_utils import build_cve_llm_engine\n",
    "\n",
    "from morpheus.messages import ControlMessage\n",
    "from morpheus.pipeline.linear_pipeline import LinearPipeline\n",
    "from morpheus.stages.input.in_memory_source_stage import InMemorySourceStage\n",
    "from morpheus.stages.llm.llm_engine_stage import LLMEngineStage\n",
    "from morpheus.stages.output.in_memory_sink_stage import InMemorySinkStage\n",
    "from morpheus.stages.preprocess.deserialize_stage import DeserializeStage\n",
    "from morpheus.utils.concat_df import concat_dataframes\n",
    "\n",
    "\n",
    "async def run_cve_pipeline(p_config: Config, e_config: EngineConfig, input_cves: list[str], retry_bad_input = True):\n",
    "    source_dfs = [cudf.DataFrame({\"cve_info\": input_cves})]\n",
    "\n",
    "    # Create a completion task to be used by the DeserializeStage. This indicates which columns to use from the\n",
    "    # dataframe\n",
    "    completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"cve_info\"], }}\n",
    "\n",
    "    pipe = LinearPipeline(p_config)\n",
    "\n",
    "    # Create a source object which will emit our dataframe into the pipeline\n",
    "    pipe.set_source(InMemorySourceStage(p_config, dataframes=source_dfs))\n",
    "\n",
    "    # The deserialize stage will take the dataframe and convert it into a ControlMessage object\n",
    "    pipe.add_stage(\n",
    "        DeserializeStage(p_config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=completion_task))\n",
    "\n",
    "    # Add the LLM Engine stage to the pipeline. This executes our CVE workflow and runs the LLM model\n",
    "    pipe.add_stage(LLMEngineStage(p_config, engine=build_cve_llm_engine(e_config, retry_bad_input)))\n",
    "\n",
    "    # Add a sink to the pipeline to capture the output of the pipeline\n",
    "    sink = pipe.add_stage(InMemorySinkStage(p_config))\n",
    "\n",
    "    # Run the pipeline. This will complete once all messages have been processed\n",
    "    await pipe.run_async()\n",
    "\n",
    "    messages = sink.get_messages()\n",
    "    responses = concat_dataframes(messages)\n",
    "\n",
    "    logger.info(\"Received %s responses:\\n%s\", len(messages), responses[[\"checklist\", \"response\"]].to_json(indent=2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf81f7ed",
   "metadata": {},
   "source": [
    "#### 2.4.3 - The Engine Config\n",
    "\n",
    "The `EngineConfig` object controls options about the CVE pipeline we are building. It allows us to contain all of the settings in a single object which can be easily used from many different classes which will be used to construct the pipeline. Below we will create the default configuration we will be using for the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b628e418",
   "metadata": {
    "tags": []
   },
   "source": [
    "from cyber_dev_day.config import EngineConfig\n",
    "\n",
    "# Create the engine configuration\n",
    "engine_config = EngineConfig.model_validate({\n",
    "    \"checklist\": {\n",
    "        \"model\": {\n",
    "            \"service\": {\n",
    "                \"type\": \"NIM\", \"api_key\": None\n",
    "            },\n",
    "            \"base_url\": \"https://integrate.api.nvidia.com/v1\",\n",
    "            \"model_name\": \"meta/llama3-70b-instruct\",\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    },\n",
    "    \"agent\": {\n",
    "        \"model\": {\n",
    "            \"service\": {\n",
    "                \"type\": \"NIM\", \"api_key\": None\n",
    "            },\n",
    "            \"base_url\": \"https://integrate.api.nvidia.com/v1\",\n",
    "            \"model_name\": \"meta/llama3-70b-instruct\",\n",
    "            \"temperature\": 0.02\n",
    "        },\n",
    "        \"sbom\": {\n",
    "            \"data_file\":\n",
    "                os.path.join(os.getenv(\"MORPHEUS_ROOT\", \"../\"),\n",
    "                             \"data\",\n",
    "                             \"morpheus_24.03-runtime_sbom.csv\")\n",
    "        },\n",
    "        \"code_repo\": {\n",
    "            \"faiss_dir\": code_faiss_dir, \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\"\n",
    "        }\n",
    "    }\n",
    "})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "09f759e4-fc8c-4c84-ba15-3f3408a2bff3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Print the current configuration object\n",
    "print(engine_config.model_dump_json(indent=2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "806649e4",
   "metadata": {},
   "source": [
    "#### 2.4.4 - Running the Pipeline\n",
    "\n",
    "Now that the pipeline has been defined and the configuration variables have been created, it's time to run the pipeline. The final step is to convert the PyArrow intel dictionary into a single string that our `run_cve_pipeline` function accepts using a template `cve_details_template`. To simplify converting intel dictionaries into strings in the rest of the notebook, we will reuse this template."
   ]
  },
  {
   "cell_type": "code",
   "id": "001ee70c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "# Create a template to generate the cve_details from an intel dictionary\n",
    "cve_details_template = \"\"\"- CVE ID: {cve}\n",
    "- Description: {cve_description}\n",
    "- Vulnerable Package Name: {vuln_package}\n",
    "- Vulnerable Package Version: {vuln_package_version}\n",
    "- CVSS3 Vector String: {cvss3}\"\"\"\n",
    "\n",
    "# Generate the CVE details from the pyarrow intel\n",
    "cve_details = cve_details_template.format(**PYARROW_CVE_INTEL)\n",
    "\n",
    "# Now run the pipeline with a specified CVE description\n",
    "await run_cve_pipeline(pipeline_config, engine_config, [cve_details], retry_bad_input=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c4b8a57",
   "metadata": {},
   "source": [
    "The output of the pipeline should be similar in theme to the following: \n",
    "```\n",
    "Received 1 responses:\n",
    "{\n",
    "  \"checklist\":{\n",
    "    \"0\":[\n",
    "      \"Check for PyArrow: Verify if your project uses the PyArrow library, which is the affected package. If PyArrow is not a dependency in your project, then your code is not vulnerable to this CVE.\",\n",
    "      \"Review Affected Versions: If PyArrow is used, check the version that your project depends on. According to the vulnerability details, versions before 14.0.1 are vulnerable.\",\n",
    "      \"Review Code To Check for Deserialization of Untrusted Data: Check if the IPC and Parquet readers are used to deserialize untrusted data, which can lead to arbitrary code execution.\",\n",
    "      \"Check for Mitigation: If upgrading to PyArrow 14.0.1 or later is not possible, check if the `pyarrow-hotfix` package is imported to disable the vulnerability on older PyArrow versions.\"\n",
    "    ]\n",
    "  },\n",
    "  \"response\":{\n",
    "    \"0\":[\n",
    "      \"Yes, the project uses the PyArrow library, which is the affected package.\",\n",
    "      \"Yes, the Docker container is using a vulnerable version of PyArrow (11.0.0).\",\n",
    "      \"No, the IPC and Parquet readers are not used to deserialize untrusted data.\",\n",
    "      \"Yes, the `pyarrow-hotfix` package is imported to disable the vulnerability on older PyArrow versions.\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "In the output, we can see the output from the first model, which will be the generated checklist items, and the output of each agent, which will be the response to each checklist item. Looking at the checklist items and answers, we can see that the model has successfully determined that the project is vulnerable to the CVE.\n",
    "\n",
    "**NOTE**: Depending on your choice for the Agent or Checklist model, you will see different outputs that can vary in quality quite drastically. Try and explore a few different model choices and temperatures to explore what that looks like. You may also find some inconsistency in results when keeping your parameters constant. This stochasticity is a natural occurence with LLMs, and can be mitigated with prompt engineering or fine tuning.\n",
    "\n",
    "#### 2.4.5 - Hitting the Limits of the LLMs\n",
    "\n",
    "While LLMs can work well for many tasks, they are not perfect. They can fail on seemingly simple tasks, get into a loop, or not follow the output formatting correctly. These edge cases can be hard to catch and can be difficult to debug. For example, if we use the below prompt about Log4j and change the model we use for the Agent, what happens when we run the pipeline?\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1a7e791c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "# Generate the CVE details for the log4j intel\n",
    "cve_details=cve_details_template.format(**LOG4J_CVE_INTEL)\n",
    "\n",
    "# Create the engine configuration\n",
    "suboptimal_engine_config = EngineConfig.model_validate({\n",
    "    \"checklist\": {\n",
    "        \"model\": {\n",
    "            \"service\": {\n",
    "                \"type\": \"NIM\", \"api_key\": None\n",
    "            },\n",
    "            \"base_url\": \"https://integrate.api.nvidia.com/v1\",\n",
    "            \"model_name\": \"meta/llama3-70b-instruct\",\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    },\n",
    "    \"agent\": {\n",
    "        \"model\": {\n",
    "            \"service\": {\n",
    "                \"type\": \"NIM\", \"api_key\": None\n",
    "            },\n",
    "            \"base_url\": \"https://integrate.api.nvidia.com/v1\",\n",
    "            \"model_name\": \"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "            \"temperature\": 0.02\n",
    "        },\n",
    "        \"sbom\": {\n",
    "            \"data_file\":\n",
    "                os.path.join(os.getenv(\"MORPHEUS_ROOT\", \"../\"),\n",
    "                             \"data\",\n",
    "                             \"morpheus_24.03-runtime_sbom.csv\")\n",
    "        },\n",
    "        \"code_repo\": {\n",
    "            \"faiss_dir\": code_faiss_dir, \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\"\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# Now run the pipeline with a specified CVE description\n",
    "await run_cve_pipeline(pipeline_config, suboptimal_engine_config, [cve_details], retry_bad_input=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e8c30da",
   "metadata": {},
   "source": [
    "When we run the pipeline with the Log4j example, it hits an exception instead of running the pipeline to completion. The error message is `Error running agent: An output parsing error occurred` because the agent was not able to reason through the checklist while following LangChain's formatting guidelines. If we look closer, we can see that the model generated the following output for each checklist item:\n",
    "```\n",
    "[\n",
    "      \"Error running agent: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: ` I need to check if the log4j library is present in the Docker`\",\n",
    "      \"Error running agent: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: ` To answer this question, I need to find out which version of log4j`\",\n",
    "      \"Error running agent: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: ` To answer this question, I need to inspect the log4j configuration within the`\",\n",
    "      \"Error running agent: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: ` To answer this question, I need to check if the Docker container uses log`\"\n",
    "    ]\n",
    "```\n",
    "\n",
    "Such errors can be hard to debug as it is explicit why a seemingly innocuous sentence about a thought leads to a parsing error. The reason this occurs is because the LangChain Zero Shot Agent requires every response from the Agent to always end with either a request for an `Action` or a `Final Answer`. We see above that the response contains neither. This occurs despite us explicitly asking the agent to follow those guidelines, as is evident in the `cyber_dev_day.pipeline_utils.build_agent_executor` method as follows:\n",
    "\n",
    "```\n",
    " Action input must only contain the exact input, do not provide any text following that in your response. Always end your response with either an action, or a final answer.\n",
    "```\n",
    "\n",
    "Careful debugging of such output is critical, and some strategies for preventing such errors could include few-shot prompting techniques, model fine-tuninging, or changing the choice of our model.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Please wait here until instructed to continue with running the notebook.\n",
    "</div>\n",
    "\n",
    "## 3 - Beyond Prototyping\n",
    "\n",
    "Up until now, we have been using the pipeline we built to answer questions about the code in the repository. While this works for a few hand picked use cases, it is not suitable to deploy into a production environment for several reasons:\n",
    "\n",
    "1. The LLM models fail to work on some questions which can generate errors in the pipeline\n",
    "   - Since the pipeline chains many LLM calls together, a single error can cause the entire pipeline to fail. For a production environment, we would need to handle these errors more gracefully or improve the model to reduce the number of errors.\n",
    "2. The pipeline is not optimized for performance\n",
    "   - The pipeline is slow to run, because each model needs to be executed sequentially. For a production environment, we would need to optimize the pipeline to handle multiple requests at once.\n",
    "3. The pipeline cannot be easily integrated into other systems\n",
    "   - The pipeline is a standalone script which reads from a single file and needs to be run manually. For a production environment, the pipeline would need to be integrated with other systems, such as a web server or a chatbot.\n",
    "\n",
    "In this section, we will address some of the limitations we encountered in the previous section and discuss how we can overcome them utilizing NIM and Morpheus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a826b5",
   "metadata": {},
   "source": [
    "### 3.1 - Scaling the Pipeline\n",
    "\n",
    "When running pipelines which utilize LLMs, it's important to understand how the LLMs are executed to parallelize their execution as much as possible. This is because LLMs can take a long time to run, as low as a few hundred milliseconds and upwards of a few seconds. Running LLMs serially can compound those runtimes, leading to execution times that grow linearly with the number of LLM requests. A simple diagram of the execution of LLMs for our CVE pipeline is shown below:\n",
    "\n",
    "![Single CVE - Serial](./images/single_cve_serial.jpg)\n",
    "\n",
    "In the diagram above, we can see that the LLMs are executed serially, one after the other. This is not ideal as the execution time of the pipeline is directly proportional to the number of LLMs that are executed. However, there is no dependency between the LLM calls in each of the checklist items. This means that we can parallelize the execution of the LLMs to reduce the overall execution time of the pipeline. A simple diagram of the parallel execution of LLMs for our CVE pipeline is shown below:\n",
    "\n",
    "![Single CVE - Parallel](./images/single_cve_parallel.jpg)\n",
    "\n",
    "In the diagram above, we can see that the total execution time has been reduced as the checklist agent LLMs are executed in parallel. The total execution time is now the maximum time taken to execute any of the LLM agent chains. This is a significant improvement over the serial execution of the LLMs. But what happens if we need to run the entire pipeline for multiple CVEs? A naive approach would be to run the pipeline for each CVE serially, which is shown below:\n",
    "\n",
    "![Multiple CVE - Serial](./images/multiple_cve_serial.jpg)\n",
    "\n",
    "With most LLM libraries, this is the default behavior and improving on this requires more complex solutions such as multiprocessing or distributed workers. However, with Morpheus, this is trivial since Morpheus benefits from pipeline parallelism where each message is processed in an assembly line fashion. This means that we can start processing the next message before the previous one is even completed. A simple diagram of the parallel execution of the pipeline for multiple CVEs is shown below:\n",
    "\n",
    "![Multiple CVE - Parallel](./images/multiple_cve_parallel.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "895f9bcf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "# Create multiple CVE requests\n",
    "cves = [\n",
    "    cve_details_template.format(**PYARROW_CVE_INTEL),\n",
    "    cve_details_template.format(**LOG4J_CVE_INTEL),\n",
    "] * 2\n",
    "\n",
    "await run_cve_pipeline(pipeline_config, engine_config, cves)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c266d439",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you look at the above output, you should see a section that looks like the following:\n",
    "\n",
    "```\n",
    "> Entering new AgentExecutor chain...\n",
    "\n",
    "> Entering new AgentExecutor chain...\n",
    "\n",
    "> Entering new AgentExecutor chain...\n",
    "\n",
    "> Entering new AgentExecutor chain...\n",
    "\n",
    "> Entering new AgentExecutor chain...\n",
    "```\n",
    "\n",
    "Because each executor chain is being started before the previous one completes, they are all running in parallel. But can we verify that this is actually leading to a performance improvement? Let's run the pipeline for a single CVE and multiple CVEs and compare their execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b1f51590",
   "metadata": {
    "tags": []
   },
   "source": [
    "from morpheus.utils.logging_timer import log_time\n",
    "\n",
    "# Update the agent config to disable verbose logging\n",
    "non_verbose_config = engine_config.model_copy(deep=True)\n",
    "non_verbose_config.agent.verbose = False\n",
    "\n",
    "# Disable the logger to make it easer to see the timings\n",
    "parent_logger: logging.Logger = logger.parent\n",
    "saved_level = parent_logger.level\n",
    "parent_logger.setLevel(logging.ERROR)\n",
    "\n",
    "execution_times: dict[int, float] = {}\n",
    "\n",
    "for count in [1, 5, 10]:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with log_time(print, count=count, msg=\"Executing {count} CVE(s). Total: {duration} ms, Average: {ms_per_count} ms\"):\n",
    "        await run_cve_pipeline(pipeline_config, non_verbose_config, [cves[0]] * count, retry_bad_input=True)\n",
    "\n",
    "    execution_times[count] = time.time() - start_time\n",
    "\n",
    "parent_logger.setLevel(saved_level)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "138ebec5",
   "metadata": {},
   "source": [
    "Your actual execution time may differ, but it should look something like the following:\n",
    "\n",
    "```\n",
    "Executing 1 CVE(s). Total: 44263.65375518799 ms, Average: 44263.65375518799 ms\n",
    "Executing 5 CVE(s). Total: 66988.19637298584 ms, Average: 13397.639274597168 ms\n",
    "Executing 10 CVE(s). Total: 62277.28486061096 ms, Average: 6227.728486061096 ms\n",
    "```\n",
    "\n",
    "As you can see, the average execution time per CVE actually goes down as we increase the number of pipeline runs due to the fact that they are being run in parallel. To get an idea of how well the pipeline scales, we can make a plot of the CVE count vs runtimes for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "id": "e84b6723",
   "metadata": {
    "tags": []
   },
   "source": [
    "# importing matplotlib module\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Function to plot\n",
    "plt.plot(execution_times.keys(),\n",
    "         list(\n",
    "             zip(execution_times.values(), [execution_times[1] * x for x in execution_times.keys()],\n",
    "                 [execution_times[1] for _ in execution_times.keys()])),\n",
    "         label=[\"actual\", \"serial\", \"parallel\"])\n",
    "plt.legend()\n",
    "\n",
    "# function to show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d66334a9",
   "metadata": {},
   "source": [
    "We can see the `actual` line is much closer to the `parallel` line than the `serial` line, indicating we are running most of the LLMs in parallel.\n",
    "\n",
    "### 3.2 - Event Driven Pipeline: Creating a Microservice\n",
    "\n",
    "In a true production environment, the CVE scans would be triggered by some other event, such as a new container being uploaded into a registry or a new project being created. In this section, we will show how to create a microservice that can be triggered by an event and run the pipeline we built in the previous sections.\n",
    "\n",
    "Previously, when our pipeline was started, it would read all inputs from a DataFrame and run the pipeline for each input. Once the pipeline was done processing the DataFrame, it would shut down. To run the pipeline as a microservice, we need to modify the pipeline to run continuously and listen for new inputs on an HTTP endpoint.\n",
    "\n",
    "Fortunately, in Morpheus this is as easy as changing out the type of source that is used in the pipeline. The code below is identical to the previous pipeline except we have changed the source from `InMemorySourceStage` to `HttpServerSourceStage`. The `HttpServerSourceStage` class listens for new inputs on an HTTP endpoint and passes them to the next stage in the pipeline. It pulls the inputs from the request body and passes them to the pipeline to be processed.\n",
    "\n",
    "Additionally, right after the `HttpServerSourceStage` we have added a simple custom stage to the pipeline `print_payload`. This custom stage simply prints the payload that was passed to the pipeline. This is useful for debugging and logging exactly when the pipeline was triggered since the results may take time to process and be shown to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca8d4182",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Code for creating a microservice\n",
    "from cyber_dev_day.pipeline_utils import build_cve_llm_engine\n",
    "\n",
    "from morpheus.messages import ControlMessage\n",
    "from morpheus.messages import MessageMeta\n",
    "from morpheus.pipeline.linear_pipeline import LinearPipeline\n",
    "from morpheus.pipeline.stage_decorator import stage\n",
    "from morpheus.stages.input.http_server_source_stage import HttpServerSourceStage\n",
    "from morpheus.stages.llm.llm_engine_stage import LLMEngineStage\n",
    "from morpheus.stages.output.in_memory_sink_stage import InMemorySinkStage\n",
    "from morpheus.stages.preprocess.deserialize_stage import DeserializeStage\n",
    "from morpheus.utils.concat_df import concat_dataframes\n",
    "from morpheus.utils.http_utils import HTTPMethod\n",
    "\n",
    "\n",
    "async def run_cve_pipeline_microservice(p_config: Config, e_config: EngineConfig):\n",
    "\n",
    "    completion_task = {\"task_type\": \"completion\", \"task_dict\": {\"input_keys\": [\"cve_info\"], }}\n",
    "\n",
    "    pipe = LinearPipeline(p_config)\n",
    "\n",
    "    # Use an HTTP source to listen for requests. The expected payload is:\n",
    "    # [{\"cve_info\": <sting>},\n",
    "    #  {\"cve_info\": <sting>},]\n",
    "    pipe.set_source(\n",
    "        HttpServerSourceStage(p_config, bind_address=\"0.0.0.0\", port=26302, endpoint=\"/scan\", method=HTTPMethod.POST))\n",
    "\n",
    "    # To see when our pipeline has been triggered, add a simple logging stage to print the payload\n",
    "    @stage\n",
    "    def print_payload(payload: MessageMeta) -> MessageMeta:\n",
    "        serialized_str = payload.df.to_json(orient=\"records\", lines=True)\n",
    "\n",
    "        logger.info(\"======= Got Request =======\\n%s\\n===========================\", serialized_str)\n",
    "\n",
    "        return payload\n",
    "\n",
    "    pipe.add_stage(print_payload(config=p_config))\n",
    "\n",
    "    # The remainder of the pipeline is identical to the previous example\n",
    "    pipe.add_stage(\n",
    "        DeserializeStage(p_config, message_type=ControlMessage, task_type=\"llm_engine\", task_payload=completion_task))\n",
    "\n",
    "    pipe.add_stage(LLMEngineStage(p_config, engine=build_cve_llm_engine(e_config)))\n",
    "\n",
    "    sink = pipe.add_stage(InMemorySinkStage(p_config))\n",
    "\n",
    "    await pipe.run_async()\n",
    "\n",
    "    messages = sink.get_messages()\n",
    "    responses = concat_dataframes(messages)\n",
    "\n",
    "    logger.info(\"Received %s responses:\\n%s\", len(messages), responses[\"response\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ababebfe",
   "metadata": {},
   "source": [
    "Finally, we can start our microservice by running the pipeline as we have in the past. While the pipeline is running, move on to the next section to see how to trigger the pipeline with an HTTP request.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> When executed, the following cell will run indefinitely. You will need to interrupt the kernel to stop it. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4f87ac9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "await run_cve_pipeline_microservice(pipeline_config, engine_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ccb51d69",
   "metadata": {},
   "source": [
    "#### 3.2.1 - Triggering the Microservice\n",
    "\n",
    "To trigger the microservice, we will use a CURL request to send a request to the microservice. Since the notebook cannot run commands while the microservice is running, we need to open up a new terminal to send the request. To do that, follow the steps below:\n",
    "\n",
    "1. In Jupyter Lab, press Ctrl + Shift + L (Shift + ⌘ + L on Mac) to open a new Launcher tab\n",
    "2. In the Launcher tab, click on the Terminal icon to open a new terminal\n",
    "3. In the terminal, run the following command to send a request to the microservice:\n",
    "\n",
    "```bash\n",
    "curl --request POST \\\n",
    "  --url http://localhost:26302/scan \\\n",
    "  --header 'Content-Type: application/json' \\\n",
    "  --data '[{\n",
    "      \"cve_info\" : \"An issue was discovered in the Linux kernel through 6.0.9. drivers/media/dvb-core/dvbdev.c has a use-after-free, related to dvb_register_device dynamically allocating fops.\"\n",
    "   }]'\n",
    "```\n",
    "\n",
    "4. Once the request is sent, the microservice will process the request and return the results in the terminal\n",
    "   1. To see the results, switch back to the Notebook tab. You should see that the microservice received your request and started processing it.\n",
    "   ```\n",
    "   I20240308 16:00:56.422039 3010283 http_server.cpp:129] Received request: POST : /scan\n",
    "   ```\n",
    "   2. It helps to have the terminal and the notebook side by side so you can see the results in the terminal as they come in. To do this, click on the terminal tab and drag it to the right side of the screen. You should then be able to see the terminal and the notebook side by side similar to the image below:\n",
    "      ![Terminal and Notebook Side by Side](./images/side_by_side.png)\n",
    "5. To stop the microservice, interrupt the kernel by pressing the stop button in the toolbar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65113327",
   "metadata": {},
   "source": [
    "## 4 - Conclusion\n",
    "\n",
    "Throughout this notebook, we explored how GenAI and LLMs can take the transformative role in cybersecurity through automating the CVE analysis workflow. Here are the key learnings and takeaways:\n",
    "\n",
    "### Generative AI and Cybersecurity\n",
    "- **The Role of GenAI and LLMs in Cybersecurity**: Learned about the transformative impact of GenAI and LLMs in cybersecurity, particularly in automating and improving threat detection, analysis, and response. These technologies are crucial for mitigating the manual and time-consuming aspects of cybersecurity tasks.\n",
    "\n",
    "### CVE Impact Analysis\n",
    "- **Challenges in CVE impact analysis**: Challenges include the intensive effort required for gathering information, the complexity of making informed decisions, and the fact that the risk posed by vulnerabilities can vary greatly depending on the specific environment in which they are found.\n",
    "- **Event-Driven LLM Agent Pipeline**: Learned about the concept and implementation of an event-driven LLM agent pipeline as a solution to streamline the CVE analysis process. \n",
    "\n",
    "### Hands-On with LLMs \n",
    "- **LLM Inferencing Through NVIDIA Inference Microservices (NIM)**: Interacted with LLMs through NIM and the `nemollm` Python client library, leveraging the cloud-native framework to simplify the process of making LLM inference requests.\n",
    "- **Refining Model Outputs with Prompt Engineering**: Gained insights into various prompting techniques, including persona-based prompting, prompt templating, and both one-shot and few-shot learning methods.\n",
    "- **Evaluating Model Performance**: Explored strategies for assessing model performance, such as conducting format checks, undergoing manual expert reviews, and creating benchmark datasets.\n",
    "  \n",
    "### Utilization of Retrieval-Augmented Generation (RAG)\n",
    "- **RAG Functionality**: Understood how RAG can augment LLM responses by incorporating external knowledge, thus enhancing the accuracy and context-relevance of the outputs for cybersecurity applications.\n",
    "- **Building RAG Pipelines with Morpheus**: Learned to build RAG pipelines using NVIDIA Morpheus, focusing on its application in constructing high-performance AI-driven cybersecurity workflows.\n",
    "\n",
    "### Prototyping to Production\n",
    "- **Fine-Tuning for Task-Specific Improvements**: Understood the importance of fine-tuning LLMs on specific tasks to overcome limitations and improve output quality.\n",
    "- **Scaling and Parallelization**: Learned about Morpheus’ ability to execute multiple LLM inquiries in parallel, significantly reducing the overall runtime of LLM pipelines.\n",
    "- **Event-Driven Microservice Creation**: Explored the creation of a microservice that responds to real-world events, enabling the automated execution of the CVE analysis pipeline in response to triggers such as container updates.\n",
    "\n",
    "The tutorial demonstrates how to utilize **NIM** and **NVIDIA Morpheus** to develop an LLM-powered agent that assists security analysts with CVE impact analysis. It provides practical insights into refining model outputs, integrating diverse technologies into workflows, and deploying scalable, event-driven solutions for real-world applications. This tutorial serves as a solid starting point for anyone interested in leveraging LLMs to address real-world challenges in cybersecurity and beyond.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

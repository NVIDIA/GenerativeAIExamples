<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Software Component Configuration &mdash; NVIDIA Generative AI Examples 0.5.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/version.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Chain Server" href="chain-server.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="ai-foundation-models.html">AI Foundation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="local-gpu.html">Local GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-gpu.html">Multi-GPU for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized-llm-model.html">Quantized Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector-database.html">Alternative Vector Database</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/00-llm-non-streaming-nemotron.html">Basics: Prompt, Client, and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/01-llm-streaming-client.html">LLM Streaming Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_langchain_simple.html">Q&amp;A with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_llama_index_simple.html">Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/04_llamaindex_hier_node_parser.html">Advanced Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/05_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA AI Endpoints with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA AI Endpoints, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Endpoints with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/10_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm-inference-server.html">NeMo Framework Inference Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="chain-server.html">Chain Server</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Software Component Configuration</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
  SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  SPDX-License-Identifier: Apache-2.0

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<section id="software-component-configuration">
<h1>Software Component Configuration<a class="headerlink" href="#software-component-configuration" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#configuration-with-the-docker-compose-environment-file" id="id1">Configuration with the Docker Compose Environment File</a></p>
<ul>
<li><p><a class="reference internal" href="#llm-server-configuration" id="id2">LLM Server Configuration</a></p></li>
<li><p><a class="reference internal" href="#milvus" id="id3">Milvus</a></p></li>
<li><p><a class="reference internal" href="#pgvector" id="id4">Pgvector</a></p></li>
<li><p><a class="reference internal" href="#chain-server" id="id5">Chain Server</a></p></li>
<li><p><a class="reference internal" href="#rag-playground" id="id6">RAG Playground</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="configuration-with-the-docker-compose-environment-file">
<h2>Configuration with the Docker Compose Environment File<a class="headerlink" href="#configuration-with-the-docker-compose-environment-file" title="Permalink to this headline"></a></h2>
<p>The following sections identify the environment variables and parameters that are used in the <code class="docutils literal notranslate"><span class="pre">rag-app-text-chatbot.yaml</span></code> Docker Compose file in the <code class="docutils literal notranslate"><span class="pre">deploy/compose</span></code> directory of the repository.</p>
<p>You can set environment variables in the <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code> file.</p>
<section id="llm-server-configuration">
<h3>LLM Server Configuration<a class="headerlink" href="#llm-server-configuration" title="Permalink to this headline"></a></h3>
<p>LLM Inference server hosts the Large Language Model (LLM) with Triton Inference Server backend.</p>
<p>You can configure the server using the following environment variables:</p>
<dl class="myst field-list simple">
<dt class="field-odd">MODEL_DIRECTORY</dt>
<dd class="field-odd"><p>Specifies the path to the model directory where model checkpoints are stored.</p>
</dd>
<dt class="field-even">MODEL_ARCHITECTURE</dt>
<dd class="field-even"><p>Defines the architecture of the model used for deployment.</p>
</dd>
<dt class="field-odd">MODEL_MAX_INPUT_LENGTH</dt>
<dd class="field-odd"><p>Maximum allowed input length, with a default value of 3000.</p>
</dd>
<dt class="field-even">QUANTIZATION</dt>
<dd class="field-even"><p>Specifies to enable activation-aware quantization for the LLM. By default, quantization is not enabled.</p>
</dd>
<dt class="field-odd">INFERENCE_GPU_COUNT</dt>
<dd class="field-odd"><p>Specifies the GPUs to be used by Triton for model deployment, with the default setting being “all.”</p>
</dd>
</dl>
</section>
<section id="milvus">
<h3>Milvus<a class="headerlink" href="#milvus" title="Permalink to this headline"></a></h3>
<p>Milvus is the default vector database server.
You can configure Milvus using the following environment variable:</p>
<dl class="myst field-list simple">
<dt class="field-odd">DOCKER_VOLUME_DIRECTORY</dt>
<dd class="field-odd"><p>Specifies the location of the volume mount on the host for the vector database files.
The default value is <code class="docutils literal notranslate"><span class="pre">./volumes/milvus</span></code> in the current working directory.</p>
</dd>
</dl>
</section>
<section id="pgvector">
<h3>Pgvector<a class="headerlink" href="#pgvector" title="Permalink to this headline"></a></h3>
<p>Pgvector is an alternative vector database server.
You can configure pgvector using the following environment variables:</p>
<dl class="myst field-list simple">
<dt class="field-odd">DOCKER_VOLUME_DIRECTORY</dt>
<dd class="field-odd"><p>Specifies the location of the volume mount on the host for the vector database files.
The default value is <code class="docutils literal notranslate"><span class="pre">./volumes/data</span></code> in the current working directory.</p>
</dd>
<dt class="field-even">POSTGRES_PASSWORD</dt>
<dd class="field-even"><p>Specifies the password for authenticating to pgvector.
The default value is <code class="docutils literal notranslate"><span class="pre">password</span></code>.</p>
</dd>
<dt class="field-odd">POSTGRES_USER</dt>
<dd class="field-odd"><p>Specifies the user name for authenticating to pgvector.
The default value is <code class="docutils literal notranslate"><span class="pre">postgres</span></code>.</p>
</dd>
<dt class="field-even">POSTGRES_DB</dt>
<dd class="field-even"><p>Specifies the name of the database instance.
The default value is <code class="docutils literal notranslate"><span class="pre">api</span></code>.</p>
</dd>
</dl>
</section>
<section id="chain-server">
<h3>Chain Server<a class="headerlink" href="#chain-server" title="Permalink to this headline"></a></h3>
<p>The chain server is the core component that interacts with the LLM Inference Server and the Milvus server to obtain responses.
You can configure the server using the following environment variable:</p>
<dl class="myst field-list">
<dt class="field-odd">APP_VECTORSTORE_URL</dt>
<dd class="field-odd"><p>Specifies the URL of the vector database server.</p>
</dd>
<dt class="field-even">APP_VECTORSTORE_NAME</dt>
<dd class="field-even"><p>Specifies the vendor name of the vector database. Values are <code class="docutils literal notranslate"><span class="pre">milvus</span></code> or <code class="docutils literal notranslate"><span class="pre">pgvector</span></code>.</p>
</dd>
<dt class="field-odd">COLLECTION_NAME</dt>
<dd class="field-odd"><p>Specifies the example-specific collection in the vector database.</p>
</dd>
<dt class="field-even">APP_LLM_SERVERURL</dt>
<dd class="field-even"><p>Specifies the URL of Triton Inference Server.</p>
</dd>
<dt class="field-odd">APP_LLM_MODELNAME</dt>
<dd class="field-odd"><p>The model name used by the Triton server.</p>
</dd>
<dt class="field-even">APP_LLM_MODELENGINE</dt>
<dd class="field-even"><p>An enum that specifies the backend name hosting the model. Supported values are as follows:</p>
<p><code class="docutils literal notranslate"><span class="pre">triton-trt-llm</span></code> to use locally deployed LLM models.</p>
<p><code class="docutils literal notranslate"><span class="pre">nv-ai-foundation</span></code> to use models hosted from NVIDIA AI Endpoints.</p>
</dd>
<dt class="field-odd">APP_RETRIEVER_TOPK</dt>
<dd class="field-odd"><p>Number of relevant results to retrieve. The default value is <code class="docutils literal notranslate"><span class="pre">4</span></code>.</p>
</dd>
<dt class="field-even">APP_RETRIEVER_SCORETHRESHOLD</dt>
<dd class="field-even"><p>The minimum confidence score for the retrieved values to be considered. The default value is <code class="docutils literal notranslate"><span class="pre">0.25</span></code>.</p>
</dd>
<dt class="field-odd">APP_PROMPTS_CHATTEMPLATE</dt>
<dd class="field-odd"><p>Specifies the instructions to provide to the model.
The prompt is combined with the user-supplied query and then presented to the model.
The chain server uses this prompt when the query does not use a knowledge base.</p>
</dd>
<dt class="field-even">APP_PROMPTS_RAGTEMPLATE</dt>
<dd class="field-even"><p>Specifies the instructions to provide to the model.
The prompt is combined with the user-supplied query and then presented to the model.
The chain server uses this prompt when the query uses a knowledge base.</p>
</dd>
</dl>
</section>
<section id="rag-playground">
<h3>RAG Playground<a class="headerlink" href="#rag-playground" title="Permalink to this headline"></a></h3>
<p>The RAG playground component is the user interface web application that interacts with the chain server to retrieve responses and provide a user interface to upload documents.
You can configure the server using the following environment variables:</p>
<dl class="myst field-list simple">
<dt class="field-odd">APP_SERVERURL</dt>
<dd class="field-odd"><p>Specifies the URL for the chain server.</p>
</dd>
<dt class="field-even">APP_SERVERPORT</dt>
<dd class="field-even"><p>Specifies the network port number for the chain server.</p>
</dd>
<dt class="field-odd">APP_MODELNAME</dt>
<dd class="field-odd"><p>Specifies the name of the large language model used in the deployment.
This information is for display purposes only and does not affect the inference process.</p>
</dd>
<dt class="field-even">RIVA_API_URI</dt>
<dd class="field-even"><p>Specifies the host name and port of the NVIDIA Riva server.
This field is optional and provides automatic speech recognition (ASR) and text-to-speech (TTS) functionality.</p>
</dd>
<dt class="field-odd">RIVA_API_KEY</dt>
<dd class="field-odd"><p>Specifies a key to access the Riva API.
This field is optional.</p>
</dd>
<dt class="field-even">RIVA_FUNCTION_ID</dt>
<dd class="field-even"><p>Specifies the function ID to access the Riva API.
This field is optional.</p>
</dd>
<dt class="field-odd">TTS_SAMPLE_RATE</dt>
<dd class="field-odd"><p>Specifies the sample rate in hertz (Hz).
The default value is <code class="docutils literal notranslate"><span class="pre">48000</span></code>.</p>
</dd>
</dl>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="chain-server.html" class="btn btn-neutral float-left" title="Chain Server" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>
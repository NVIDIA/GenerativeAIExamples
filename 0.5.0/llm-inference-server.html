<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NeMo Framework Inference Server &mdash; NVIDIA Generative AI Examples 0.5.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/version.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="RAG Playground Web Application" href="frontend.html" />
    <link rel="prev" title="Architecture" href="architecture.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="ai-foundation-models.html">AI Foundation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="local-gpu.html">Local GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-gpu.html">Multi-GPU for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized-llm-model.html">Quantized Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector-database.html">Alternative Vector Database</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/00-llm-non-streaming-nemotron.html">Basics: Prompt, Client, and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/01-llm-streaming-client.html">LLM Streaming Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_langchain_simple.html">Q&amp;A with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_llama_index_simple.html">Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/04_llamaindex_hier_node_parser.html">Advanced Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/05_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA AI Endpoints with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA AI Endpoints, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Endpoints with LangChain Agent</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">NeMo Framework Inference Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>NeMo Framework Inference Server</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
  SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  SPDX-License-Identifier: Apache-2.0

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<section id="nemo-framework-inference-server">
<h1>NeMo Framework Inference Server<a class="headerlink" href="#nemo-framework-inference-server" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#about-the-inference-server" id="id1">About the Inference Server</a></p></li>
<li><p><a class="reference internal" href="#running-the-inference-server-individually" id="id2">Running the Inference Server Individually</a></p></li>
</ul>
</div>
<section id="about-the-inference-server">
<h2>About the Inference Server<a class="headerlink" href="#about-the-inference-server" title="Permalink to this headline"></a></h2>
<p>The generative AI examples use <a class="reference external" href="https://docs.nvidia.com/nemo-framework/user-guide/latest/index.html">NeMo Framework Inference Server</a> container.
NeMo can create optimized LLM using TensorRT-LLM and can deploy models using NVIDIA Triton Inference Server for high-performance, cost-effective, and low-latency inference.
Many examples use Llama 2 models and LLM Inference Server container contains modules and scripts that are required for TRT-LLM conversion of the Llama 2 models and deployment using NVIDIA Triton Inference Server.</p>
<p>The inference server is used with examples that deploy a model on-premises.
The examples that use <a class="reference external" href="https://www.nvidia.com/en-in/ai-data-science/foundation-models/">NVIDIA AI foundation models</a> or NVIDIA AI Endpoints do not use this component.</p>
</section>
<section id="running-the-inference-server-individually">
<h2>Running the Inference Server Individually<a class="headerlink" href="#running-the-inference-server-individually" title="Permalink to this headline"></a></h2>
<p>The following steps describe how a Llama 2 model deployment.</p>
<ul>
<li><p>Download Llama 2 Chat Model Weights from <a class="reference external" href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">Meta</a> or <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf/">HuggingFace</a>. You can check <a class="reference internal" href="support-matrix.html"><span class="doc std std-doc">support matrix</span></a> for GPU requirements for the deployment.</p></li>
<li><p>Update the <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code> file with <code class="docutils literal notranslate"><span class="pre">MODEL_DIRECTORY</span></code> as the downloaded Llama 2 model path and other model parameters as needed.</p></li>
<li><p>Build the LLM inference server container from source:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">source</span><span class="w"> </span>deploy/compose/compose.env
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>build<span class="w"> </span>llm
</pre></div>
</div>
</li>
<li><p>Run the container. The container starts Triton Inference Server with TRT-LLM optimized Llama 2 model:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">source</span><span class="w"> </span>deploy/compose/compose.env
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>up<span class="w"> </span>llm
</pre></div>
</div>
</li>
</ul>
<p>After the optimized Llama 2 model is deployed in Triton Inference Server, clients can send HTTP/REST or gRPC requests directly to the server.
A sample implementation of a client can be found in the <code class="docutils literal notranslate"><span class="pre">triton_trt_llm.py</span></code> file of GitHub repository at <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/tree/main/integrations/langchain/llms">integrations/langchain/llms/</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="architecture.html" class="btn btn-neutral float-left" title="Architecture" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="frontend.html" class="btn btn-neutral float-right" title="RAG Playground Web Application" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>